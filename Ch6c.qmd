
```{r} 
#| include: false
library(dsld)
library(qeML)
```

{{< pagebreak >}}

# Singular Value Decomposition

::: {.callout-note}

## Goals of this chapter:

We've seen the notion of eigenanalysis for square matrices. Well, it
turns out this can be extended usefully for nonsquare matrices, via
*Singular Value Decomposition* (SVD), the topic of this chapter.

::: 



```{r} 
#| include: false
library(dsld)
library(qeML)
```

## Basic Idea

Here is what we are aiming for.

::: {.callout-note}

## Our target relation:

Given an $m \times n$ matrix $A$, we wish to find orthogonal matrices
$U$ and $V$, and a matrix $\Sigma = diag(\sigma_1,...,\sigma_n)$ whose
nonzero elements are positive, such that

$$
A = U \Sigma V'
$${#eq-svd}

By convention the ordering of columns is set so that the $\sigma_i$
occur in nonincreasing order.

Note that, by matrix partitioning, @eq-svd also says that 

$$
A = \sum_{i=1}^n \sigma_i u_i v_i'
$${#eq-aissumuivi}

:::

Note the dimensions: 

* $U$ is $m \times n$

* $V$ is $n \times n$

* $\Sigma$ is $n \times n$

Let $r$ denote the rank of $A$. Recall that $r \leq \min(m,n)$. It will
turn out that $\sigma_i = 0$ for $i=r+1,...,n$.

## Solution

There are various derivations, e.g. along the lines of @sec-firstPC (one
maximizes the quantity $U A V'$), but let's go quickly to the answer:  

1. Compute the eigenvalues $\sigma_1,...,\sigma_n$
and eigenvectors $q_1,...,q_n$ of $A'A$. 

2. Since $A'A$ is symmetric and positive-semidefinite, its eigenvalues 
   will be nonnegative and its length-1 eigenvectors will be orthogonal. 

3. Set $\Sigma$ to $diag(\sqrt{\sigma_1},...,\sqrt{\sigma_n})$, the
   nonincreasing list of eigenvalues. Set the columns of $V$ to
   the corresponding eigenvectors (recall @sec-diagonalizable). 

4. Set 

$$
u_i = \frac{1}{\sigma_i} Av_i, ~ i=1,...,r
$$

Using the Gram-Schmidt Method (@sec-gramschmidt), we can compute (if $r
< m$ necessitates it) vectors $u_{r+1},...u_m$ so that $u_1,...,u_m$ is
an orthonormal basis for $\mathcal{R}^m$.  Set $U$, decribed in
partitioning terms: to 

$$
U = (u_1|...|u_m)
$$

## Checking the Formula

Are $U$ and $V$ really orthogonal, and does $U \Sigma V'$ really work out
to be $A$?

* $U$ is orthogonal.

  This follows immediately from the fact that the columns of $U$ are an
  orthonormal basis from $\mathcal{R}^m$.

* $V$ is orthogonal, by @sec-eigortho.

* The product evaluates to $A$, as claimed.

    Using matrix partitioning, we have
    
    $$
    U \Sigma = (\sigma_1 u_1 | ... | \sigma_n u_n)
    $$
    
    So, again using partitioning,
    
    $$
    U \Sigma V' = (\sigma_1 u_1 | ... | \sigma_n u_n) 
     \left (
     \begin{array}{r}
     v_1' \\
     ... \\
     v_n'  \\
     \end{array}
     \right )
    = \sum_{i=1}^n \sigma_i u_i v_i'  
    $$
    
    That last expression is equal to $A$, by @eq-aissumuivi.

## Uniqueness

The SVD can be shown to be unique, if there are no repeated eigenvalues.
If the latter exist, one can permute the corresponding columns of $U$
and $V$, but otherwise uniqueness holds.

## The Fundamental Theorem of Linear Algebra

One can define four fundamental subspaces for any matrix $A$. We will
need the following:

* $\textrm{row space}(A), \mathcal{R}(A)$: $\{x'A\}~~$ (all linear
  combinations of rows of $A$)

* $\textrm{column space}(A), \mathcal{C}(A)$: $\{Ax\}~~$ (all linear
  combinations of columns of $A$)

* $\textrm{null space}(A), \mathcal{N}(A)$: $\{x: Ax = 0\}$

* $\textrm{left null space}(A) = \mathcal{N}(A')$: $\{x: x'A = 0\}$ 

These subspaces have various properties, which may be verified via
SVD. @eq-aissumuivi will come in very handy. 

Consider the column space first. From @eq-aissumuivi write

$$
Ax = \sum_{i=1}^n \sigma_i u_i v_i' x 
$${#eq-Ax}
 
Note that $v_i'x$ is a scalar, so we have

$$ 
\begin{align}
Ax &= \sum_{i=1}^n [\sigma_i (v_i' x)] u_i \\
&= \sum_{i=1}^r [\sigma_i (v_i' x)] u_i 
\end{align}
$${#eq-Ax1}

So 

$$
\mathcal{C}(A) \subset \textrm{ span}(u_1,...,u_r)
$${#eq-colsubset} 

Now, is the left side equal to all of the right side? Say a member of
the right side is $c_1 u_1+...+c_r u_r$. Then, using the fact that 
$\{v_1....,v_r\}$ is an orthonormal set, take $x$ to be

$$
x = \sum_{j=1}^r \frac{c_j}{\sigma_j} v_j
$$

Then for each $i$, we have

$$
\sigma_i v_i'x =c_i
$$

and 

$$
Ax = \sum_{i=1}^r c_i u_i
$$

as desired, so @eq-colsubset is actually an equality.

We have thus characterized one of the four fundamental subspaces:

<blockquote>

The column space of $A$ has dimension $r$, with basis
$u_1,...[_r]()$.
</blockquote>

Now look at the null space.  $Ax = 0$ means:

* The vector $x$ is orthogonal to each row of $A$.

*

  But the above argument also has the biproduct of showing that 
  the null space of $A$ is exactly $D_2$.

  We now have characterizations of two of the four subspaces:

  <blockquote>
  
  The column space of $A$ has dimension $r$, with basis $v_1,...v_r$.
  The null space has dimension $n-r$, with basis $v_{r+1},...,v_n$.
  
  </blockquote>

* Now for the row space, just piggyback on the result for the column
  space. 

  $\mathcal{R}(A) = \mathcal{C}(A')$

  Since $A = U \Sigma V'$ we have that the SVD for $A'$ is 
  $V \Sigma U'$. (Recall the uniqueness of SVD.). So $A'$
  has its column space of dimension $r$ and basis $u_1,...u_r$.
  This is then the row space of $A$.

## The Data Scientist's Swiss Army Knife

There are a great many pieces of information and sources of tools packed
into this innocuous-looking factorization.  Let's take a look at some:

### Rank of $A$

This is the number of nonzero values in $\Sigma$. This follows from
@thm-ranknotchange, since $U$ and $V$, as orthogonal matrices, are invertible.

It should be noted, though, that if there are some really tiny elements
there, one might also entertain the thought that the true rank is less
than this size, as these tiny values may be due to roundoff error from 0.

### SVD as matrix pseudoinverse

Recall the example in @sec-censusrref. We could not have dummy-variable
columns for both male and female, as their sum would be a column of all
1s, in addition to a column the X data matrix already had. The three columns
would then have a nonzero linear combination that evaluates to the 0
vector.  Then in @eq-linregformula, $A$ (i.e. X) would not be of full
rank, and $(A'A)^{-1}$ would not exist.

And yet the equation from which that comes,

$$
A'A b = A'S
$${#eq-overdetermined}

is still valid. We could, as in that example, remove one of the gender
columns, thus solving the problem of less than full rank, but the use of
*pseudoinverses* (also known as *generalized inverses*) solves the
problem directly. If $A$ has hundreds or thousands of columns, say, the
use of pseudoinverses may be more convenient.

We omit the formal definition of pseudoinverses and their properties. We
will note the use of the latter as the need arises. For now, we state
without proof that:

* One of the most famous forms of pseudoinverse, Moore-Penrose, is 
  based on SVD.  Given the SVD of a matrix $M$, 

  $$
  M = U_M \Sigma_M V_{M}'
  $$

  its Moore-Penrose inverse, denoted by $M^{-1}$ is 

  $$
  M^{-} = V_M \Sigma_M^{-} U_{M}'
  $$

  where $\Sigma_M^{-}$ is the diagonal matrix obtained form $\Sigma_M$
  by replacing each nonzero element by its reciprocal.

* The Moore-Penrose solution of $Mz = w$ for vectors $z$ and $w$, is
  $M^{-} w$.

By the way, the R function **MASS::ginv** performs the necessary
computation for us; we need not call **svd()**.

### SVD in linear models

Now apply this to our linear model problem @eq-overdetermined. The
claim is that 

$$
b = A^{-} S = V \Sigma^{-} U' S
$$ 

solves the equation. [As in @sec-matalg, where we found that the inverse of
a product is the reverse product of the inverses, the same holds for
pseudoinverses.]{.column-margin} Let's check (warning: this will be
messy):

$$
\begin{align}
A'A b &= (U \Sigma V')' (U \Sigma V') (U \Sigma V')^{-} S \\
&= (V \Sigma U') (U \Sigma V') (V \Sigma^{-} U') S \\
&= V \Sigma^2 V'V \Sigma^{-} U' S \\
&= V \Sigma^{-} U' S
\end{align}
$$

But that is exactly the expression we found for $A^{-1}S$ above.

$\blacksquare$

## Example: Census Data 

```{r}
library(qeML)
data(svcensus)
# have only 1 categorical/dichotomous variable, for simple example
head(svcensus)
svc <- svcensus[,-c(2,3)]
svc <- factorsToDummies(svc)
head(svc)
x <- cbind(1,svc[,-2])
head(x)
xminus <- MASS::ginv(x)
bhat <- xminus %*% svc[,2]
bhat
lm(wageinc ~ .,svcensus[,-c(2,3)])$coef
```

The two approaches are consistent with each other (though internally
they are solving slightly different problems). Note that

$$
-13361.634-(-2660.821) = -10700.81
$$
 
### SVD as the minimum-norm solution

In an overdetermined linear system such as @eq-overdetermined, there are
many solutions. However, an advantage of Moore-Penrose is that it gives
us the *minimum norm* solution. We'll discuss the significance of this
shortly, but let's prove it first.

::: {#thm-shortest}

## The Moore-Penrose Solution Is Min-Norm

Of all solutions $b$ to

$$
A'A b = A'S
$${#eq-linsyst}

the Moore-Penrose solution

$$
b = V \Sigma^{-} U' S
$$

minimizes $||b||$.

:::

::: {.proof}

Partition $U$ into $(U_1 | U_2)$, its first $r$ columns and the last
$n-r$ columns. Do the same for $V$, so that

$$
V' =
 \left (
 \begin{array}{r}
 V_1' \\
 V_2' \\
 \end{array}
 \right )
$$

Multiply @eq-linsyst on the left by $U'$

:::

### SVD as the best low-rank approximation

## Your Turn

❄️  **Your Turn:** Show that in the SVD factorization, $U$ consists of
the eigenvectors of $AA'$.

