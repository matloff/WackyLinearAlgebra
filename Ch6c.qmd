
```{r} 
#| include: false
library(dsld)
library(qeML)
```

{{< pagebreak >}}

# Singular Value Decomposition {#sec-svd}

::: {.callout-note}

## Goals of this chapter:

We've seen the notion of eigenanalysis for square matrices. Well, it
turns out this can be extended usefully for nonsquare matrices, via
*Singular Value Decomposition* (SVD), the topic of this chapter,
with important applications.

::: 



```{r} 
#| include: false
library(dsld)
library(qeML)
```

## Basic Idea

Here is what we are aiming for.

::: {.callout-note}

## Our target relation:

Given an $m \times n$ matrix $A$, we wish to find orthogonal matrices
$U$ and $V$, and a matrix $\Sigma = diag(\sigma_1,...,\sigma_n)$ whose
nonzero elements are positive, such that

$$
A = U \Sigma V'
$${#eq-svd}

By convention the ordering of columns is set so that the $\sigma_i$
occur in nonincreasing order.

Note that, by matrix partitioning, @eq-svd also says that 

$$
A = \sum_{i=1}^n \sigma_i u_i v_i'
$${#eq-aissumuivi}

where $u_i$ and $v_i$ are the $i^{th}$ rows of $U$ and $V$.

:::

Note the dimensions: 

* $U$ is $m \times n$

* $V$ is $n \times n$

* $\Sigma$ is $n \times n$

Let $r$ denote the rank of $A$. It will turn out that $\sigma_i = 0$ for
$i=r+1,...,n$.

## Example Applications 

### Recommender Systems

Recall @sec-recsys, which began with movie ratings data. We would like
to predict the rating some particular user would give to some particular
movie. It will turn out that we can set up SVD in such a way that $U$
contains movie data and $V$ contains user data. 

### Text Classification

An oft-used example is that in which we have a collection of newspaper
articles that we wish to categorize, say politics, sports, health,
finance and so on. Say one of the articles includes the word *bonds*; is
it referring to financial instruments, family relations, former baseball
star Barry Bonds etc.?

In spite of today's dazzling array of Large Language Models, a simple problem 
like this may be better solved using straightforward methods, such as
SVD. We set up a *document-term matrix*, with element $(i,j)$ being 1 or
0, according to whether document $i$ contains word $j$. Applying SVD to
this matrix, we have a setting with similar appeal to the recommender 
systems example above, in which $U$ contains our data on documents and
$V$ does the same for words.

### Dimension Reduction

Recall that in the matrix $\Sigma$, the eigenvalues are arranged in
decreasing order, possibly followed by 0s. This suggests that we can
achieve dimension reduction by replacing even some of the smaller
eigenvalues by 0s as well, with corresponding adjustments to the columns
of $U$ and $V$. We will discuss this below in @sec-svddimred. 



## Solution to Our SVD Goal

There are various derivations, e.g. along the lines of @sec-firstPC, 
but let's go directly to the answer:  

1. Say $A$ is $p \times n$, so that $A'A$ will be of size $n
   \times n$. Let $r$ denote the rank of $A'A$, which will be the same
   as the rank of $A$ from @thm-rankapa.

2. Compute the eigenvalues $\sigma_1,...,\sigma_n$
   and eigenvectors $v_1,...,v_n$ of $A'A$. Normalize the $v_i$ by
   dividing by their lengths. (The same eigenvalues will still hold.)
   Order the $\sigma_i$ from largest to smallest, and use the same
   ordering for the $v_i$.

3. Since $A'A$ is symmetric and positive-semidefinite, its eigenvalues
   will be nonnegative and its eigenvectors $v_1,...,v_r$ will be
   orthogonal as long as $\sigma_1,...,\sigma_r$ are distinct, as is the
   case for numeric data (@sec-prob0). Since these eigenvectors
   diagonalize $A'A$ with the $\sigma_i$ on the diagonal (@sec-pca),
   we will have $\sigma_{r+1} = ...  = \sigma_n = 0$.

4. Set $\Sigma$ to $diag(\sigma_1,...,\sigma_n)$, the
   nonincreasing list of those eigenvalues. Set the first $r$ columns of
   $V$ to the corresponding eigenvectors. Use the Gram-Schmidt Method
   (see @sec-gramschmidt) to add $n-r$ more vectors. $V$ will then have
   orthonormal columns, and thus be an orthogonal matrix.

5. Set 

   $$ 
   u_i = \frac{1}{\sigma_i} Av_i, ~ i=1,...,r 
   $${#eq-uifromvi}

   The $u_i$ will be orthogonal: For $i \neq j$,

   $$
   u_i' u_j = \frac{1}{\sigma_i \sigma_j} v_i' A'A v_j =
   \frac{1}{\sigma_i \sigma_j} v_i' v_j = 0
   $$

   where we have used the facts that $v_j$ is an eigenvector of $A'A$
   and the $v_k$ are orthogonal.
   
   Using Gram-Schmidt, we can compute (if $r < n$ necessitates it)
   vectors $u_{r+1},...u_n$ so that $u_1,...,u_n$ is an orthonormal
   basis for $\mathcal{R}^m$.  Set $U$, described in partitioning terms:
   to 
   
   $$ U = (u_1|...|u_n) $$

## Interpretation of $U$ and $V${#sec-interpuv}

Again, say $A$ is of size $m \times n$.  Recall that $V$ consists
of the eigenvectors of the $n \times n$ matrix $A'A$. $U$ is constructed
from vectors as in @eq-uifromvi, but actually those are eigenvectors
of $AA'$:

\begin{align}
(AA') u_{i} &= (AA') ( \frac{1}{\sigma_i} Av_i) \\
&= \frac{1}{\sigma_i} A[(A'A) ~ v_i] \\
&= \frac{1}{\sigma_i} A [\sigma_i v_i] \\
&= \sigma_i u_i
\end{align}

So, $u_i$ is an eigenvector of $AA'$, with eigenvalue $\sigma_i$. 

In other words, taking for concreteness the application in which
moviegoers rate movies (we take $A$ to be fully known for now):

* The columns of $U$ are basically eigenvectors of the matrix $AA'$,
  which in turn has one row and one column for each moviegoer.

* The columns of $V$ are basically eigenvectors of the matrix $A'A$,
  which in turn has one row and one column for each movie.

* Thus the SVD $A = U \Sigma V'$ expresses the ratings matrix $A$ in
  terms of a moviegoers data factor $U$ and a movies data factor $V$.
  To write it as a genuine product of two factors, write

  $$
  A = U \Sigma V' = (U \Sigma^{0.5}) (\Sigma^{0.5} V') = WH'
  $${#eq-wh}

  where $\Sigma^{0.5}$ is the diagonal matrix with elements
  $\sqrt{\sigma_i}$.

## SVD as a basis for matrix generalized inverse

Recall the example in @sec-censusrref. We could not have dummy-variable
columns for both male and female, as their sum would be a column of all
1s, in addition to a column of 1s the X data matrix already had. The
three columns would then have a nonzero linear combination that
evaluates to the 0 vector.  Then in @eq-linregformula, $A$ (i.e. X)
would not be of full rank, nor would $A'A$ (@thm-rankapa), so that
$(A'A)^{-1}$ would not exist.

And yet the equation from which that comes,

$$
A'A b = A'S
$${#eq-overdetermined}  

is still valid. We could, as in that example, remove one of the gender
columns, thus solving the problem of less than full rank, but the use of
*generalized inverses* solves the problem directly. If $A$ has many
binary variables, the use of generalized inverses may be more
convenient.

We will omit the formal definition of generalzed inverses and their
properties, returning to the topic in @sec-chapDD.  For now, we note the
following:

One of the most famous forms of generalized inverse, is the
Moore-Penrose *pseudoinverse*, based on SVD.  Given the SVD of a matrix
$M$, 

$$
M = U_M \Sigma_M V_{M}'
$$

its Moore-Penrose inverse, denoted by $M^{+}$, is 

$$
M^{+} = V_M \Sigma_M^{+} U_{M}'
$$

where $\Sigma_M^{+}$ is the diagonal matrix obtained from $\Sigma_M$
by replacing each nonzero element by its reciprocal.

The Moore-Penrose solution of $Mz = w$ for vectors $z$ and $w$, is

$$
z = M^{+} w
$${#eq-MoorePenrose}

The R function **MASS::ginv** performs the necessary
computation for us; we need not call **svd()**.

## SVD in linear models  

Now apply this to our linear model problem (@eq-overdetermined).
We claim that 

$$
b = A^{+} S = V \Sigma^{+} U' S
$$ 

solves the equation. (Actually if $A$ is of less than full rank, there
are infinitely many solutions, a point discussed in detail in
@sec-chapDD.)

Let's check. Before beginning, recall that the orthogonal nature of $U$
and $V$ implies that $U'U = I$ and $V'V = I$.

$$
\begin{aligned}
A'A b &= (U \Sigma V')' (U \Sigma V') (U \Sigma V')^{+} S \\
&= (V \Sigma U') (U \Sigma V') (V \Sigma^{+} U' S) \\
&= V \Sigma^2 V'(V \Sigma^{+} U' S) \\
&= V \Sigma U' S \\
&= A'S
\end{aligned}
$$

$\square$

## Example: Census Data 

```{r}
library(qeML)
data(svcensus)
head(svcensus)
svc <- svcensus[,-c(2,3)]
# have only 1 categorical/dichotomous variable, for simple example
svc <- factorsToDummies(svc)
head(svc)
x <- cbind(1,svc[,-2])
head(x)
xplus <- MASS::ginv(x)
bhat <- xplus %*% svc[,2]
bhat
lm(wageinc ~ .,svcensus[,-c(2,3)])$coef
```

Since the two analyses use different sets of predictors, it is not
surprising that the intercept terms differ, but otherwise
the two approaches are consistent with each other. This includes the
gender variables, since

$$
-13361.634-(-2660.821) = -10700.81
$$

## Dimension Reduction: SVD as the Best Low-Rank Approximation{#sec-svddimred}

This is a very common application of SVD.

### "Thin" SVD

The SVD of a rank $r$, $q \times n$ matrix $A$ can be partitioned as

$$
(U_1 | U_2) 
\left (
\begin{array}{rr}
D_1 & 0 \\
0 & 0  \\
\end{array}
\right )
(V_1 | V2)'
$$

where $U_1$ is of size $q \times r$, $V_1$ is of size $n \times r$, and
$D_1$ is $r \times r$. Simplifying, we have

$$
A = (U_1 D_1 | 0)
\left (
\begin{array}{r}
V_1' \\
V_2' \\
\end{array}
\right ) =
U_1 D_1 V_1'
$${#eq-u1d1v1}

So, we've reduced the size of memory needed to store the SVD, by using
$U_1$, $D_1$ and $V_1$ instead of the larger $U$, $D$ and $V$. This is
called the *thin* SVD.

But there's more:

### Low-rank approximation

The eigenvalues of a matrix, arranged from largest to smallest, tend to
degrade in a gradual manner, as seen for example in @sec-backtoafrica.
So, in the context of SVD, with the last $n-r$ eigenvalues being 0s, 
it will typically be the case that the last few eigenvalues among
$\sigma_1,...,\sigma_r$ are *near* 0. 

In other words, we probably can get a good approximation to the SVD by
treating those near-0 eigenvalues as 0s, and removing the corresponding
columns of $U_1$ and $V_1$. Why do this?

* Achieve a further reduction in storage requirements. for instance in
  settings in which we have many, many images or even better, many, many
  videos. [It can be shown that the above recipe produces the "best"
  low-rank approximation, in terms of *Frobenius* norm. The lattern
  treats an $m \times n$ matrix as a vector of length $mn$ and applies
  the ordinary $l_2$ norm.]{.column-margin}

* Reduce noise, again for example with images. Small blotches are
  smoothed out.

* In the spirit of @sec-shrinkage, the low-rank approximation may be
  viewed as a "shrunken" SVD, a remedy to possible overfitting.

### Example: Image Compression

Let's try that idea with the picture on the cover of this book. We will

* convert from color to grayscale for simplicity (a color image consists
  of three matrices, one for each primary color)

* calculate the SVD of the resulting matrix

* retain only the first $k$ eigenvectors and eigenvalues; the smaller $k$
  is, the greater the storage savings but the poorer the approximation

Here is the code:

```{r}
library(imager)
img <- load.image('prj.png')  
# grayscale() doesn't work directly on this, 
# image, due to alpha (transparency) channel
imgNoAlpha <- rm.alpha(img)
imgGrayNoAlpha <- grayscale(imgNoAlpha)
dim(imgGrayNoAlpha)
imgSVD <- svd(imgGrayNoAlpha)
u <- imgSVD$u
v <- imgSVD$v
origImgMat <- u %*% diag(imgSVD$d) %*% t(v)
plot(as.cimg(origImgMat))
retainedRank <- 50  # keep only the first 50 columns in U and V
newImgMat <- u[,1:retainedRank] %*% diag((imgSVD$d)[1:retainedRank]) %*%
   t(v[,1:retainedRank])
plot(as.cimg(newImgMat))
```

The rank-50 version is almost as sharp as the full image -- and achieves
a huge reduction in storage space.

## Matrix Factorization in Recommender Systems

In the last section, our goal was to "thin out" a matrix In this
section, we hope to fill in a sparse matrix.

Let $A$ denote the ratings matrix, so that the element in row $i$,
column $j$ is the rating user $i$ gives to item $j$. Note that most
elements of $A$ are unknown, and we hope to predict them with some
reasonable amount of accuracy.

Again, since we do not know all of $A$, we do not know any of the
matrices on the right either. We will return to this problem shortly,
but for now pretend $A$ and the matrices are known. 

Write the SVD, as in @eq-wh:

$$
A = U \Sigma V' = (U \Sigma^{0.5}) (\Sigma^{0.5} V') = WH'
$$

For concreteness, say the context is users rating movies.  Say $A$ is $u
\times m$, for $u$ users and $m$ movies. Again, the
the point is that we have factored $A$ into the product of a matrix $W$
containing information about the users' ratings and a matrix $H'$
that does the same for items. 

Note that the row $i$, column $j$ element of $A$ is equal to $w_i h_j$,
where $w_i$ is row $i$ of $W$ and $v_j$ is column $j$ of $H'$.

This suggests that the following iterative process may work:

1. Replace the unknown elements of $A$ by some temporary values. This
   could be all 0s, say, or maybe replacing all unknown values in column
   $j$ by the mean of the intact values in that column.

2. Find $W$ and $H'$ for that temporary version of $A$, our guess.

3. Use the formula $w_i h_j$ to calculate (new guesses for) the unknown
   elements, and even the known elements, updating accordingly to a 
   new guess for $A$.

4. If convergence not yet reached, go to Step 2.

Optionally in Step 2, we can first convert the SVD to a low-rank 
approximation before computing $W$ and $H$.

And though SVD provided the motivation for the model $A = WH'$,
we can use the model more generally, i.e. without assuming
$W = U \Sigma^{0.5}$ and $H' = \Sigma^{0.5} V'$.

For example, one approach is to minimize
[Note that the "Y" variable here, i.e. the quantity being predicted
in the least-squares analysis, is $A$, a matrix rather than the usual
vector. The conditional mean is then given by @eq-condmean if one
assumes a multivariate normal settings. That assumption is not an issue,
though, as recommender systems methods tend to be *ad hoc* rather that
formal.]{.column-margin}

$$
||A - WH'||^2 + \lambda ||W||^2 + \mu ||H||^2
$${#eq-altls}

again in the spirit of @sec-shrinkage, with shrinkage parameters
$\lambda$ and $\mu$. The computation is made much easier via an
*alternating least squares* scheme. One first holds $H'$ fixed,
minimizing @eq-altls with respect to $W$. In effect, this is ridge
regresssion, with $W$ playing the role of the unknown coefficients
vector. Then in the seccond iterations, the roles are reversed, with
$H'$ now taken as the unknown coefficients vector, and so on.

## Using SVD to Gain Insight into Ridge Regression

[See Ryan Tibshirani, *High-Dimensional Regression: Ridge* and
T. Hastie, *Ridge Regularization: an Essential Concept in Data Science*.
]{.column-margin}
Consider once again @eq-overdetermined, or better, @eq-ridge. It turns
out that SVD gives us more detailed information of how ridge regression
shrinks estimated regression coefficients.

Writing as usual $A = U \Sigma V'$, with $A$ of size $n \times p$ and 
with $\Sigma = diag(\sigma_1,..., \sigma_p$, some algebraic manipulation 
yields the following equation for the estimated coefficients vector:

$$
b_{\lambda} = \Sigma_{\sigma_j > 0}
v_j \frac{\sigma_j^2}{\sigma_j^2 + \lambda^2} u_j'S
$${#eq-ridgeb}

where $u_j$ and $v_j$ are column $j$ of $U$ and $S$.

And the vector of fitted "Y" values is

$$
Ab = \Sigma_{\sigma_j > 0}
 \frac{\sigma_j^2}{\sigma_j^2 + \lambda^2} u_j u_j' S
$${#eq-ridgefitted}

These two equations form great examples of the power of SVD:

* @eq-ridgeb gives an explicit formula for the vector of estimated
  coefficients $b$ as a function of $\lambda$. Without it, we would need
  to do the matrix inversion in @eq-ridge once for each value of
  $\lambda$, potentially a very slow process.

* Ridge does not shrink the coefficients uniformly. @eq-ridgefitted tells us
  how it works. The quantity
  
  $$
\frac{\sigma_j^2}{\sigma_j^2 + \lambda^2} \leq 1
$$

  gives us a proportional shrinkage factor. Of course, non-ridge
  corresponds to $\lambda = 0$, so the shrinkage factor is in comparison
  to non-ridge. Smaller values of $\sigma_j$ give more shrinkage.
  
  But @eq-ridgefitted not only gives us insight as to the *amount* of
  shrinkage but also its *direction*. Essentially, the $u_i$ are the
  Principal Components of our data, as follows.
  
  A Your Turn problem in @sec-covar has the reader derive the relation
  
  $$
  Cov(X) = E(X X') - (EX) (EX)'
  $$
  
  If we center and scale our variables, this becomes
  
  $$
  Cov(X) = E(X X') 
  $$
  
  The sample-data based estimate is 
  
  $$
  \frac{1}{n} AA'
  $$
  
  But we found earlier that the eigenvectors of $AA'$ are the $u_j$!
  
  So, we have:
  
  > Ridge shrinks the vector of fitted "Y" values more in the directions of
  > the Principal Components of our data.

## SVD As a Basis for the Four Fundamental Subspaces

Recall the four subspaces discussed in @sec-fourspaces. We can quickly
obtain much information about them from the SVD.

Let $u_i$ and $v_i$ denote the colums of $U$ and $V$. Then:

* $u_1,...,u_r$ is an orthonormal basis for $\mathcal{C}(A)$

* $u_{r+1},...,u_m$ is an orthonormal basis for $\mathcal{R}(A)$

* $v_1,...,v_r$ is an orthonormal basis for $\mathcal{R}(A)$

* $v_{r+1},...,v_n$ is an orthonormal basis for $\mathcal{N}(A)$

## Your Turn

❄️  **Your Turn:** Write an R function will call form

``` r
getLowRank(A,lowrank)
```

that uses SVD to find the best approximation to the matrix **A** of rank
**lowrank**.

❄️  **Your Turn:** Our equation @eq-lastRow1s resulted from replacing
one row (the last, but could have been any) by all 1s, with a
corresponding change on the right-hand side. But now that we have
pseudoinverses at our disposal, we could *add* a row rather than
*replacing* a row:

$$
\left (
\begin{array}{r}
P \\
1 \\
\end{array}
\right ) \nu =
\left (
\begin{array}{r}
0 \\
1 \\
\end{array}
\right ) 
$$

where 1 on the left side means a row of 1s, 0 on the right side is a
column of 0s, and 1 on the right side means the scalar 1.

Write an R function with call form

``` r
findNuPseudoInv(p)
```

to implement this approach. Here **p** is a Markov transition matrix.
Try your code on a couple of examples.

❄️  **Your Turn:** Show that in the SVD factorization, The columns
of $U$ are the eigenvectors of $AA'$. Hint: First show that any column
of $U$ is an eigenvector of $AA'$.
