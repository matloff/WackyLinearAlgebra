
```{r} 
#| include: false
library(dsld)
library(qeML)
```

{{< pagebreak >}}

# Singular Value Decomposition

::: {.callout-note}

## Goals of this chapter:

We've seen the notion of eigenanalysis for square matrices. Well, it
turns out this can be extended usefully for nonsquare matrices, via
*Singular Value Decomposition* (SVD), the topic of this chapter.

::: 



```{r} 
#| include: false
library(dsld)
library(qeML)
```

## Basic Idea

Here is what we are aiming for.

::: {.callout-note}

## Our target relation:

Given a (very possibly nonsquare) matrix $A$, we wish to find orthogonal
matrices $U$ and $V$, and a diagonal matrix $\Sigma$ whose nonzero
elements are positive, such that

$$
A = U \Sigma V'
$$

Note that, by matrix partitioning, this also says that 

$$
A = \sum_{i=1}^n \sigma_i u_i v_i'
$$ {#eq-aissumuivi}

:::

## Solution

There are various derivations, e.g. along the lines of @sec-firstPC (one
maximizes the quantity $U A V'$), but let's go quickly to the answer:  

1. Compute the eigenvalues and eigenvectors of $A'A$. 

2. Since $A'A$ is symmetric and positive-semidefinite, its eigenvalues 
   will be nonnegative and its length-1 eigenvectors will be orthogonal. 

3. Set $\Sigma$ to $diag(\sqrt{\sigma_1},...,\sqrt{\sigma_r})$, the
   nonincreasing list of positive eigenvalues. Set the columns of $V$ to
   the corresponding eigenvectors (recall @sec-diagonalizable). (We will
   assume that all the $u_i$ are positive; the proof needs a slight
   modification for the general case.)

4. Set the columns of $U$ as

$$
u_i = \frac{1}{\sigma_i} v_i
$$

Note that if the dimensions of $A$ are $m \textrm{ x } n$, then $U$,
$\Sigma$ and $V$ will be of dimensions $m \textrm{ x } n$,  $n \textrm{
x } n$  and $n \textrm{ x } n$, respectively. 

## Checking the Formula

Are the two matrices orthogonal. and does $U \Sigma V'$ really work out
to be $A$?

* $U$ is orthogonal.

    Since $V$ arises eigenvectors of the symmetric matrix $A'A$, it is an
    orthogonal matrix. Then
    
    $$
    \sigma_i^2 u_i'u_i =  (A v_i)' A v_i = \sigma^2 v_i'v_i = \sigma_i^2
    $$
    
    since $V$ is orthogonal.  Thus the columns of $U$ are of length 1.
    
    Replacing the second $u_i$ and second $v_i$ above by
    $u_j$ and $v_j$, we find that the columns of $U$ are orthogonal.
    
    Thus the matrix $U$ is orthogonal.

* The product evaluates ot $A$, as claimed.

    Using matrix partitioning, we have
    
    $$
    U \Sigma = (\sigma_1 u_1 | ... | \sigma_n u_n)
    $$
    
    So, again using partitioning,
    
    $$
    U \Sigma V' = (\sigma_1 u_1 | ... | \sigma_n u_n) 
     \left (
     \begin{array}{r}
     v_1' \\
     ... \\
     v_n'  \\
     \end{array}
     \right )
    = \sum_{i=1}^n \sigma_i u_i v_i'  
    $$
    
    That last expression is equal to $A$, by @eq-aissumuivi.

## The Data Scientist's Swiss Army Knife

There are a great many pieces of information and sources of tools packed
into this innocuous-looking factorization.  Let's take a look at some:

### Rank of $A$

This is the number of nonzero values in $\Sigma$. This follows from
@thm-ranknotchange, since $U$ and $V$, as orthogonal matrices, are invertible.

It should be noted, though, that if there are some really tiny elements
there, one might also entertain the thought that the true rank is less
than this size, as these tiny values may be due to roundoff error from 0.

### SVD as matrix pseudoinverse

Recall the example in @sec-censusrref. We could not have dummy-variable
columns for both male and female, as their sum would be a column of all
1s, in addition to a column the X data matrix already had. The three columns
would then have a nonzero linear combination that evaluates to the 0
vector.  Then in @eq-linregformula, $A$ (i.e. X) would not be of full
rank, and $(A'A)^{-1}$ would not exist.

And yet the equation from which that comes,

$$
A'A b = A'S
$${#eq-overdetermined}

is still valid. We could, as in that example, remove one of the gender
columns, thus solving the problem of less than full rank, but the use of
*pseudoinverses* (also known as *generalized inverses*) solves the
problem directly. If $A$ has hundreds or thousands of columns, say, the
use of pseudoinverses may be more convenient.

We omit the formal definition of pseudoinverses and their properties. We
will note the use of the latter as the need arises. For now, we state
without proof that:

* One of the most famous forms of pseudoinverse, Moore-Penrose, is 
  based on SVD.  Given the SVD of a matrix $M$, 

  $$
  M = U_M \Sigma_M V_{M}'
  $$

  its Moore-Penrose inverse, denoted by $M^{-1}$ is 

  $$
  M^{-} = V_M \Sigma_M^{-} U_{M}'
  $$

  where $\Sigma_M^{-}$ is the diagonal matrix obtained form $\Sigma_M$
  by replacing each nonzero element by its reciprocal.

* The Moore-Penrose solution of $Mz = w$ for vectors $z$ and $w$, is
  $M^{-} w$.

By the way, the R function **MASS::ginv** performs the necessary
computation for us; we need not call **svd()**.

### SVD in linear models

Now apply this to our linear model problem @eq-overdetermined. The
claim is that 

$$
b = A^{-} S = V \Sigma^{-} U' S
$$ 

solves the equation. [As in @sec-matalg, where we found that the inverse of
a product is the reverse product of the inverses, the same holds for
pseudoinverses.]{.column-margin} Let's check (warning: this will be
messy):

$$
\begin{align}
A'A b &= (U \Sigma V')' (U \Sigma V') (U \Sigma V')^{-} S \\
&= (V \Sigma U') (U \Sigma V') (V \Sigma^{-} U') S \\
&= V \Sigma^2 V'V \Sigma^{-} U' S \\
&= V \Sigma^{-} U' S
\end{align}
$$

But that is exactly the expression we found for $A^{-1}S$ above.

$\blacksquare$

## Example: Census Data 

```{r}
library(qeML)
data(svcensus)
# have only 1 categorical/dichotomous variable, for simple example
head(svcensus)
svc <- svcensus[,-c(2,3)]
svc <- factorsToDummies(svc)
head(svc)
x <- cbind(1,svc[,-2])
head(x)
xminus <- MASS::ginv(x)
bhat <- xminus %*% svc[,2]
bhat
lm(wageinc ~ .,svcensus[,-c(2,3)])$coef
```

The two approaches are consistent with each other (though internally
they are solving slightly different problems). Note that

$$
-13361.634-(-2660.821) = -10700.81
$$

### The Fundamental Theorem of Linear Algebra

One can define four fundamental subspaces for any matrix $A$:

* $colspace(A)$: $\{Ax\}$ (all linear combinations of columns of $A$)

* $rowspace(A)$: $\{x'A\}$ (all linear combinations of rows of $A$)

* $nullspace(A)$: $\{x: Ax = 0\}$

* $leftnullspace(A)$: $\{x: x'A = 0\}$ (all linear combinations of rows of $A$)

These subspaces have various properties, which are easily verified via
SVD. @eq-aissumuivi will come in very handy. We first find bases for
these subspaces.

* Write

$$
Ax = \sum_{i=1}^n \sigma_i u_i v_i' x 
$$

Note that $v_i'x$ is a scalar, so we have

$$
Ax = \sum_{i=1}^n [\sigma_i (v_i' x)] u_i  
$$

Since $\sigma_i = 0$ for $i > r$, we now have a characterization of one
of the four subspaces:

<blockquote>

The column space of $A$ has dimension $r$, with basis $u_1,...[_r]()$.

</blockquote>

### SVD as the minimum-norm solution

In an overdetermined linear system such as @eq-overdetermined, there are
many solutions. However, an advantage of Moore-Penrose is that it gives
us the *minimum norm* solution. We'll discuss the significance of this
shortly, but let's prove it first.

::: {#thm-shortest}

## The Moore-Penrose Solution Is Min-Norm

\newline 
Of all solutions $b$ to

$$
A'A b = A'S
$${#eq-linsyst}

the Moore-Penrose solution

$$
b = V \Sigma^{-} U' S
$$

minimizes $||b||$.

:::

::: {.proof}

Partition $U$ into $(U_1 | U_2)$, its first $r$ columns and the last
$n-r$ columns. Do the same for $V$, so that

$$
V' =
 \left (
 \begin{array}{r}
 V_1' \\
 V_2' \\
 \end{array}
 \right )
$$

Multiply @eq-linsyst on the left by $U'$

:::

### SVD as the best low-rank approximation

## Your Turn

❄️  **Your Turn:** Show that in the SVD factorization, $U$ consists of
the eigenvectors of $AA'$.

