
```{r} 
#| include: false
library(dsld)
library(qeML)
```

{{< pagebreak >}}

# Singular Value Decomposition

::: {.callout-note}

## Goals of this chapter:

We've seen the notion of eigenanalysis for square matrices. Well, it
turns out this can be extended usefully for nonsquare matrices, via
*Singular Value Decomposition* (SVD), the topic of this chapter.

::: 



```{r} 
#| include: false
library(dsld)
library(qeML)
```

## Basic Idea

Here is what we are aiming for.

::: {.callout-note}

## Our target relation:

Given an $m \times n$ matrix $A$, we wish to find orthogonal matrices
$U$ and $V$, and a matrix $\Sigma = diag(\sigma_1,...,\sigma_n)$ whose
nonzero elements are positive, such that

$$
A = U \Sigma V'
$${#eq-svd}

By convention the ordering of columns is set so that the $\sigma_i$
occur in nonincreasing order.

Note that, by matrix partitioning, @eq-svd also says that 

$$
A = \sum_{i=1}^n \sigma_i u_i v_i'
$${#eq-aissumuivi}

:::

Note the dimensions: 

* $U$ is $m \times n$

* $V$ is $n \times n$

* $\Sigma$ is $n \times n$

Let $r$ denote the rank of $A$. Recall that $r \leq \min(m,n)$. It will
turn out that $\sigma_i = 0$ for $i=r+1,...,n$.

## Solution

There are various derivations, e.g. along the lines of @sec-firstPC (one
maximizes the quantity $U A V'$), but let's go quickly to the answer:  

1. Compute the eigenvalues $\sigma_1,...,\sigma_n$
and eigenvectors $q_1,...,q_n$ of $A'A$. 

2. Since $A'A$ is symmetric and positive-semidefinite, its eigenvalues 
   will be nonnegative and its length-1 eigenvectors will be orthogonal. 

3. Set $\Sigma$ to $diag(\sqrt{\sigma_1},...,\sqrt{\sigma_n})$, the
   nonincreasing list of eigenvalues. Set the columns of $V$ to
   the corresponding eigenvectors (recall @sec-diagonalizable). 

4. Set 

$$
u_i = \frac{1}{\sigma_i} Av_i, ~ i=1,...,r
$$

Using the Gram-Schmidt Method (@sec-gramschmidt), we can compute (if $r
< m$ necessitates it) vectors $u_{r+1},...u_m$ so that $u_1,...,u_m$ is
an orthonormal basis for $\mathcal{R}^m$.  Set $U$, decribed in
partitioning terms: to 

$$
U = (u_1|...|u_m)
$$

## Checking the Formula

Are $U$ and $V$ really orthogonal, and does $U \Sigma V'$ really work out
to be $A$?

* $U$ is orthogonal.

  This follows immediately from the fact that the columns of $U$ are an
  orthonormal basis from $\mathcal{R}^m$.

* $V$ is orthogonal, by @sec-eigortho.

* The product evaluates to $A$, as claimed.

    Using matrix partitioning, we have
    
    $$
    U \Sigma = (\sigma_1 u_1 | ... | \sigma_n u_n)
    $$
    
    So, again using partitioning,
    
    $$
    U \Sigma V' = (\sigma_1 u_1 | ... | \sigma_n u_n) 
     \left (
     \begin{array}{r}
     v_1' \\
     ... \\
     v_n'  \\
     \end{array}
     \right )
    = \sum_{i=1}^n \sigma_i u_i v_i'  
    $$
    
    That last expression is equal to $A$, by @eq-aissumuivi.

## Uniqueness

The SVD can be shown to be unique, if there are no repeated eigenvalues.
If the latter exist, one can permute the corresponding columns of $U$
and $V$, but otherwise uniqueness holds.

## The Fundamental Theorem of Linear Algebra

One can define four fundamental subspaces for any matrix $A$. We will
need the following:

* $\textrm{row space}(A), \mathcal{R}(A)$: $\{x'A\}~~$ (all linear
  combinations of rows of $A$)

* $\textrm{column space}(A), \mathcal{C}(A)$: $\{Ax\}~~$ (all linear
  combinations of columns of $A$)

* $\textrm{null space}(A), \mathcal{N}(A)$: $\{x: Ax = 0\}$

* $\textrm{left null space}(A) = \mathcal{N}(A')$: $\{x: x'A = 0\}$ 

These subspaces have various properties, which may be verified via
SVD. @eq-aissumuivi will come in very handy. 

Consider the column space first. From @eq-aissumuivi write

$$
Ax = \sum_{i=1}^n \sigma_i u_i v_i' x 
$${#eq-Ax}
 
Note that $v_i'x$ is a scalar, so we have

$$ 
\begin{aligned}
Ax &= \sum_{i=1}^n [\sigma_i (v_i' x)] u_i \\
&= \sum_{i=1}^r [\sigma_i (v_i' x)] u_i 
\end{aligned}
$${#eq-Ax1}

So 

$$
\mathcal{C}(A) \subset \textrm{ span}(u_1,...,u_r)
$${#eq-colsubset} 

Now, is the left side equal to all of the right side? Say a member of
the right side is $c_1 u_1+...+c_r u_r$. Then, using the fact that 
$\{v_1....,v_r\}$ is an orthonormal set, take $x$ to be

$$
x = \sum_{j=1}^r \frac{c_j}{\sigma_j} v_j
$$

Then for each $i$, we have

$$
\sigma_i v_i'x =c_i
$$

and 

$$
Ax = \sum_{i=1}^r c_i u_i
$$

as desired, so @eq-colsubset is actually an equality.

We have thus characterized one of the four fundamental subspaces:

<blockquote>

$\mathcal{C}(A)$ has dimension $r$, with basis $u_1,...,u_r$.

</blockquote>

Now look at the null space.  $Ax = 0$ means that 
the vector $x$ is orthogonal to each row of $A$. Thus

$$
\begin{aligned}
\mathcal{N}(A) &= \mathcal{R}(A)^{\perp} \\
&= \mathcal{C}(A')^{\perp}
\end{aligned}
$$

We already found above a characterization of the column space of a
matrix, which we now apply to $A'$: $\mathcal{C}(A')$ has dimension $r$,
with basis $v_1,...,v_r$. Applying $\perp$, and noting that
orthogonality of $v_1,...,v_n$, we can now characterize a second, and
even a third,  of the four subspaces:

<blockquote>

$\mathcal{N}(A)$ is of dimension $n-r$, with basis 
$v_{r+1},...,v_n$.

$\mathcal{R}(A)$ is of dimension $r$, with basis 
$v_1,...,v_r$.

</blockquote>

We leave the fourth subspace as a Your Turn problem.

## The Data Scientist's Swiss Army Knife

There are a great many pieces of information and sources of tools packed
into this innocuous-looking factorization.  Let's take a look at some:

### Rank of $A$

This is the number of nonzero values in $\Sigma$. This follows from
@thm-ranknotchange, since $U$ and $V$, as orthogonal matrices, are invertible.

It should be noted, though, that if there are some really tiny elements
there, one might also entertain the thought that the true rank is less
than this size, as these tiny values may be due to roundoff error from 0.

### SVD as matrix pseudoinverse

Recall the example in @sec-censusrref. We could not have dummy-variable
columns for both male and female, as their sum would be a column of all
1s, in addition to a column the X data matrix already had. The three columns
would then have a nonzero linear combination that evaluates to the 0
vector.  Then in @eq-linregformula, $A$ (i.e. X) would not be of full
rank, and $(A'A)^{-1}$ would not exist.

And yet the equation from which that comes,

$$
A'A b = A'S
$${#eq-overdetermined}

is still valid. We could, as in that example, remove one of the gender
columns, thus solving the problem of less than full rank, but the use of
*pseudoinverses* (also known as *generalized inverses*) solves the
problem directly. If $A$ has hundreds or thousands of columns, say, the
use of pseudoinverses may be more convenient.

We omit the formal definition of pseudoinverses and their properties. We
will note the use of the latter as the need arises. For now, we state
without proof that:

* One of the most famous forms of pseudoinverse, Moore-Penrose, is 
  based on SVD.  Given the SVD of a matrix $M$, 

  $$
  M = U_M \Sigma_M V_{M}'
  $$

  its Moore-Penrose inverse, denoted by $M^{-1}$ is 

  $$
  M^{-} = V_M \Sigma_M^{-} U_{M}'
  $$

  where $\Sigma_M^{-}$ is the diagonal matrix obtained form $\Sigma_M$
  by replacing each nonzero element by its reciprocal.

* The Moore-Penrose solution of $Mz = w$ for vectors $z$ and $w$, is
  $M^{-} w$.

By the way, the R function **MASS::ginv** performs the necessary
computation for us; we need not call **svd()**.

### SVD in linear models

Now apply this to our linear model problem @eq-overdetermined. The
claim is that 

$$
b = A^{-} S = V \Sigma^{-} U' S
$$ 

solves the equation. [As in @sec-matalg, where we found that the inverse of
a product is the reverse product of the inverses, the same holds for
pseudoinverses.]{.column-margin} Let's check (warning: this will be
messy):

$$
\begin{aligned}
A'A b &= (U \Sigma V')' (U \Sigma V') (U \Sigma V')^{-} S \\
&= (V \Sigma U') (U \Sigma V') (V \Sigma^{-} U') S \\
&= V \Sigma^2 V'V \Sigma^{-} U' S \\
&= V \Sigma^{-} U' S
\end{aligned}
$$

But that is exactly the expression we found for $A^{-1}S$ above.

$\square$

## Example: Census Data 

```{r}
library(qeML)
data(svcensus)
# have only 1 categorical/dichotomous variable, for simple example
head(svcensus)
svc <- svcensus[,-c(2,3)]
svc <- factorsToDummies(svc)
head(svc)
x <- cbind(1,svc[,-2])
head(x)
xminus <- MASS::ginv(x)
bhat <- xminus %*% svc[,2]
bhat
lm(wageinc ~ .,svcensus[,-c(2,3)])$coef
```

The two approaches are consistent with each other (though internally
they are solving slightly different problems). Note that

$$
-13361.634-(-2660.821) = -10700.81
$$
 
### SVD as the minimum-norm solution

In an overdetermined linear system such as @eq-overdetermined, there are
many solutions. However, an advantage of Moore-Penrose is that it gives
us the *minimum norm* solution. We'll discuss the significance of this
shortly, but let's prove it first.

::: {#thm-shortest}

## The Moore-Penrose Solution Is Min-Norm

Consider a matrix $B$ and vector $q$.  Of all solutions $x$ to

$$
Bx = q
$${#eq-linsyst}

the Moore-Penrose solution minimizes $||x||$.

:::

::: {.proof}

Again, write[Adapted from a derivation by Carlo Tomasi]{.column-margin}  

$$
B = U \Sigma V',
$$

and consider the residual sum of squares 

$$
||Bx - q||^2 = ||U (\Sigma V' x - U'q)||^2
$${#eq-RSS}

since $UU'=I$.

A Your Turn problem below says that for an orthogonal matrix $M$ and a
vector $w$, $||Mw|| = ||w||$. Thus we can remove the factor $U$
in @eq-RSS, yielding

$$
||Bx - q||^2 = ||\Sigma V' x - U'q||^2
$${#eq-RSS1}

Rename $$V'x$$ to $y$:

$$
||Bx - q||^2 = ||\Sigma y - U'q||^2
$${#eq-RSS2}

Now bring in the fact that $\sigma_i = 0$ for $i > r$, by writing
everything in partitioned fashion.  Break $U$ into $r$ and $n-r$
rows,

$$
U = 
\left (
\begin{array}{r}
U_1 \\
U_2 \\
\end{array}
\right )
$$

and partition other objects similarly:

$$
V = (V_1 | V_2),
$$

$$
\Sigma =
 \left (
 \begin{array}{rr}
 \Sigma_1 & 0 \\
 0 & 0  \\
 \end{array}
 \right ),
$$

and 

$$
y =
 \left (
 \begin{array}{r}
 y_1 \\
 y_2 \\
 \end{array}
 \right )
$$

Substituting into @eq-RSS2, we have

$$
||Bx - q||^2 
= ||
\left (
\begin{array}{r}
\Sigma_1 y_1 \\
0 \\
\end{array}
\right ) -
\left (
\begin{array}{r}
U_1q \\
U_2q \\
\end{array}
\right )
||^2
$${#eq-RSS3}

This means that $y_2$ can be anything at all, without changing the
residual sum of squares, confirming that there are infinitely many
equally-effective solutions!

But if we want $x$ to be of minimum length, we need $y$ to be of minimum
length (the shortest $y$ gives us the shortest $x = Vy$, again by the
Your Turn problem).  And,

$$
||y||^2 = ||y_2||^2 + ||y_2||^2
$$

Thus we take $y_2 = 0$.  

Now, reviewing @eq-RSS3, note that the equation

$$
\Sigma_1 y_1 = U_1 q
$$

has the solution

$$
y_1 = \Sigma_1^{-1} U_1 q
$$


$$

$$

$$
y_1 = \Sigma_1^{-1} U_1 q
$$

An interesting sidelight of this is that it means that

$$
||Bx - q||^2 
= ||
\left (
\begin{array}{r}
0  \\
U_2q \\
\end{array}
\right )
||^2,
$${#eq-RSS3}

i.e. the residual sum of squares depends only on $U_2$, not $U_1$. But
returning to our quest for the minimum-norm solution, 
we map back to $x = Vy$, and have

$$
x_1 = V_1 \Sigma_1^{-1} U_1'q
$$

which is the SVD solution! Thus the SVD solution does have minimum norm.

$\square$

:::

## Application: Explaining the Mystery of ``Double Descent"

Around 2018-2019, one of the statistics field's most deeply-held notions
was thrown off its pedestal, largely by some researchers in machine
learning. (This is arguably when the idea first became widespread, but
for earlier instances, see Marco Loog *et al*, *A brief prehistory of
double descent*, 2020,
*https://www.pnas.org/doi/10.1073/pnas.2001875117*.) And many of the
explanations given have been related to SVD.

### Motivating example: polynomial regression

To illustrate the issue, let's look at *polynomial regression*. As an
example, let's look once again at our **svcensus** dataset.

```{r}
head(svcensus)
```

To keep things simple, let's predict wage income from just age.

```{r}
lm(wageinc ~ age,svcensus)$coef
```

That model has mean income as a linear function of age. But we might try
a quadratic function, for two reasons:

* Linearity is an idealization. In practice, most functions are
  nonlinear, and though a linear approximation may be adequate, we might
  do better with a more complex model, such as quadratic, cubic and so
  on.

* Age discrimination is rampant in Silicon Valley, with the effect that
  at the older end of the demographic, mean income may actually decline.
  A linear model cannot reflect that but a quadratic model can. 

So, how would we do this?

```{r}
svc1 <- svcensus[c('age','wageinc')]
head(svc1)
svc1$age2 <- svc1$age^2
head(svc1)
```

So now we have the model

mean income = $\beta_0$ + $beta_1$ age + $beta_1$ age$^2$

Note carefully that *we still have a linear model*, meaning that it is
linear the $\beta_i$. If say we multiply all the $\beta_i$ by 2, the
value of the above expression will be doubled. Or, viewed another way,
our calculation of the $\beta_i$ will still use the same calculations as
in @eq-betahat.

So, let's run the model:

```{r}
lm(wageinc ~ .,svc1)$coef
```

As a side comment, note that If our above hunch about the effects of age
discrimination is corrected, the coefficient of the squared term should
be negative, which turns out to be the case.

### Polynomial regression in the context of overfitting

Recall that a well-known rule of thumb is that the number $p$ of
predictors should be less than $\sqrt{n}$, where $n$ is the number of
data points. Let's abandon this and see what happens.

In our quadratic fit above, we had $p = 2$ predictors. Since the Census
data has more than 20,000 rows, this is not a problem in terms of the
rule of thumb at all. But what if $n = 3$?

Think first of a linear model for mean income as a function of age. Now
we have $p = 1$. Suppose $n = 2$, i.e.\ we have just two data points.
The result would be a straight line going through the two points
exactly. The sum of squared prediction errors, @eq-matrixss, also known
as the *residual sum of squares* (RSS), would equal 0.

But...a linear model with $n = 3$ would be ovefit. With $n = 2$, we
already got a 0 RSS, so an extra data point would seem to be useless.
Moreover, it can be shown that the inverse in @eq-linregformula would
not exist. We could still use SVD to get a solution in this case, but
that was viewed as useless.

In the quadratic setting with $n = 3$, we are fitting a
quadratic curve through 3 data points, and it will be an exact fit,
going through each point exactly. But $n = 4$ would overfit.

For a polynomial model of degree $d$, $n = d+1$ gives a perfect fit, and
$n > d+1$ would overfit.

### The classic  and modern views

Let $\kappa$ denote the complexity of a model. In the case of a linear
model, $\kappa$ would be the number of predictor columns in our dataset,
and there are ways of defining it for other methods.

We might try several values of $\kappa$, as we did in the table
in @eq-nyc, and then choose the one with smallest MAPE or other accuracy
measure.

Before 2018-2019, the view was:

<blockquote>

In plotting MAPE against $\kappa$, the curve will generally be roughly
U-shaped, up to the point at which $\kappa$ gives us a 0 value of RSS in
the training set. That value of $\kappa$, called the *interpolation*
point, will give us "perfect" prediction in the training set, but very
poor prediction in the test set, which is what counts.

We choose the value of $\kappa$ at which the curve is
lowest. There is no point in trying values of $\kappa$ past the
interpolation point.

</blockquote>

This was taken for granted throughout the statistics and machine learning
fields. But around 2018-2019, machine learning engineers were routinely
analyzing dataset of extraordinarily large sizes, using unprecedently
large values of $\kappa$. And they discovered that, bizarrely, as
$\kappa$ moved past the interpolation point, the curve often went
*down*--the second "descent"--often tracing its own second U-shape.
And most significantly, the low point of the second U was sometimes
below that of the first U! Overfitting--grossly so--may pay off!

So the modern view is:

<blockquote>

The first U-shaped curve is as in the classic view. But the curve should
be plotted past $\kappa$, and the overall minimum may be in that second
U.

</blockquote>

This is a rare sea change in classic quantitative analysis.

### SVD as the best low-rank approximation

## Your Turn

❄️  **Your Turn:** Show that in the SVD factorization, $U$ consists of
the eigenvectors of $AA'$.

❄️  **Your Turn:** Find a characterization of the left null space of a
matrix.

❄️  **Your Turn:** Write an R function with a matrix as its sole argument,
with return value consisting of an R list containing four matrices,
representing the four fundamental subspaces of the input argument.
Each matrix will consist of columns equal to a basis for the given
subspace.

❄️  **Your Turn:**

Show that for an orthogonal matrix $M$ and a vector $w$, $||Mw|| =
||w||$.

