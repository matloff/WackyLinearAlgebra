
```{r} 
#| include: false
library(dsld)
library(qeML)
```

{{< pagebreak >}}

# Singular Value Decomposition

::: {.callout-note}

## Goals of this chapter:

We've seen the notion of eigenanalysis for square matrices. Well, it
turns out this can be extended usefully for nonsquare matrices, via
*Singular Value Decomposition* (SVD), the topic of this chapter,
with important applications.

::: 



```{r} 
#| include: false
library(dsld)
library(qeML)
```

## Basic Idea

Here is what we are aiming for.

::: {.callout-note}

## Our target relation:

Given an $m \times n$ matrix $A$, we wish to find orthogonal matrices
$U$ and $V$, and a matrix $\Sigma = diag(\sigma_1,...,\sigma_n)$ whose
nonzero elements are positive, such that

$$
A = U \Sigma V'
$${#eq-svd}

By convention the ordering of columns is set so that the $\sigma_i$
occur in nonincreasing order.

Note that, by matrix partitioning, @eq-svd also says that 

$$
A = \sum_{i=1}^n \sigma_i u_i v_i'
$${#eq-aissumuivi}

where $u_i$ and $v_i$ are the $i^{th}$ rows of $U$ and $V$.

:::

Note the dimensions: 

* $U$ is $m \times n$

* $V$ is $n \times n$

* $\Sigma$ is $n \times n$

Let $r$ denote the rank of $A$. Recall that $r \leq \min(m,n)$. It will
turn out that $\sigma_i = 0$ for $i=r+1,...,n$.

## Application: Recommender Systems

Recall @sec-recsys, which began with movie ratings data. We would like
to predict the rating some particular user would give to some particular
movie. It will turn out that we can set up SVD in such a way that $U$
contains movie data and $V$ contains user data. We will explore this and
other methods in @sec-recsys.

## Application: Text Classification

An oft-used example is that in which we have a collection of newspaper
articles that we wish to categorize, say politics, sports, health,
finance and so on. Say one of the articles includes the word *bonds*; is
it referring to financial instruments, family relations, former baseball
star Barry Bonds etc.?

In spite of today's dazzling array of Large Language Models, a simple problem 
like this may be better solved using straightforward methods, such as
SVD. We set up a *document-term matrix*, with element $(i,j)$ being 1 or
0, according to whether document $i$ contains word $j$. Applying SVD to
this matrix, we have a setting with similar appeal to the recommender 
systems example above, in which $U$ contains our data on documents and
$V$ does the same for words.

## Application: Dimension Reduction

Recall that in the matrix $\Sigma$, the eigenvalues are arranged in
decreasing order, possibly followed by 0s. This suggests that we can
achieve dimension reduction by replacing the smaller eigenvalues by 0s,
with corresponding adjustments to the columns of $U$ and $V$. We will
discuss this below in @sec-svddimred. 

## The Four Fundamental Subspaces of a Matrix

Before going to SVD, let us first define four fundamental subspaces for
any matrix $A$: 

* $\textrm{row space}(A), \mathcal{R}(A)$: $\{x'A\}~~$ (all linear
  combinations of rows of $A$)

* $\textrm{column space}(A), \mathcal{C}(A)$: $\{Ax\}~~$ (all linear
  combinations of columns of $A$)

* $\textrm{null space}(A), \mathcal{N}(A)$: $\{x: Ax = 0\}$

* $\textrm{left null space}(A) = \mathcal{N}(A')$: $\{x: x'A = 0\}$ 

The null space is also called the *kernel* of $A$, viewing the matrix as
a function from $\mathcal{R}^n$ to $\mathcal{R}^n$; what vectors are
mapped to 0?

These subspaces will play a prominent role in this chapter. Say $A$ is
of size $m \textrm{ x } n$. Use 'dim()' to indicate vector space
dimension, and 'rank()' for matrix rank.  

::: {#thm-colspacerank}

$$
rank(A) = dim(\mathcal{C}(A))
$$

:::

::: {.proof}

Let $z_1,...,z_r$ be a maximal linearly independent set of columns of
$A$, where $r$ is the rank of that matrix. Then this set is in
$\mathcal{C}(A)$, and in fact must span that subspace. If not, there
would be some vector $x$ outside the span of $z_1,...,z_r$, i.e. no
linear combination of those vectors would equal $x$. Then
$z_1,...,z_r,x$ would be a linearly independent set, contradicting the
maximal nature of the $z_i$.

In other words, the $z_i$ form a basis for $\mathcal{C}(A)$, and thus
the dimension of that subspace is $r$.

$\square$

:::

::: {#thm-colnullspaces}

The $\mathcal{C}(A) \textrm{ and } \mathcal{N}(A)$ subspaces
are closely related:

$$
\mathcal{C}(A')^{\perp} = \mathcal{N}(A)
$$

and

$$
dim(\mathcal{C}(A')) + dim(\mathcal{N}(A)) = m
$$

:::

::: {.proof}

Say $w$ is in $\mathcal{N}(A)$. Once again using partitioning, we have

$$
\left (
\begin{array}{r}
0 \\
... \\
0 \\
\end{array}
\right )
=
Aw
=
\left (
\begin{array}{r}
a_1 w \\
... \\
a_m w \\
\end{array}
\right )
$$

where $a_i$ is row $i$ of $A$. Thus $w$ is orthogonal to the rows of
$A$, thus to the row space of $A$. The latter is the column space of $A'$.
Thus $\mathcal{N}(A)$ is a subset of $\mathcal{C}(A')^{\perp}$. This
argument works exactly in reverse, so the first claim is established.

The second claim follows from first forming an orthonormal basis
$q_1,...,q_r$ for $\mathcal{C}(A')$, then extending it to one for all of
$\mathcal{R}^m$, $q_1,...,q_r,q_{r+1},...,m$.



$\square$
:::




## Solution to Our SVD Goal

There are various derivations, e.g. along the lines of @sec-firstPC (one
maximizes the quantity $U A V'$), but let's go quickly to the answer:  

1. Say $A$ is $p \textrm{ x } n$, so that $A'A$ will be of size $n
   \textrm{ x } n$. Let $r$ denote the rank of $A$, which will be the same
   as the rank of $A'A$ from @thm-rankapa.

2. Compute the eigenvalues $\sigma_1,...,\sigma_n$
   and eigenvectors $v_1,...,v_n$ of $A'A$. Normalize the $v_i$ by
   dividing by their lengths. (The same eigenvalues will still hold.)
   Order the $\sigma_i$ from largest to smallest, and use the same
   ordering for the $v_i$.

3. Since $A'A$ is symmetric and positive-semidefinite, its eigenvalues
   will be nonnegative and its eigenvectors $v_1,...,v_r$ will be
   orthogonal as long as $\sigma_1,...,\sigma_r$ are distinct, as is the
   case for numeric data (@sec-prob0). We will have $\sigma_{r+1} = ...
   = \sigma_n = 0$.

4. Set $\Sigma$ to $diag(\sqrt{\sigma_1},...,\sqrt{\sigma_n})$, the
   nonincreasing list of those eigenvalues. Set the first $r$ columns of
   $V$ to the corresponding eigenvectors. Use the Gram-Schmidt Method
   (see @sec-gramschmidt) to add $n-r$ more vectors. $V$ will then have
   orthonormal columns, and thus be an orthogonal matrix.

5. Set 

   $$ u_i = \frac{1}{\sigma_i} Av_i, ~ i=1,...,r $$

   The $u_i$ will be orthogonal: For $i \neq j$,

   $$
   u_i' u_j = \frac{1}{\sigma_i \sigma_j} v_i' A'A v_j =
   \frac{1}{\sigma_i \sigma_j} v_i' v_j = 0
   $$

   where we have used the facts that $v_j$ is an eigenvector of $A'A$
   and the $v_k$ are orthogonal.

   
   Using Gram-Schmidt, we can compute (if $r < m$ necessitates it)
   vectors $u_{r+1},...u_m$ so that $u_1,...,u_m$ is an orthonormal
   basis for $\mathcal{R}^m$.  Set $U$, decribed in partitioning terms:
   to 
   
   $$ U = (u_1|...|u_m) $$

## Checking the Formula

Are $U$ and $V$ really orthogonal, and does $U \Sigma V'$ really work out
to be $A$?

* $U$ and $V$ are orthogonal, by construction.

* The product evaluates to $A$, as claimed:

    Using matrix partitioning, we have
    
    $$
    U \Sigma = (\sigma_1 u_1 | ... | \sigma_n u_n)
    $$
    
    So, again using partitioning,
    
    $$
    U \Sigma V' = (\sigma_1 u_1 | ... | \sigma_n u_n) 
     \left (
     \begin{array}{r}
     v_1' \\
     ... \\
     v_n'  \\
     \end{array}
     \right )
    = \sum_{i=1}^n \sigma_i u_i v_i'  
    $$
    
    That last expression is equal to $A$, by @eq-aissumuivi.

## SVD as a basis for matrix generalized inverse

Recall the example in @sec-censusrref. We could not have dummy-variable
columns for both male and female, as their sum would be a column of all
1s, in addition to a column the X data matrix already had. The three columns
would then have a nonzero linear combination that evaluates to the 0
vector.  Then in @eq-linregformula, $A$ (i.e. X) would not be of full
rank, and $(A'A)^{-1}$ would not exist.

And yet the equation from which that comes,

$$
A'A b = A'S
$${#eq-overdetermined}

is still valid. We could, as in that example, remove one of the gender
columns, thus solving the problem of less than full rank, but the use of
*generalized inverses* solves the problem directly. If $A$ has hundreds
or thousands of columns, say, the use of generalized inverses may be more
convenient.

We will omit the formal definition of inverses and their
properties, returning to the topic in @sec-chapDD.  For now, we note the
following:

One of the most famous forms of generalized inverse, is the
Moore-Penrose *pseudoinverse*, based on SVD.  Given the SVD of a matrix
$M$, 

$$
M = U_M \Sigma_M V_{M}'
$$

its Moore-Penrose inverse, denoted by $M^{+}$ is 

$$
M^{+} = V_M \Sigma_M^{+} U_{M}'
$$

where $\Sigma_M^{+}$ is the diagonal matrix obtained form $\Sigma_M$
by replacing each nonzero element by its reciprocal.

The Moore-Penrose solution of $Mz = w$ for vectors $z$ and $w$, is
$M^{+} w$.

The R function **MASS::ginv** performs the necessary
computation for us; we need not call **svd()**.

## SVD in linear models

Now apply this to our linear model problem @eq-overdetermined. The
claim is that 

$$
b = A^{+} S = V \Sigma^{+} U' S
$$ 

solves the equation. [As in @sec-matalg, where we found that the inverse of
a product is the reverse product of the inverses, the same holds for
pseudoinverses.]{.column-margin} Let's check (warning: this will be
messy):

$$
\begin{aligned}
A'A b &= (U \Sigma V')' (U \Sigma V') (U \Sigma V')^{+} S \\
&= (V \Sigma U') (U \Sigma V') (V \Sigma^{+} U') S \\
&= V \Sigma^2 V'V \Sigma^{+} U' S \\
&= V \Sigma^{+} U' S
\end{aligned}
$$

But that is exactly the expression we found for $A^{-1}S$ above.

$\square$

## Example: Census Data 

```{r}
library(qeML)
data(svcensus)
# have only 1 categorical/dichotomous variable, for simple example
head(svcensus)
svc <- svcensus[,-c(2,3)]
svc <- factorsToDummies(svc)
head(svc)
x <- cbind(1,svc[,-2])
head(x)
xminus <- MASS::ginv(x)
bhat <- xminus %*% svc[,2]
bhat
lm(wageinc ~ .,svcensus[,-c(2,3)])$coef
```

The two approaches are consistent with each other (though internally
they are solving slightly different problems). Note that

$$
-13361.634-(-2660.821) = -10700.81
$$

## Dimension Reduction: SVD as the Best Low-Rank Approximation{#sec-svddimred}


## Your Turn

❄️  **Your Turn:** Show that in the SVD factorization, $U$ consists of
the eigenvectors of $AA'$.

❄️  **Your Turn:** Find a characterization of the left null space of a
matrix $A$.

❄️  **Your Turn:** Write an R function will call form

``` r
getLowRank(A,lowrank)
```

that uses SVD to find the best approximation to the matrix **A** of rank
**lowrank**.

❄️  **Your Turn:**  xval dim red
