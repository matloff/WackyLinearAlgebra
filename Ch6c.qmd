
```{r} 
#| include: false
library(dsld)
library(qeML)
```

{{< pagebreak >}}

# Singular Value Decomposition {#sec-svd}

::: {.callout-note}

## Goals of this chapter:

We've seen the notion of eigenanalysis for square matrices. Well, it
turns out this can be extended usefully for nonsquare matrices, via
*Singular Value Decomposition* (SVD), the topic of this chapter,
with important applications.

::: 



```{r} 
#| include: false
library(dsld)
library(qeML)
```

## Basic Idea

Here is what we are aiming for.

::: {.callout-note}

## Our target relation:

Given an $m \times n$ matrix $A$, we wish to find orthogonal matrices
$U$ and $V$, and a matrix $\Sigma = diag(\sigma_1,...,\sigma_n)$ whose
nonzero elements are positive, such that

$$
A = U \Sigma V'
$${#eq-svd}

By convention the ordering of columns is set so that the $\sigma_i$
occur in nonincreasing order.

Note that, by matrix partitioning, @eq-svd also says that 

$$
A = \sum_{i=1}^n \sigma_i u_i v_i'
$${#eq-aissumuivi}

where $u_i$ and $v_i$ are the $i^{th}$ rows of $U$ and $V$.

:::

Note the dimensions: 

* $U$ is $m \times n$

* $V$ is $n \times n$

* $\Sigma$ is $n \times n$

Let $r$ denote the rank of $A$. It will turn out that $\sigma_i = 0$ for
$i=r+1,...,n$.

## Example Applications 

### Recommender Systems

Recall @sec-recsys, which began with movie ratings data. We would like
to predict the rating some particular user would give to some particular
movie. It will turn out that we can set up SVD in such a way that $U$
contains movie data and $V$ contains user data. 

### Text Classification

An oft-used example is that in which we have a collection of newspaper
articles that we wish to categorize, say politics, sports, health,
finance and so on. Say one of the articles includes the word *bonds*; is
it referring to financial instruments, family relations, former baseball
star Barry Bonds etc.?

In spite of today's dazzling array of Large Language Models, a simple problem 
like this may be better solved using straightforward methods, such as
SVD. We set up a *document-term matrix*, with element $(i,j)$ being 1 or
0, according to whether document $i$ contains word $j$. Applying SVD to
this matrix, we have a setting with similar appeal to the recommender 
systems example above, in which $U$ contains our data on documents and
$V$ does the same for words.

### Dimension Reduction

Recall that in the matrix $\Sigma$, the eigenvalues are arranged in
decreasing order, possibly followed by 0s. This suggests that we can
achieve dimension reduction by replacing the smaller eigenvalues by 0s,
with corresponding adjustments to the columns of $U$ and $V$. We will
discuss this below in @sec-svddimred. 



## Solution to Our SVD Goal

There are various derivations, e.g. along the lines of @sec-firstPC (one
maximizes the quantity $U A V'$), but let's go quickly to the answer:  

1. Say $A$ is $p \textrm{ x } n$, so that $A'A$ will be of size $n
   \textrm{ x } n$. Let $r$ denote the rank of $A$, which will be the same
   as the rank of $A'A$ from @thm-rankapa.

2. Compute the eigenvalues $\sigma_1,...,\sigma_n$
   and eigenvectors $v_1,...,v_n$ of $A'A$. Normalize the $v_i$ by
   dividing by their lengths. (The same eigenvalues will still hold.)
   Order the $\sigma_i$ from largest to smallest, and use the same
   ordering for the $v_i$.

3. Since $A'A$ is symmetric and positive-semidefinite, its eigenvalues
   will be nonnegative and its eigenvectors $v_1,...,v_r$ will be
   orthogonal as long as $\sigma_1,...,\sigma_r$ are distinct, as is the
   case for numeric data (@sec-prob0). We will have $\sigma_{r+1} = ...
   = \sigma_n = 0$.

4. Set $\Sigma$ to $diag(\sqrt{\sigma_1},...,\sqrt{\sigma_n})$, the
   nonincreasing list of those eigenvalues. Set the first $r$ columns of
   $V$ to the corresponding eigenvectors. Use the Gram-Schmidt Method
   (see @sec-gramschmidt) to add $n-r$ more vectors. $V$ will then have
   orthonormal columns, and thus be an orthogonal matrix.

5. Set 

   $$ u_i = \frac{1}{\sigma_i} Av_i, ~ i=1,...,r $$

   The $u_i$ will be orthogonal: For $i \neq j$,

   $$
   u_i' u_j = \frac{1}{\sigma_i \sigma_j} v_i' A'A v_j =
   \frac{1}{\sigma_i \sigma_j} v_i' v_j = 0
   $$

   where we have used the facts that $v_j$ is an eigenvector of $A'A$
   and the $v_k$ are orthogonal.

   
   Using Gram-Schmidt, we can compute (if $r < m$ necessitates it)
   vectors $u_{r+1},...u_m$ so that $u_1,...,u_m$ is an orthonormal
   basis for $\mathcal{R}^m$.  Set $U$, decribed in partitioning terms:
   to 
   
   $$ U = (u_1|...|u_m) $$

## Checking the Formula

Are $U$ and $V$ really orthogonal, and does $U \Sigma V'$ really work out
to be $A$?

* $U$ and $V$ are orthogonal, by construction.

* The product evaluates to $A$, as claimed:

    Using matrix partitioning, we have
    
    $$
    U \Sigma = (\sigma_1 u_1 | ... | \sigma_n u_n)
    $$
    
    So, again using partitioning,
    
    $$
    U \Sigma V' = (\sigma_1 u_1 | ... | \sigma_n u_n) 
     \left (
     \begin{array}{r}
     v_1' \\
     ... \\
     v_n'  \\
     \end{array}
     \right )
    = \sum_{i=1}^n \sigma_i u_i v_i'  
    $$
    
    That last expression is equal to $A$, by @eq-aissumuivi.

## SVD as a basis for matrix generalized inverse

Recall the example in @sec-censusrref. We could not have dummy-variable
columns for both male and female, as their sum would be a column of all
1s, in addition to a column the X data matrix already had. The three columns
would then have a nonzero linear combination that evaluates to the 0
vector.  Then in @eq-linregformula, $A$ (i.e. X) would not be of full
rank, and $(A'A)^{-1}$ would not exist.

And yet the equation from which that comes,

$$
A'A b = A'S
$${#eq-overdetermined}  

is still valid. We could, as in that example, remove one of the gender
columns, thus solving the problem of less than full rank, but the use of
*generalized inverses* solves the problem directly. If $A$ has hundreds
or thousands of columns, say, the use of generalized inverses may be more
convenient.

We will omit the formal definition of inverses and their
properties, returning to the topic in @sec-chapDD.  For now, we note the
following:

One of the most famous forms of generalized inverse, is the
Moore-Penrose *pseudoinverse*, based on SVD.  Given the SVD of a matrix
$M$, 

$$
M = U_M \Sigma_M V_{M}'
$$

its Moore-Penrose inverse, denoted by $M^{+}$ is 

$$
M^{+} = V_M \Sigma_M^{+} U_{M}'
$$

where $\Sigma_M^{+}$ is the diagonal matrix obtained form $\Sigma_M$
by replacing each nonzero element by its reciprocal.

The Moore-Penrose solution of $Mz = w$ for vectors $z$ and $w$, is
$M^{+} w$.

The R function **MASS::ginv** performs the necessary
computation for us; we need not call **svd()**.

## SVD in linear models  

Now apply this to our linear model problem (@eq-overdetermined).
We claim that 

$$
b = A^{+} S = V \Sigma^{+} U' S
$$ 

solves the equation. (Actually if $A$ is of less than full rank, there
are infinitely many solutions, a point discussed in detail in
@sec-chapDD.)

Let's check. Before beginning, note that as in @sec-matalg, where we
found that the inverse of a product is the reverse product of the
inverses, the same holds for pseudoinverses.  Also, recall that the
orthogonal nature of $U$ and $V$ implies that $U'U = I$ and $V'V = I$.

$$
\begin{aligned}
A'A b &= (U \Sigma V')' (U \Sigma V') (U \Sigma V')^{+} S \\
&= (V \Sigma U') (U \Sigma V') (V \Sigma^{+} U') S \\
&= V \Sigma^2 V'V \Sigma^{+} U' S \\
&= V \Sigma^{+} U' S
\end{aligned}
$$

But that is exactly the expression we found for $A^{+}S$ above.

$\square$

## Example: Census Data 

```{r}
library(qeML)
data(svcensus)
# have only 1 categorical/dichotomous variable, for simple example
head(svcensus)
svc <- svcensus[,-c(2,3)]
svc <- factorsToDummies(svc)
head(svc)
x <- cbind(1,svc[,-2])
head(x)
xplus <- MASS::ginv(x)
bhat <- xplus %*% svc[,2]
bhat
lm(wageinc ~ .,svcensus[,-c(2,3)])$coef
```

The two approaches are consistent with each other (though internally
they are solving slightly different problems). Note that

$$
-13361.634-(-2660.821) = -10700.81
$$

## Dimension Reduction: SVD as the Best Low-Rank Approximation{#sec-svddimred}

## Matrix Factorization in Recommender Systems

Let $R$ denote the ratings matrix, so that the element in row $i$,
column $j$ is the rating user $i$ gives to item $j$. Note that most
elements of $A$ are unknown, and we hope to predict them with some
reasonable amount of accuracy.

Write the SVD:

$$
A = U \Sigma V' = (U \Sigma^{0.5}) (\Sigma^{0.5} V') = WH
$$

where $\Sigma^{0.5}$ is the diagonal matrix with elements
$\sqrt{\sigma_i}$.

Again, since we do not know all of $A$, we do not know any of the
matrices on the right either. We will return to this problem shortly,
but for now pretend the matrices are known.

The point is that we have factored $A$ into the product of a matrix $W$
containing information about the users' ratings and a matrix $H$
that does the same for items. Note that the row $i$, column $j$
element of $A$ is equal to $w_i h_j$, where $w_i$ is row $i$ of $W$ and
$v_j$ is column $j$ of $H$.

This suggests that the following iterative process may work:

1. Replace the unknown elements of $A$ by some temporaru values. This
   could be all 0s, say, or maybe replacing all unknown values in column
   $j$ by the mean of the intact values in that column.

2. Find $W$ and $H$ for that temporary version of $A$, our guess.

3. Use the formula $w_i h_j$ to find the unknown elements, updating
   accordingly to a new guess for $A$.

4. Go to Step 2.

We iterate until convergence.

And though SVD provided the motivation for the model $A = WH$,
we can use the model more generally, i.e. without assuming
$W = U \Sigma^{0.5}$ and $H = \Sigma^{0.5} V'$.

lme4, ALS with ridge (and mention the 2-way regul standard)

## SVD As a Basis for the Four Fundamental Subspaces

Recall the four subspaces discussed in @sec-fourspaces. We can quickly
obtain much information about them from the SVD.

Let $u_i$ and $v_i$ denote the colums of $U$ and $V$. Then:

* $u_1,...,u_r$ is an orthonormal basis for $\mathcal{C}(A)$

* $u_{r+1},...,u_m$ is an orthonormal basis for $\mathcal{R}(A)$

* $v_1,...,v_r$ is an orthonormal basis for $\mathcal{R}(A)$

* $v_{r+1},...,v_n$ is an orthonormal basis for $\mathcal{N}(A)$

## Your Turn

❄️  **Your Turn:** Show that in the SVD factorization, $U$ consists of
the eigenvectors of $AA'$.

❄️  **Your Turn:** Find a characterization of the left null space of a
matrix $A$.

❄️  **Your Turn:** Write an R function will call form

``` r
getLowRank(A,lowrank)
```

that uses SVD to find the best approximation to the matrix **A** of rank
**lowrank**.

