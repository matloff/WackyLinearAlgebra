
```{r}
#| include: false
library(dsld)
library(qeML)
```

{{< pagebreak >}}

# Linear Statistical Models {#sec-linmods}


::: {.callout-note}

## Goals of this chapter: 

Here we bring together the concepts of previous chapters by presenting
the *linear model*, one of the most fundamental techniques in 
statistics. It relies heavily on linear algebra, notably matrix inverse,
but the use of linear algebra extends far beyond that.

:::

This chapter could have been titled, say, "Optimization, Part I," since
many applications of linear algebra involve minimization or maximization
of some kind, and this chapter will involve calculus derivatives.  But
the statistical applications of linear algebra are equally important.

## Linear Regression through the Origin

Let's consider the **Nile**  dataset built-in to R on the height of the
Nile River. It is a time series, one measurement per year. We will
predict each year's value from the previous one.

```{r}
head(Nile)
n1 <- Nile[-(length(Nile))]
head(n1)
# we will need a lag-1 version of the data
n2 <- Nile[-1]
head(n2)
plot(n1,n2,cex=0.4,xlim=c(0,1400),yli=c(0,1400))
```

We would like to fit a straight line through that data point cloud. We
might have two motivations for doing this:

* The line might serve as nice summary of the data.

* More formally, let $C$ and $V$ denote the current and previous year's
  measurements. Then the model 

  $$E(C | V) = \beta V$$ 

  may be useful. Here the slope $\beta$ is an unknown value to be
  estimated from the data.  [Readers with some background in linear
  regression models should note that this assumption of a linear trend
  through the origin is the only assumption we are making here. Nothing
  on normal distributions etc.]{.column-margin}

::: {.callout-important}
## Model Validity
The great statistician George Box once said, "All models are wrong but
some are useful." All data scientists should keep this at the forefronts
of their minds.
:::

### Least squares approach{#sec-leastsquaresintro}

We wish to estimate $\beta$ from our data, which we regard as a sample
from the population/data generating process.  Denote our data by
$(C_i,V_i), i = 1,...,100$.[As noted in @sec-samplevspop, this too is
something all data scientists should keep at the forefronts of their
minds. We are always working with sample data, subject to intersample
variations. The quantity $\widehat{\beta}$ is a random
variable.]{.column-margin} Let $\widehat{\beta}$ denote our
estimate.[The "hat" notation '^' is statistical convention for "estimate
of"; $\widehat{\beta}$ is our estimate of the unknown parameter $\beta$.
Make sure to distinguish between the two!]{.column-margin} How should we
obtain it?

Pretend for a moment that we don't know, say, $C_{28}$. Using our
estimated $\beta$, our predicted value would be $\widehat{\beta}
V_{28}$.  Our squared prediction error would then be $(C_{28} - \widehat{\beta} 
W_{28})^2$.

Well, we actually do know $C_{28}$ (and the others in our data), so we
can answer the question:

<blockquote>

In our search for a good value of $\widehat{\beta}$, we can ask how well
we would predict our known data, using that candidate value of
$\widehat{\beta}$ in our data.  Our total squared prediction error would
be

$$
\sum_{i=1}^{100} [C_{i} - \widehat{\beta} V_{i} )^2
$$

A natural choice for $\widehat{\beta}$ would be the value that minimizes this
quantity.
[Why not look at the absolute value instead of
the square? The latter makes the math flow well, as will be seen
shortly.]{.column-margin}

</blockquote>

### Calculation

As noted, our choice for $\widehat{\beta}$ will be the minimizer of

$$
\sum_{i=1}^{100} (C_{i} - b V_{i})^2
$$

over all possible values of $b$. We then set $\widehat{\beta}$ to that
minimizing value of $b$.

This is a straightforward calculus problem. Setting

$$
0 = \frac{d}{db} \sum_{i=1}^{100} (C_{i} - b V_{i} )^2 =
-2 \sum_{i=1}^{100} (C_{i} - b V_{i}) V_i
$$

and solving $b$, we find that

$$
b = \frac{\sum_{i=1}^n C_i V_i}{\sum_{i=1}^nV_i^2}
$$

### R code

```{r}
lm(n2 ~ n1-1)
```



This says, "Fit the model $E(C | V) = \beta V$ to the data, with the
line constrained to pass through the origin." The constraint is
specified by the -1 term. 

We see that the estimated regression line is

$$
\widehat{E}(C | V) = 0.98 V
$$

## Linear Regression Model with Intercept Term

[In most applications, we do not assume such a constraint, and thus do
include a $\beta_0$ term in our model.]{.column-margin} Say  we do not
want to constrain the model to pass the line through the origin. Our
model is then

$$E(C | V) = \beta_0 + \beta_1 V$$

where we now have two unknown parameters to be estimated.

### Least-squares estimation, single predictor

Our sum of squared prediction errors is now

$$
\sum_{i=1}^{100} [C_{i} - (b_0 + b_1 V_{i}) ]^2
$$

This means setting two derivatives to 0 and solving. Since the
derivatives involve two different quantities to be optimized, $b_0$ 
and $b_1$, the derivatives are termed *partial*, and the $\partial$
symbol is used instead of 'd'.

\begin{align}
0 &= 
\frac{\partial}{\partial b_0}\sum_{i=1}^{100} [C_{i} - (b_0 + b_1 V_{i })]^2 \\
&=
-2 \sum_{i=1}^{100} [C_{i} - (b_0 + b_1 V_{i}) ] 
\end{align}

and

\begin{align}
0 &= 
\frac{\partial}{\partial b_1}\sum_{i=1}^{100} [C_{i} - (b_0 + b_1 V_{i})]^2 \\
&=
-2 \sum_{i=1}^{100} [C_{i} - (b_0 + b_1 V_{i})] V_i 
\end{align}

We could then solve for the $b_i$, but let's go straight to the general
case.

## Least-Squares Estimation, General Number of Predictors

### Nile example  

As we have seen, systems of linear equations are natural applications of
linear algebra.  The equations setting the derivatives to 0 can be
written in matrix terms as

$$
 \left (
 \begin{array}{r}
 \sum_{i=1}^n C_i \\
 \sum_{i=1}^n C_i V_i \\
 \end{array}
 \right ) =
 \left (
 \begin{array}{rr}
 100 & \sum_{i=1}^n V_i \\
 \sum_{i=1}^n V_i & \sum_{i=1}^n V_i^2 \\
 \end{array}
 \right ) 
 \left (
 \begin{array}{r}
 b_0 \\
 b_1 \\
 \end{array}
 \right )
$${#eq-nilebetahat}

Actually, that matrix equation can be derived more easily by using
matrices to begin with:

Define $S$ and $T$:

$$
S = 
 \left (
 \begin{array}{r}
 C_1 \\
 C_2 \\
 ... \\
 C_{100} \\
 \end{array}
 \right )
$$

and

$$
T = 
 \left (
 \begin{array}{r}
 V_1 \\
 V_2 \\
 ... \\
 V_{100} \\
 \end{array}
 \right )
$$

Then our linear assumption, $E(C | V) = \beta _0 + \beta_1 V$, applied to
$S$ and $T$, is

$$
E(S | T) =
A \beta
$$

where

$$
A =  
 \left (
 \begin{array}{rr}
 1 & V_1 \\
 1 & V_2 \\
 ... & ... \\
 1 & V_{100} \\
 \end{array}
 \right )
$$ {#eq-Aex}

and $\beta = (\beta_0,\beta_1)'.$

Our predicted error vector, using our candidate estimate $b$ of $\beta$,
is very simply expressed:

$$
S - Ab
$$

And since for any column vector $u$, the sum of its squared elements is

$$
u'u
$$

[Note: Our use of the letters 'A' and 'S' here is nonstandard; 'X' and
'Y' are the typical choices. However, a major problem is that 'X' and
'Y' are then used for other separate quantities, causing confusion,
hence our use of 'A' and 'S'. We will sometimes use X and Y with
quotation marks, e.g. "X" and "Y" in informal comments, to indicate
our predictor variables and the quantity to be predicted. For instance,
'Say we have "X" and "Y" as human height and weight' means 'Say we wish
to predict human weight from height.']{.column-margin}

our sum of squared prediction errors is

$$
(S - Ab)'(S - Ab)
$${#eq-matrixss}

Now how we will minimize that matrix expression with respect to the
vector $b$? That is the subject of the next sections.

### General setting{#sec-genset}

We consider the general linear regression setting in which we have
$p$ predictor variables $V^{(1)},...,V^{(p)}$ with

$$
E(C | V^{(1)} = t_1 ,..., V^{(p)} = t_p) =
\beta_0 +
\beta_1 t_1 + ... +
\beta_p t_p
$$

Say we have $n$ data points. Denote the value of $V^{(j)}$ in the $i^{th}$
data point by $a_{i,j}$, and write

$$
A =
\left (
\begin{array}{rrrr}
1 & a_{11} & ... & a_{1p} \\
... & ... & ... & ... \\
1 & a_{n1} & ... & a_{np} \\
\end{array}
\right )
$$

and 

$$
E(S|A) = A \beta
$$

Note the column of 1s, needed to pick up the $\beta_0$ term.

As before $S$ is the vector of associated "Y" values.

### Matrix derivatives{#sec-matrixderivs}

The (column) vector of partial derivatives of a scalar quantity is
called the *gradient* of that quantity. For instance, with 

$$
u = 2x + 3y^2 + xy
$$

we have that its gradient is

$$
 \left (
 \begin{array}{r}
 2 + y \\
 6y + x \\
 \end{array}
 \right )
$$

With care, we can compute gradients entirely at the matrix level, using
easily derivable properties, without ever resorting to returning to the
scalar expressions. Let's apply them to the case at hand in the last
section, 

$$
(S - Ab)'(S - Ab)
$${#eq-rss}

### Differentiation purely in matrix terms{#sec-linregformula}

It can be shown that for a column vector $a$,

$$
\frac{d}{da} a'a = 2a
$${#eq-dda}

@eq-rss is indeed of the form $a'a$, with $a = S - Ab$, but the problem
here is that $a$ in turn is a function of $b$, This calls for the Chain
Rule, which does exist at the matrix level:

For example if $u = Mv + w$, with $M$ and $w$ constants (i.e. not
functions of $v$, then

$$
\frac{d}{dv} u'u = 2M'u
$$[We must keep in mind that we are working with vectors and
matrices, so that $M'u$, say $r \times s$ times $s \times 1$,
is conformable matrix multiplication.]{.column-margin}

In our case at hand, we have $M = -A$ and $w = S$, so that

$$
\frac{d}{db} [(S - Ab)'(S - Ab)] = -2 A'(S - Ab)
$$

So, set

$$
0 = A'(S - Ab) = A'S - A'A b
$$

yield our minimizing $b$:

$$
\widehat{\beta} = (A'A)^{-1} A'S
$${#eq-linregformula}

providing the inverse exists (more on this in later chapters).

Let's check this with the Nile example:

```{r}
A <- cbind(1,n1)
S <- n2
Ap <- t(A)  # R matrix transpose
solve(Ap %*% A) %*% Ap %*% S  # R matrix inverse
# check via R
lm(n2 ~ n1)
```

Also,

```{r}
det(Ap %*% A) 
```

Nonzero! So $(A'A)^{-1}$ does exist, as we saw.

### The general case

Say our data consists of $n$ points, each of which is of length $p$.
Write the $j^{th}$ element of the $i^{th}$ data point as $X_{ij}$.
Then set

$$
A =
\left (
\begin{array}{rrrr}
1 & X_{11} & ... & X_{p1} \\
1 & X_{21} & ... & X_{p2} \\
... & ... & ... & ... \\
1 & X_{n1} & ... & X_{np} \\
\end{array}
\right )
$$ {#eq-designmat}

Continue to set $S$ to the length-$n$ column vector of our response
variable. Our model is

$$
E(S | A) = A \beta 
$$

for an unknown vector $\beta$ of length $p+1$. Our estimated of that
vector based on our data will be denoted

$$
b = (b_0,b_1,...,b_p)'
$$

Then, using the same reasoning as before, we have the minimizing
value of $b$:

$$
\widehat{\beta} = (A'A)^{-1} A'S
$${#eq-betahat}

again providing that the inverse exists.

### Example: **mlb1** data{#sec-mlb}

As an example, let's take the **mlb1**
from my [qeML ('Quick and Easy Machine
Learning](https://github.com/matloff/qeML) package.
[Dataset kindly provided by the UCLA Dept. of Statistics]{.column-margin}
The data is on major league baseball players. We will predict weight
from height and age.

```{r}
library(qeML)
data(mlb1)
head(mlb1)
ourData <- as.matrix(mlb1[,-1]) # must have matrix to enable %*%
head(ourData)
A <- cbind(1,ourData[,c(1,3)])
Ap <- t(A)
S <- as.vector(mlb1[,3])
solve(Ap %*% A) %*% Ap %*% S
# check via R
lm(Weight ~ .,data=mlb1[,-1])
```

So, if we have a player of known height and age, we would predict the
weight to be

-187.6382 + 4.9236 x height + 0.9115 x age

### Homogeneous variance case{#sec-varbhat}

In addition to

$$
E(S | A) = A \beta 
$$

$$
Cov(S | A) = \sigma^2 I
$$

where $\sigma$ is an unknown constant to be estimated from the data.
This is called the *homoskedasticity* assumption.

So $Var(S_i) = \sigma^2$ for all $i$.  It is usually not a realistic
assumption. Say for instance we are predicting human weight from height.
[A variation called the *sandwich* estimator allows for nonhomogeneous
and unknown $Cov(S | A)$. See the package **sandwich**.]{.column-margin}
There should be more variation in weight among taller people than above
shorter people. But it's a simplifying assumption, so it is commonly
used.

And in that setting, our formulas from @sec-covar come in
handy, as follows.

Recall that $\widehat{\beta}$ is our estimate of the unknown population
parameter $\beta$, based on our random sample data. But that means that
[Fixed-X vs. random-X settings: In some applications, $A$ is actually
chosen by an experimenter, so that it is not random, the so-called
*fixed-X* setting. But even in the random case (*random-X* setting), it
is standard to condition on $A$.  When we derive confidence intervals in
@sec-confsets, the distinction won't matter.  By @eq-iterexpect, a
conditional confidence interval, say, at the 95% level also has that
level unconditionally.]{.column-margin} $\widehat{\beta}$ is a random
vector, and thus has a covariance matrix. Using @eq-acova with $R =
(A'A)^{-1}$, and recalling the properties of transpose, we have

\begin{align}
Cov(\widehat{\beta}) &= Cov(RA'S) \\
&= RA' \sigma^2 I AR' \\
&= \sigma^2 (A'A)^{-1} 
\end{align}

Classical statistical formulas use this relation to find standard errors
for $\widehat{\beta}_i$ etc., to be presented in @sec-confsets.

## Update Formulas

One important theme in developing prediction models (linear regression,
neural networks etc.) is the avoidance of *overfitting*, meaning that we
fit an overly elaborate model to our data. We simply are estimating too
many things for the amount of data we have, "spreading our data too
thin."

A common example is using too many predictor variables, so that, e.g.
in the linear model case, we are estimating a large number of
coefficients $\beta_i$.

Or we may draw a histogram with too many bins:

```{r}
library(qeML)
data(forest500)  # data on forest ground cover
hist(forest500$V1,breaks=10)
hist(forest500$V1,breaks=100)
```

In a histogram, we are estimating the heights of the bins. With 10 bins
we obtained a smooth graph, but with 100 bins it became choppy. So
again, we are estimating too many things, given the capacity of the
data.[A histogram is an estimate of the probability density function of
the observed random variable. Having more, thus narrower, bins reduces
estimation bias but increases estimation variance.]{.column-margin} 

With larger datasets, we can use more predictor variables, more
histogram bins, and so on. The question then arises is, for instance,
*How many* predictors, *how many* bins and so on,  can we afford to use
with our given data?

The typical solution is to fit several models of different complexity,
then choose the one that predicts the best. But we must do this
evaluation on "fresh" data; we should not predict on the same data on
which we fitted our model.

We thus rely on partitioning our data, into a *training set* to which we
fit our model, and a *test set*, on which we predict using the fitted
model. We may wish to do this several times.

A special case is the Leaving One Out method, in which the test set
size is 1. It might go like this, for a dataset d:

```
sumErrs = 0
for i = 1,...,n  # dataset has n datapoints
   fit lm to d[-i,]
   use result to predict d[i,]
   add prediction error to sumErrs
return sumErrs
```

This can become computationally challenging, as we would need to refit
the model each time. Each call to **lm** involves a matrix inversion
(or equivalent), and we must do this $n$ times.

It would be nice if we could have an "update"
formula that would quickly recalculate the model found on the full
dataset. Then we would need to perform matrix inversion just once.
In the case of linear models, such a formula exists, in the
Sherman-Morrison-Woodbury relation:

<blockquote>

Given an invertible matrix $B$ and row vectors $u$ and $v$ having
lengths equal to the number of columns of $B$. form the matrix 

$$
C = B + uv'
$$  

Then $C^{-1}$ exists and is equal to[Note that the quantity 
$uv'$ is a square matrix the size of $B$, so the sum and product make
sense.]{.column-margin}

$$
B^{-1} - \frac{1}{1+v'B^{-1}u} B^{-1} (uv') B^{-1}
$$

</blockquote>

Now, how can we apply this to the Leave One Out method? In the matrix
$A$ in Equation @eq-designmat, we wish to remove row $i$; call the
result $A_{-i}$. Our new version of $A'A$ is then

$$
A_{-i}' A_{-i}
$$

So our main task is to obtain

$$
(A_{-i}' A_{-i})^{-1}
$$

by updating $(A'A)^{-1}$, which we already have from
our computation in the full dataset.

We can do this as follows. Denote row $i$ of $A$ by $a_i$, and set

$$
u = -a_i, v = a_i
$$

To show why these choices for $u$ and $v$ work, consider the case in
which we delete the last row of $A$. (The analysis would be similar for
other cases.) Write the latter as a partitioned matrix,

$$
 \left (
 \begin{array}{r}
 A_{(-n)} \\
 a_n \\
 \end{array}
 \right )
$$

We pretend it is a $2 \times 1$ "matrix," and $A'$ is then "$1
\times 2$":

$$
A' = (A_{(-n)}' | a_n') 
$$ 

Thus

$$
A'A = 
A_{-i}' A_{-i} + a_n a_n'
$$

yielding

$$
A_{-i}' A_{-i} = A'A - a_n a_n'
$$


just what we need for Sherman-Morrison-Woodbury: With $B = A'A$,
we have

$$
[A_{-i}' A_{-i}]^{-1} =
B^{-1} + \frac{1}{1-a_i'B^{-1}a_i} 
B^{-1} (a_i a_i') B^{-1}
$$

Let's check it:

```{r}
a <- rbind(c(1,3,2),c(1,0,5),c(1,1,1),c(1,9,-3)) 
apa <- t(a) %*% a
apai <- solve(apa)
a2 <- a[-2,]
apa2 <- t(a2) %*% a2
apa2i <- solve(apa2)
# prepare for S-M-W
adel <- matrix(a[2,],ncol=1)
w1 <- 1/(1 - t(adel) %*% apai %*% adel)
w1 <- as.numeric(w1)
uvt <- adel %*% t(adel)
w2 <- apai %*% uvt %*% apai
# S-M-W says this will be apa2i
apai + w1 * w2
apa2i
```

## Generalized Linear Models

These are nonlinear models that have a "linear" component to them. The
best known is the *logistic* (or *logit*) model. Here our "Y" is a
binary variable, $Y = 0,1$.

Note first that now

\begin{align}
E(Y|X=t) &= 1 \cdot P(Y=1 | X=t) +
0 \cdot P(Y=0 | X=t) \\
&= P(Y=1 | X=t) \\
\end{align}

So the conditional mean now reduces to a conditional probability. This
is very useful, e.g. finding the probability that a patient has a
certain disease, given test results and symptoms.

So, what about the "linear component"? Here is the model:

$$
P(Y=1 | x=t) =
\frac{1}{1+\exp{[-\beta' \tilde{t}}]}
$$

where as usual, $\beta = (\beta_0,\beta_1,...,\beta_p)'$. The quantity
$\tilde{t}$ is

$$
\left (
\begin{array}{r}
1 \\
t \\
\end{array}
\right )
$$

and the 1 is there to pick up the $\beta_0$ term. So there is the linear
component, $\beta' \tilde{t}$.

Least-squares computation doesn't work here, though some modifications
do, e.g. *Iteratively Reweighted Least Squares*. This topic is beyond
the scope of this book.

### Example: census data{#sec-svcensus}

This is data for Silicon Valley programmers and engineers in the 2000
Census.

Let's predict female gender:

```{r}
data(svcensus)
head(svcensus)
svcensus$gender <- as.numeric(svcensus$gender == 'female')  # Y=0,1
head(svcensus)
# for simplicity, omit a couple of the columns
glmOut <- glm(gender ~ .,data=svcensus[,-c(2,5)],family=binomial)
coef(glmOut)
```

The 'binomial' value here specifies the logistic model. Others are
available, e.g. *Poisson regression*.

The direction of influence on "Y" is the same as in the linear case: A
positive coefficient means, holding other predictor variables fixed,
larger values of this variable produce larger probabilities for $Y = 1$.
The most impactful predictors seem to be occupations 140 and 141, each
[We of course must form confidence intervals for a careful analysis, the
topic of @sec-confsets.]{.column-margin} of which reduces the
probability of female. 

It should be noted, though, that the amount of impact depends on the
values of the other pedictors, unlike the linear case. A popular
interpretation of the logit model is the *log-odds* view, which forces
focus on the linear component:

$$
\log{
\left [
\frac{P(Y=1|X=t)}{P(Y=0|X=t)}
\right ]
= \beta' \tilde{t}
}
$$

However, I have never felt this is helpful. Why on Earth should we be
interested in the log-odds value? And it detracts from the focus of
logit modeling a probability, of central importance.

## Centering and Scaling One's Data{#sec-ctrscl}

You often hear data analysts speak of *centering and scaling* their
data. What does mean, and why is useful?

* Centering a variable means to subtract its mean. The new version now
  has mean 0.

* Scaling a variable means to divide by its standard deviation. The new
  version now has standard deviation 1.0.

Both operations make the variables more comparable, especially scaling.
If say two predictor variables are of different sizes, say Height and
Age in our baseball data above, it's hard to compare their
$\widehat{\beta}_i$ values. Scaling them makes things more comparable.

As to centering, this can reduce roundoff error in computations, and
anyway it seems questionable to scale but not center.

Note that if we center and scale not only the "X" variables but also
"Y," it can be shown this is equivalent to setting a model without an
intercept term $\beta_0$. This often simplifies matters in various ways.
Note that in this setting, we do not include a 1s column in the $A$
matrix.

The R function **scale** does centering and/or scaling.

## Your Turn

❄️  **Your Turn:** Show @eq-dda. Write $a = (a_1,...,a_k)'$ and find the
gradient "by hand." Compare to $2a$.

❄️  **Your Turn:** Show that 

$$
\frac{d}{du} u'Qu = 2Qu
$$

for a constant symmetric matrix $Q$ and a vector $u$. ($u'Qu$ is called
a *quadratic form*.)

