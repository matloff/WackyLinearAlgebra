
```{r}
#| include: false
library(dsld)
library(qeML)
```

{{< pagebreak >}}

# Eigenanalysis 

::: {.callout-note}

## Goals of this chapter:

The concept of eigenvalues and eigenvectors is one of the most important
of all appplications of linear algebra to data science. We introduce the
ideas in this chapter, and present some applications.

:::

## Definition

The concept itself is simple:

<blockquote>

Consider an $m \textrm{ x } m$ matrix $M$. If there is a nonzero vector
$x$ and a number $\lambda$ such that

$$
Mx = \lambda x
$$

we say that $\lambda$ is an *eigenvalue* of $M$, with *eigenvector*
$x$.
  
</blockquote>

## A First Look

Here are a few properties to start with:

* The definition is equivalent to

  $$
  (M - \lambda I) x = 0
  $$

  which in turn implies that $(M - \lambda I$ is noninvertible. That
  then implies that 

  $$
  det(M - \lambda I) = 0
  $$

* In principle, that means we can solve the above determinant equation
  to find the eigenvalues of the matrix. The comments in @sec-detdef
  regarding the "ugly" definition of determinants imply that the
  equation involves a degree-$m$ polynomial in $\lambda$.

* There are much better ways to calculate the eigenvalues than this, but
  a useful piece information arising from that analysis is that $M$ as
  $m$ eigenvalues (not necessarily distinct).  

❄️  **Your Turn:** Show that if $x$ is an eigenvector of $M$ with
eigenvalue $\lambda \neq 0$, then for any nonzero number $c$, $cx$ will
also be an eigenvector with eigenvalue $\lambda$.

❄️  **Your Turn:** Show that if a matrix $M$ has a 0 eigenvalue, $M$
must be singular. Also prove the converse. (Hint: Consider the column
rank.)

## The Special Case of Symmetric Matrices

Many matrices in data science are symmetric, such as covariance matrices
and $A'A$ in @eq-linregformula.

### Eigenvalues are real

Some eigenvalues may be complex numbers, i.e. of the form $a+bi$, but it
can be shown that if $M$ is symmetric, its eigenvalues are guaranteed to
be real. This is good news for data science, as many matrices in that
field are symmetric, such covariance matrices.

### Eigenvectors are orthogonal

<blockquote>

Eigenvectors corresponding to distinct eigenvalues of a symmetric matrix
$M$ are othogonal.

*Proof:* Let $u$ and $v$ be such eigenvectors, corresponding to
eigenvalues $\mu$ and $\nu$.

$$
\mu' M \nu = (\mu' M \nu)' = \nu' M' \mu = \nu' M \mu
$$

(in that first equality, we used the fact that the transpose of a number
is that number). Substitute for $M\nu$ and $M\mu$, then subtract.

</blockquote>

### Symmetric matrices are diagonalizable

A square matrix $R$ is said to be *diagonalizable* if there exists an
invertible matrix $P$ such that $P^{-1}RP$ is equal to some diagonal
matrix $D$.

One application of this is the computation of powers of a matrix:

$$
R^k = P^{-1}RP ~ P^{-1}RP ~ ... ~ P^{-1}RP = P^{-1}D^kP
$$

$D^k$ is equal to the diagonal matrix with elements $d_k^k$, so the
computation of $M^k$ is easy.

<blockquote>

*Theorem:* Symmetric matrices $M$ are diagonalizable. In fact, the matrix $P$
is equal to the matrix whose columns are the eigenvectors of $M$, chosen
to have length 1. The associated diagonal matrix has as its diagonal
elements the eigenvalues of $M$. The matrix $P$ has the property that
$P^{-1} = P'$.

*Proof:* Use partitioned matrices. 

Let $D = diag(d_1,...,d_m)$, where the $d_i$ are the eigenvalues of $M$,
corresponding to eigenvectors $P_i$. Set the latter to have length 1,
by dividing by their lengths.  

Recall that the eigenvectors of $M$, i.e. the columns of $P$, are
orthogonal. That means the rows of $P'$ are orthogonal. Again using
partitioning, we see that 

$$
P'P = I
$$

so that $P^{-1} = P'$.

Now write

$$
MP = M(P_1 | ... | P_m) = (MP_1 | ... | MP_n) =
(d_1 P_1 | ... | (d_m P_m) = PD
$$

Finally, multiply on the left by $P'$, and we are done.

</blockquote>

*Example:*

Let's illustrate all this with the data in @sec-pgtn. We will form the
matrix $A'A$ in @sec-linregformula, which as mentioned, is symmetric.

Recall that in that example, $A$ is not of full rank. Thus we should
expect to see a 0 eigenvalue.

```{r}
library(qeML)
data(svcensus)
svc <- svcensus[,c(1,4:6)]
svc$man <- as.numeric(svc$gender == 'male')
svc$woman <- as.numeric(svc$gender == 'female')
svc$gender <- NULL
a <- as.matrix(svc[,-2])
a <- cbind(1,a)  # add the column of 1s
m <- t(a) %*% a
eigs <- eigen(m)
eigs
m %*% eigs$vectors[,1]
eigs$values[1] %*% eigs$vectors[,1]
```

Yes, that first column is indeed an eigenvector, with the claimed eigenvalue. 

Note that the expected 0 eigenvalue shows up as 2.801489e-12, quite
small but nonzero, due to roundoff error.

## Application: Dimension Reduction (PCA)

The term *dimension reduction* means thinning out ones set of predictor
variables/features. This could be for the purpose of avoiding
overfitting, or just to keep things simple. One classical method for
achieving this is Principal Components Analysis (PCA).

### Example: African Soils Data

To get things started, let's consider the African Soils dataset,
available at various Internet locations, such as
[Kaggle](https://www.kaggle.com/c/afsis-soil-properties). Unzip the file
to obtain **train.zip**, and unzip that to get **training.csv**. Make
sure the latter file is in the same directory/folder as your Quarto
files for this document.

Let's take a look around:

``` r 
afrs <- read.csv('training.csv')
dim(afrs)
# 1157 3600
names(afrs)
[1] "PIDN"     "m7497.96" "m7496.04" "m7494.11" "m7492.18" "m7490.25"
[7] "m7488.32" "m7486.39" "m7484.46" "m7482.54" "m7480.61" "m7478.68"
[3577] "m603.617" "m601.688" "m599.76"  "BSAN"     "BSAS"     "BSAV"
[3583] "CTI"      "ELEV"     "EVI"      "LSTD"     "LSTN"     "REF1"
[3589] "REF2"     "REF3"     "REF7"     "RELI"     "TMAP"     "TMFI"
[3595] "Depth"    "Ca"       "P"        "pH"       "SOC"      "Sand"
```

Let's try predicting **pH**, the acidity. But that leaves 3599 possible
predictors. There is an old rule of thumb that one should have 
$$p < \sqrt{n}$$ to avoid overfitting, which in our setting of $$n = 1157$$
grossly violates.  We need to do dimension reduction. One way to
accomplish this is to use PCA. 

### Overall Idea 

The goal is to find a few important linear combinations of our original
predictor variables. Let's see how this is done.

Recall the matrix $A$ in {@eq-linregformula}.

## Application: Measuring Multicollinearity

## Application: Another Solution to the $p > n$ Problem in Linear Models

## Application: Graph Clustering

