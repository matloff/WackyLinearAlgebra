
```{r}   
#| include: false
library(dsld)
library(qeML)
```

{{< pagebreak >}}

# Eigenanalysis 

::: {.callout-note}

## Goals of this chapter:

The concept of eigenvalues and eigenvectors is one of the most important
of all appplications of linear algebra to data science. We introduce the
ideas in this chapter, and present some applications.

:::

## Definition

The concept itself is simple:

<blockquote>

Consider an $m \textrm{ x } m$ matrix $M$. If there is a nonzero vector
$x$ and a number $\lambda$ such that

$$
Mx = \lambda x
$$

we say that $\lambda$ is an *eigenvalue* of $M$, with *eigenvector*
$x$.
  
</blockquote>

## A First Look

Here are a few properties to start with:

* The definition is equivalent to

  $$
  (M - \lambda I) x = 0
  $$

  which in turn implies that $M - \lambda I$ is noninvertible. That
  then implies that 

  $$
  det(M - \lambda I) = 0
  $$

* In principle, that means we can solve the above determinant equation
  to find the eigenvalues of the matrix. The comments in @sec-detdef
  regarding the "ugly" definition of determinants imply that the
  equation involves a degree-$m$ polynomial in $\lambda$.

* There are much better ways to calculate the eigenvalues than this, but
  a useful piece information arising from that analysis is that $M$ as
  $m$ eigenvalues (not necessarily distinct).  

❄️  **Your Turn:** Show that if $x$ is an eigenvector of $M$ with
eigenvalue $\lambda \neq 0$, then for any nonzero number $c$, $cx$ will
also be an eigenvector with eigenvalue $\lambda$.

❄️  **Your Turn:** Show that if a matrix $M$ has a 0 eigenvalue, $M$
must be singular. Also prove the converse. (Hint: Consider the column
rank.)

## The Special Case of Symmetric Matrices

Many matrices in data science are symmetric, such as covariance matrices
and $A'A$ in @eq-linregformula. 

### Eigenvalues are real

Some eigenvalues may be complex numbers, i.e. of the form $a+bi$, but it
can be shown that if $M$ is symmetric, its eigenvalues are guaranteed to
be real. This is good news for data science, as many matrices in that
field are symmetric, such covariance matrices.

### Eigenvectors are orthogonal

<blockquote>

Eigenvectors corresponding to distinct eigenvalues of a symmetric matrix
$M$ are othogonal.

*Proof:* Let $u$ and $v$ be such eigenvectors, corresponding to
eigenvalues $\mu$ and $\nu$.

$$
\mu' M \nu = (\mu' M \nu)' = \nu' M' \mu = \nu' M \mu
$$

(in that first equality, we used the fact that the transpose of a number
is that number). Substitute for $M\nu$ and $M\mu$, then subtract.

</blockquote>

### Symmetric matrices are diagonalizable

A square matrix $R$ is said to be *diagonalizable* if there exists an
invertible matrix $P$ such that $P^{-1}RP$ is equal to some diagonal
matrix $D$.

One application of this is the computation of powers of a matrix:

$$
R^k = P^{-1}RP ~ P^{-1}RP ~ ... ~ P^{-1}RP = P^{-1}D^kP
$$

$D^k$ is equal to the diagonal matrix with elements $d_k^k$, so the
computation of $M^k$ is easy.

<blockquote>

*Theorem:* Symmetric matrices $M$ are diagonalizable. In fact, the matrix $P$
is equal to the matrix whose columns are the eigenvectors of $M$, chosen
to have length 1. The associated diagonal matrix has as its diagonal
elements the eigenvalues of $M$. The matrix $P$ has the property that
$P^{-1} = P'$.

*Proof:* Use partitioned matrices. 

Let $D = diag(d_1,...,d_m)$, where the $d_i$ are the eigenvalues of $M$,
corresponding to eigenvectors $P_i$. Set the latter to have length 1,
by dividing by their lengths.  

Recall that the eigenvectors of $M$, i.e. the columns of $P$, are
orthogonal. That means the rows of $P'$ are orthogonal. Again using
partitioning, we see that 

$$
P'P = I
$$

so that $P^{-1} = P'$.

Now write

$$
MP = M(P_1 | ... | P_m) = (MP_1 | ... | MP_n) =
(d_1 P_1 | ... | (d_m P_m) = PD
$$

Finally, multiply on the left by $P'$, and we are done.

</blockquote>

*Example:*

Let's illustrate all this with the data in @sec-pgtn. We will form the
matrix $A'A$ in @sec-linregformula, which as mentioned, is symmetric.

Recall that in that example, $A$ is not of full rank. Thus we should
expect to see a 0 eigenvalue.

```{r}
library(qeML)
data(svcensus)
svc <- svcensus[,c(1,4:6)]
svc$man <- as.numeric(svc$gender == 'male')
svc$woman <- as.numeric(svc$gender == 'female')
svc$gender <- NULL
a <- as.matrix(svc[,-2])
a <- cbind(1,a)  # add the column of 1s
m <- t(a) %*% a
eigs <- eigen(m)
eigs
m %*% eigs$vectors[,1]
eigs$values[1] %*% eigs$vectors[,1]
```

Yes, that first column is indeed an eigenvector, with the claimed eigenvalue. 

Note that the expected 0 eigenvalue shows up as 2.801489e-12, quite
small but nonzero, due to roundoff error.

## Application: Dimension Reduction (PCA)

The term *dimension reduction* means thinning out ones set of predictor
variables/features. This could be for the purpose of avoiding
overfitting, or just to keep things simple. One classical method for
achieving this is Principal Components Analysis (PCA).

### Example: African Soils Data

To get things started, let's consider the African Soils dataset,
available at various Internet locations, such as
[Kaggle](https://www.kaggle.com/c/afsis-soil-properties). Unzip the file
to obtain **train.zip**, and unzip that to get **training.csv**. Make
sure the latter file is in the same directory/folder as your Quarto
files for this document.

Let's take a look around:

```{r}
afrs <- read.csv('training.csv')
dim(afrs)
names(afrs)[1:25]
names(afrs)[3576:3600]
```

Let's try predicting **pH**, the acidity. But that leaves 3599 possible
predictors. There is an old rule of thumb that one should have 
$$p < \sqrt{n}$$ to avoid overfitting, which in our setting of $$n = 1157$$
grossly violates.  We need to do dimension reduction. One way to
accomplish this is to use PCA. 

### Overall Idea 

The goal is to find a few important linear combinations of our original
predictor variables--important in the sense that they roughly summarize
our data.  Let's see how this is done.

Let **X** denote a set of variables of interest (not necessarily in a
prediction context).
In searching for good linear combinations, we want to aim for ones with
high variance. We certainly don't want ones with low variance; after
all, a random variable with 0 variance is a constant.  So we wish to
find linear combinations 

$$
Xu
$$

which maximize 

$$
Var(Xu) = u' Cov(X) u
$$

where we have invoked @eq-acova.

But that goal is ill-defined, since we could take larger and larger
vectors $u$, thus larger and larger vectors $Xu$, no maximum.  So, let's
constrain it to vectors $u$ of length 1:

$$
u'u = 1
$$

The method of *Lagrange multipliers* is used to solve maximum/minimum
problems that have constraints, by adding a new variable corresponding
to the constraint. In our case, we maximize

$$
u'Cov(X)u + \gamma (u'u - 1)
$$

with respect to $u$ and $\gamma$.

Setting derivatives to 0, we have
$$
0 = 
2 Cov(X) u + 2 \gamma u
$$

and

$$
0 = u'u - 1
$$

In other words,

$$
Cov(X) u = -\gamma u
$$

Aha! The vector $u$ is an eigenvector of the matrix $Cov(X)$ with
eigenvalue $-\gamma$.  We've discovered that our quest for dimension
reduction can be couched as an eigenanalysis problem!

More commonly, we start with a covariance matrix rather than $A'A$, but
with the same result: Our solution is eigenanalysis on the covariance
matrix.  (If we center our predictors, and divide by $n$, we get the
covariance matrix anyway.)

### Back to the Example

So, we remove the ``Y'' variable, $pH$, number 3598 as seen above, as
proceed. We will also remove the nonnumeric columns,
**PIDN** and **Depth**.

```{r} 
x <- afrs[,-c(1,3595,3598)]
```

At this point, we could run

``` r
xcov <- cov(x)
eigs <- eigen(xcov)
```

to perform the eigenanalysis, but R's **prcomp** function does all the
work for us.  

```{r}
pcs <- prcomp(x)
```

Our goal is to retain linear combinations of the columns of $X$ that
have large variances.  For whatever reason, the designers of **prcomp**
report standard deviations instead of variances.

```{r}
pcs$sdev[1:50]
```

Ah, the eigenvalues fall off rapidly after the first few. So, let's say
we decide to use the first 9 $u$ vectors.

## Application: Measuring Multicollinearity

## Application: Another Solution to the $p > n$ Problem in Linear Models

## Application: Graph Clustering

