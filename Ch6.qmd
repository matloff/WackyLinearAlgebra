
```{r}
#| include: false
library(dsld)
library(qeML)
```

{{< pagebreak >}}

# Eigenanalysis 

::: {.callout-note}

## Goals of this chapter:

The concept of eigenvalues and eigenvectors is one of the most important
of all appplications of linear algebra to data science. We introduce the
ideas in this chapter, and present some applications.

:::

## Definition

The concept itself is simple:

<blockquote>

Consider an $m \textrm{ x } m$ matrix $M$. If there is a nonzero vector
$x$ and a number $\lambda$ such that

$$
Mx = \lambda x
$$

we say that $\lambda$ is an *eigenvalue* of $M$, with *eigenvector*
$x$.
  
</blockquote>

## A First Look

Here are a few properties to start with:

* The definition is equivalent to

  $$
  (M - \lambda I) x = 0
  $$

  which in turn implies that $(M - \lambda I$ is noninvertible. That
  then implies that 

  $$
  det(M - \lambda I) = 0
  $$

* In principle, that means we can solve the above determinant equation
  to find the eigenvalues of the matrix. The comments in @sec-detdef
  regarding the "ugly" definition of determinants imply that the
  equation involves a degree-$m$ polynomial in $\lambda$.

* There are much better ways to calculate the eigenvalues than this, but
  a useful piece information arising from that analysis is that $M$ as
  $m$ eigenvalues (not necessarily distinct).  

❄️  **Your Turn:** Show that if $x$ is an eigenvector of $M$ with
eigenvalue $\lambda \neq 0$, then for any nonzero number $c$, $cx$ will
also be an eigenvector with eigenvalue $\lambda$.

## The Special Case of Symmetric Matrices

Many matrices in data science are symmetric, such as covariance matrices
and $A'A$ in @eq-linregformula.

### Eigenvalues are real

Some eigenvalues may be complex numbers, i.e. of the form $a+bi$, but it
can be shown that if $M$ is symmetric, its eigenvalues are guaranteed to
be real. This is good news for data science, as many matrices in that
field are symmetric, such covariance matrices.

### Eigenvectors are orthogonal

<blockquote>

Eigenvectors corresponding to distinct eigenvalues of a symmetric matrix
$M$ are othogonal.

*Proof:* Let $u$ and $v$ be such eigenvectors, corresponding to
eigenvalues $\mu$ and $\nu$.

$$
\mu' M \nu = (\mu' M \nu)' = \nu' M' \mu = \nu' M \mu
$$

(in that first equality, we used the fact that the transpose of a number
is that number). Substitute for $M\nu$ and $M\mu$, then subtract.

</blockquote>

### Symmetric matrices are diagonalizable

A square matrix $R$ is said to be *diagonalizable* if there exists an
invertible matrix $P$ such that $P^{-1}RP$ is equal to some diagonali
matrix $D$.

<blockquote>

*Theorem:* Symmetric matrices $M$ are diagonalizable. In fact, the matrix $P$
is equal to the matrix whose columns are the eigenvectors of $M$, chosen
to have length 1. The associated diagonal matrix has as its diagonal
elements the eigenvalues of $M$. The matrix $P$ has the property that
$P^{-1} = P'$.

*Proof:* Use partitioned matrices. (The reader may wish to review
@sec-partition before continuing.) 

Let $D = diag(d_1,...,d_m)$.

First write

$$
MP = M(P_1 | ... | P_m) = (MP_1 | ... | MP_n) =
(d_1 P_1 | ... | (d_m P_m) = PD
$$

Recall that the eigenvectors of $M$, i.e. the columns of $P$, are
orthogonal. That means the rows of $P'$ are orthogonal. Again using
partitioning, we see that 

$$
P'P = I
$$

so that $P^{-1} = P'$.

Thus we inspect 

$$
P'MP = P'DP
$$


Next, note that

$$

$$




</blockquote>

## Application: Dimension Reduction

## Application: Measuring Multicollinearity

## Application: Another Solution to the $p > n$ Problem in Linear Models

## Application: Graph Clustering

