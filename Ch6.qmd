
```{r}
#| include: false
library(dsld)
library(qeML)
```

{{< pagebreak >}}

# Eigenanalysis 

::: {.callout-note}

## Goals of this chapter:

The concept of eigenvalues and eigenvectors is one of the most important
of all appplications of linear algebra to data science. We introduce the
ideas in this chapter, and present some applications.

:::

## Definition

The concept itself is simple:

<blockquote>

Consider an $m \textrm{ x } m$ matrix $M$. If there is a nonzero vector
$x$ and a number $\lambda$ such that

$$
Mx = \lambda x
$$

we say that $\lambda$ is an *eigenvalue* of $M$, with *eigenvector*
$x$.
  
</blockquote>

## A First Look

Here are a few properties to start with:

* The definition is equivalent to

  $$
  (M - \lambda I) x = 0
  $$

  which in turn implies that $(M - \lambda I$ is noninvertible. That
  then implies that 

  $$
  det(M - \lambda I) = 0
  $$

* In principle, that means we can solve the above determinant equation
  to find the eigenvalues of the matrix. The comments in @sec-detdef
  regarding the "ugly" definition of determinants imply that the
  equation involves a degree-$m$ polynomial in $\lambda$.

* There are much better ways to calculate the eigenvalues than this, but
  a useful piece information arising from that analysis is that $M$ as
  $m$ eigenvalues (not necessarily distinct).  

## The Special Case of Symmetric Matrices

Many matrices in data science are symmetric, such as covariance matrices
and $A'A$ in @eq-linregformula.

* Some eigenvalues may be complex numbers, i.e. of the form $a+bi$,
  but it can be shown that if $M$ is symmetric, its eigenvalues are
  guaranteed to be real. This is good news for data science, as many
  matrices in that field are symmetric, such covariance matrices.

## Application: Dimension Reduction

## Application: Measuring Multicollinearity

## Application: Another Solution to the $p > n$ Problem in Linear Models

## Application: Graph Clustering

