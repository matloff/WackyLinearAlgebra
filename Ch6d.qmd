
```{r} 
#| include: false
library(dsld)
library(qeML)
```

{{< pagebreak >}}

# Pseudoinverse and Double Descent {#sec-chapDD}

::: {.callout-note}

## Goals of this chapter:

We learned earlier that in a linear regression model, a certain matrix
inverse needs to exist in order to calculate the standard least-squares
solution. But solutions do exist more generally, and that fact will turn
out to be useful, even exposing some very surprising numerical behavior.

::: 



```{r} 
#| include: false
library(dsld)
library(qeML)
```

## SVD as the Minimum-Norm Solution

In an overdetermined linear system such as @eq-overdetermined, there are
many solutions. However, an advantage of Moore-Penrose is that it gives
us the *minimum norm* solution. We'll discuss the significance of this
shortly, but let's prove it first.

We will need this:

::: {#thm-samenorm}

## Multiplication by an Orthogonal Matrix Preserves Norm 

For an orthogonal matrix $M$ and a vector $w$, $||Mw|| = ||w||$. 

:::

::: {.proof}

See the Your Turn problem below.

:::

::: {#thm-shortest}

## The Moore-Penrose Solution Is Min-Norm

Consider an $m \times n$ matrix $B$ and vector $q$ of length $m$.  Of
all solutions $x$ (of length $n$) to

$$
Bx = q
$${#eq-linsyst}

the Moore-Penrose solution minimizes $||x||$.

:::

::: {.proof}

Again, write[Adapted from a derivation by Carlo Tomasi.]{.column-margin}  

$$
B = U \Sigma V',
$$

where $U$ is $m \times n$ and $\Sigma$ and $V$ are $n \times n$.
Consider the residual sum of squares 

$$
||Bx - q||^2 = ||U (\Sigma V' x - U'q)||^2
$${#eq-RSS}

since $UU'=I$.

Thus from @thm-samenorm we can remove the factor $U$ in @eq-RSS, yielding

$$
||Bx - q||^2 = ||\Sigma V' x - U'q||^2
$${#eq-RSS1}

Rename $$V'x$$ to $w$:

$$
||Bx - q||^2 = ||\Sigma w - U'q||^2
$${#eq-RSS2}

Now bring in the fact that $\sigma_i = 0$ for $i > r$, by writing
everything in partitioned fashion.  Break $U$ into $r$ and $n-r$
columns,

$$
(U_1 | U_2) 
$$


and partition other objects similarly:

$$
V = (V_1 | V_2),
$$

$$
\Sigma =
\left (
\begin{array}{rr}
\Sigma_1 & 0 \\
0 & 0  \\
\end{array}
\right ),
$$

and 

$$
w =
\left (
\begin{array}{r}
w_1 \\
w_2 \\
\end{array}
\right )
$$

Substituting into @eq-RSS2, we have

$$
||Bx - q||^2 
= ||
\left (
\begin{array}{r}
\Sigma_1 w_1 \\
0 \\
\end{array}
\right ) -
\left (
\begin{array}{r}
U_1'q \\
U_2'q \\
\end{array}
\right )
||^2
$${#eq-RSS3}

This means that $w_2$ -- the last $n-r$ elements of our solution to $Bx
=q$ -- can be anything at all, without changing the residual sum of
squares, confirming that there are infinitely many equally-effective
solutions!

But if we want $x$ to be of minimum length, we need $w$ to be of minimum
length (the shortest $w$ gives us the shortest $x = Vw$, again by 
@thm-samenorm).  And,

$$
||w||^2 = ||w_2||^2 + ||w_2||^2
$$

Thus we take $w_2 = 0$.  

Now, reviewing @eq-RSS3, note that the equation

$$
\Sigma_1 w_1 = U_1 q
$$

has the solution

$$
w_1 = \Sigma_1^{-1} U_1 q
$$

An interesting sidelight of this is that it means that

$$
||Bx - q||^2 
= ||
\left (
\begin{array}{r}
0  \\
U_2q \\
\end{array}
\right )
||^2,
$${#eq-RSS3}

i.e. the residual sum of squares depends only on $U_2$, not $U_1$. But
returning to our quest for the minimum-norm solution, 
we map back to $x = Vw$, and have

$$
x_1 = V_1 \Sigma_1^{-1} U_1'q
$$

which is the SVD solution! Thus the SVD solution does have minimum norm.

$\square$

:::

to add:

possible reasons:

  at interp, min norm suddenly gives more than 1 choice; min norm is
  like min Var; maybe show unbiased by arguing A'A b = A'S for all A,
  thus A'E(estreg) = A'beta'Y for all A etc.; this of course holds only
  for estimable x'beta, meaning x is in row space of A (or equiv, A'A)

  condition number is max at interpolation

  (size of?) second U depends on min nonzero eigenvalue

my empirical example

ovrf <- function(nreps,n,maxP)
 {
    load('YearData.save')
    # yr <- yr[,1:2]
    nas <- rep(NA,nreps*(maxP-1))
    outdf <- data.frame(p=nas,mape=nas)
    rownum <- 0
    for (i in 1:nreps) {
       idxs <- sample(1:nrow(yr),n)
       trn <- yr[idxs,]
       tst <- yr[-idxs,]
       for (p in 2:maxP) {
          rownum <- rownum + 1
          # out<-qePolyLin(trn[,1:(p+1)],
          #    'V1',2,holdout=NULL)
          out <- qeLin(trn[,1:(p+1)],'V1',2,holdout=NULL)
          preds <- predict(out,tst[,2:(p+1)])
          mape <- mean(abs(preds - tst[,1]))
          outdf[rownum,1] <- p
          outdf[rownum,2] <- mape
          print(outdf[rownum,])
       }
    }
    outdf  #run through tapply() for the graph
 }

<!--
 MAKE IT GENERAL, PUT IN code/

 > set.seed(999)
 > o1 <- ovrf(1,30,30)  USE THIS
<-->

## Application: Explaining the Mystery of ``Double Descent"

Around 2018-2019, one of the statistics field's most deeply-held notions
was thrown off its pedestal, largely by some researchers in machine
learning. (This is arguably when the idea first became widespread, but
for earlier instances, see Marco Loog *et al*, *A brief prehistory of
double descent*, PNAS, 2020.) And many of the explanations given have
been related to SVD. 

### Motivating example

Let's consider the Million Song Dataset from @sec-millionsong.

### Overfitting and BEYOND

Recall that a well-known rule of thumb is that the number $p$ of
predictors should be less than $\sqrt{n}$, where $n$ is the number of
data points. We will abandon this and see what happens. 

We will add predictors one at a time, to form models of increasing
complexity. As usual, let $p$ denote the number of predictors in a given
model. We will also use only a small number of rows, say $n = 30$.  We
start with $p=1$, then $p=2$, eventually reaching $p = n-1$ (accounting
for the 1s column in the $A$ matrix in @eq-betahat), and not stopping
even there. We go to $p=n$, $p=n+1$ and so on.

When we reach $p=n+2$, the matrix $A$ will have more columns than rows,
and $(A'A)^{-1}$ will not exist. (See Your Turn problem below.) We can
still use SVD to solve for $b$, but intuitively some kind of major
change may happen at that point.

Here is the code and output

``` r
ovrf <- function(nreps,n,maxP)
 {
    load('YearData.save')
    # yr <- yr[,1:2]
    nas <- rep(NA,nreps*(maxP-1))
    outdf <- data.frame(p=nas,mape=nas)
    rownum <- 0
    for (i in 1:nreps) {
       idxs <- sample(1:nrow(yr),n)
       trn <- yr[idxs,]
       tst <- yr[-idxs,]
       for (p in 2:maxP) {
          rownum <- rownum + 1
          # don't use lm or qeLin, since no SVD; instead, fit polynomial
          # of degree 1, i.e. linear model
          out <- qePolyLin(trn[,1:(p+1)],'V1',1,holdout=NULL)
          preds <- predict(out,tst[,2:(p+1)])
          mape <- mean(abs(preds - tst[,1]))
          outdf[rownum,1] <- p
          outdf[rownum,2] <- mape
          print(outdf[rownum,])
       }
    }
    outdf  #run through tapply() for the graph
 }
```

``` r
> set.seed(111111); o1 <- ovrf(1,30,40)
  p     mape
1 2 7.865117
  p     mape
2 3 7.876853
  p     mape
3 4 7.984233
  p     mape
4 5 8.631492
  p     mape
5 6 8.608561
  p     mape
6 7 8.734537
  p    mape
7 8 8.74647
  p     mape
8 9 8.746996
   p     mape
9 10 9.042233
    p     mape
10 11 8.795864
    p    mape
11 12 8.88431
    p     mape
12 13 10.06747
    p     mape
13 14 10.17324
    p     mape
14 15 10.68381
    p     mape
15 16 10.41553
    p     mape
16 17 9.892437
    p    mape
17 18 9.71409
    p     mape
18 19 11.25314
    p     mape
19 20 11.88647
    p     mape
20 21 17.94607
    p     mape
21 22 18.63847
    p     mape
22 23 15.26327
    p     mape
23 24 19.50255
    p     mape
24 25 24.53056
    p     mape
25 26 23.60238
    p     mape
26 27 25.35561
    p    mape
27 28 36.1927
    p     mape
28 29 57.97567
    p     mape
29 30 754.9635
P > N. With polynomial terms and interactions, P is 31.


    p    mape
30 31 741.573
P > N. With polynomial terms and interactions, P is 32.


    p  mape
31 32 477.3
P > N. With polynomial terms and interactions, P is 33.


    p     mape
32 33 525.8362
P > N. With polynomial terms and interactions, P is 34.


    p     mape
33 34 465.0518
P > N. With polynomial terms and interactions, P is 35.


    p     mape
34 35 761.3692
P > N. With polynomial terms and interactions, P is 36.


    p    mape
35 36 689.829
P > N. With polynomial terms and interactions, P is 37.


    p     mape
36 37 673.8049
P > N. With polynomial terms and interactions, P is 38.


    p     mape
37 38 728.8918
P > N. With polynomial terms and interactions, P is 39.


    p     mape
38 39 666.2301
P > N. With polynomial terms and interactions, P is 40.


    p     mape
39 40 621.9375
```
MAPE here is mean absolute prediction error (calculated on the holdout
set).

Here $n = 30$, so $p = 30, 31,...$ uses SVD, and is overfitting. Indeed,
much smaller values of $p$ seem to have overfit as well. But the point
is that once we got to the point at which there is no unique solution,
MAPE actually improved, a huge shock to the field. It was always assumed
that there is no point going beyond the values of $p$ that gives us 0
for the sum of squares.

The graph of MAPE is typically a U-shape. In this case, we seem to have
only the right half of a U, but still a U. What was shocking was that
sometimes there is a *second* U-shape after we reach 0 sum of squares.
Even more shocking, in some case the low point of the second U is lower
than that of the first--it pays to radically ovefit.

### The classic  and modern views

In other words, here is the sea change that occurred in the field around
2018-2019.

Let $\kappa$ denote the complexity of a model. In the case of a linear
model, $\kappa$ would be the number of predictor columns in our dataset,
and there are ways of defining it for other methods.

We might try several values of $\kappa$, and then choose the one with
smallest MAPE or other accuracy measure.

Before 2018-2019, the view was:

<blockquote>

In plotting MAPE against $\kappa$, the curve will generally be roughly
U-shaped, up to the point at which $\kappa$ gives us a 0 value of RSS in
the training set. That value of $\kappa$, called the *interpolation*
point, will give us "perfect" prediction in the training set, but very
poor prediction in the test set, which is what counts.

We choose the value of $\kappa$ at which the curve is
lowest. There is no point in trying values of $\kappa$ past the
interpolation point.

</blockquote>

This was taken for granted throughout the statistics and machine learning
fields. But around 2018-2019, machine learning engineers were routinely
analyzing dataset of extraordinarily large sizes, using unprecedently
large values of $\kappa$. And they discovered that, bizarrely, as
$\kappa$ moved past the interpolation point, the curve often went
*down*--the second "descent"--often tracing its own second U-shape.
And most significantly, the low point of the second U was sometimes
below that of the first U! Overfitting--grossly so--may pay off!

So the modern view is:

<blockquote>

The first U-shaped curve is as in the classic view. But the curve should
be plotted past $\kappa$, and the overall minimum may be in that second
U.

</blockquote>

This is a rare sea change in classic quantitative analysis.

### How can this bizarre effect occur?

Let's look further. If at least some our predictors are numeric (with Million
Song, they all are), then for $p = n-1$, the (square) matrix $A$ itself
will very likely be invertible, and setting

$$
b = A^{-1} S
$$

will minimize @eq-matrixss--with the value of that expression being 0--a
perfect fit to the data!

Now again consider $p=n$. Let $A_{new}$ denote our new $A$ (it will be
equal to the old one with a new column added on the right), and $b_{new}$
denote a new version of $b$. We say "*a* new version" here, because
there are now infinitely many versions; recall that the SVD version has
minimum norm among them, a point we will return to shortly. Let's use
$A_{old}$ and $b_{ol}$ to denote the quantities we got for $p=n-1$.

We can write 

$$
A_{new} =
(A_{old} | c_{n+1})
$$

and 

$$
b_{new} = (b_{old} | b_{n+1})'
$$

Thus

$$
A_{new}' A_{new} b_{new} =
(A_{old}' A_{old} + c_{n+1} c_{n+1}')
\left (
\begin{array}{r}
b_{old} \\
b_{n+1} \\
\end{array}
\right )
$$

As noted, we now must use SVD. We will still get a perfect fit. To see
this, set $b_{n+1} = 0$ above, which gives us the same fit we got for $n
= p-1$.  In fact, there will be infinitely many values of $b$ that
achieve that perfect fit.  But remember, the SVD solution has minimum
norm among them all, a point we will return to shortly.

In considering the effectiveness of an estimator in statistics, we often
look at bias and variance.  Let's consider bias first.

In our setting here, the quantities of interest are of the form
$w'\beta$ the predicted value for an individual who has the characterics
$w$. We don't know $\beta$, so we use $w'b$ instead.
, estimated by $w'b$. In the non-full rank setting, not all
such quantities are physically estimable, but the theory says that any
$w$ in the row space of $A$ will be fine. Moreover, it says the least
squares estimate of $w'\beta$ will have 0 bias.

Now concerning variance, we have

$$
Var(w'b) = w' Cov(b) w 
$$

In general, shorter random vectors will have less variability, which
though not the same as variance/covariance at least gives us the feeling
that such an estimator has low variance. Recall that this is the
rationale for shrinkage estimators -- accept a small amount of bias in
return for a reduction in variance.

Thus it is at least plausible that we might get a second U-shape, as the
impact of the minimum-norm nature of SVD kicks in. On the other hand, as
$p$ grows further, $b$ has more and more components, and likely that
minmum-norm $b$ will grow in length at some point, hence the typical
later turn back upward of the second U.


## Your Turn

❄️  **Your Turn:** Show that for an orthogonal matrix $M$ and a vector
$w$, $||Mw|| = ||w||$.

❄️  **Your Turn:** Using properties of matrix rank, show that if $p+1 > n$
in @eq-betahat, the inverse will not exist.


