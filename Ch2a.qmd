

```{r}
#| include: false
library(dsld)
library(qeML)
```

{{< pagebreak >}}

# Covariance Matrices, MV Normal Distribution {#sec-covar}

::: {.callout-note}

## Goals of this chapter:  

A central entity in multivariate analysis is that of the *covariance
matrix*. In this chapter we define the term, list its many properties,
and show its role in the multivariate analog of the normal distribution
family. 

:::

## Random Vectors

You are probably familiar with the concept of a random variable, but of
even greater importance is random *vectors*.

Say we are jointly modeling height, weight, age, systolic blood pressure
and cholesterol, and are especially interested in relations between
these quantities.  We then have the random vector

$$
 X
=
 \left (
 \begin{array}{r}
 X_1 \\
 X_2 \\
 X_3 \\
 X_4 \\
 X_5 \\
  \end{array}
 \right )
=
 \left (
 \begin{array}{r}
 \textrm{height} \\
 \textrm{weight} \\
 \textrm{age} \\
 \textrm{bp} \\
 \textrm{chol} \\
  \end{array}
 \right )
$$

### Sample vs. population

We may observe $n$ realizations of $X$ in the form of sample data, say
on $n = 100$ people. In the statistics world, we treat this data as a
random sample from some population, say all Americans. Usually, we are
just given the data rather then having actual random sampling, but this
view recognizes that there are a lot more people out there than our
data.

We speak of estimating population quantities. For instance, we can
estimate the population value $E(X_1)$, i.e. mean of $X_1$ throughout the
population, by the sample analog,

$$
\frac{1}{n} \sum_{i=1}^n X_{1j}
$$

where $X_{ij}$ denotes the value of $X_i$ for the $j^{th}$ person in our
sample.

By contrast, this view is rarely taken in the machine learning
community. The data is the data, and the fact that it is a small subset
of a much larger group is irrelevant. They will often allude to the
randomness of the data by mentioning the "data generating mechanism,"
but go no further.




## Covariance{#sec-cov}

### Scalar covariance

Recall first the notion in statistics of *covariance*:  Given a pair of
random variables $U$ and $V$, their covariance is defined by

$$
Cov(U,V) = E[(U - EU)(V - EV)]
$$

Loosely speaking, this measures the degree to which the two random
variables vary together. Consider for instance human height $H$ and $W$.
Taller people tend to also be heavier. Say we sample many people from a
population. Most of those who are taller than average, i.e. $H > EH$,
will also be heavier than average, $W > EW$, making $(H - EH)(W - EW) >
0$.  Similarly, shorter people tend to be lighter, i.e. we often have $H
< EH$ and $W < EW$, but then we still have $(H - EH)(W - EW) > 0$. So,
one way or the other, usually $(H - EH)(W - EW) > 0$, and though there
will be a number of exceptions, they will be rare enough so that 

$E[(H - EH)(W - EW)] > 0$.[Of course, the *magnitude* of $(H - EH)(W - EW)$
plays a role too.]{.column-margin} 

In other words, $Cov(H,W) > 0$.  Similarly if $U$ is often large when
$V$ is small, and vice versa, we will likely have $Cov(H,W) < 0$.

If this sounds like correlation to you, then your hunch is correct.
Covariance will indeed later lead to the concept of correlation, but
that intuition will serve us now.

Note some properties of scalar covariance. 

* Symmetry: 

$$
Cov(U,V) = Cov(V,U)
$$

* $Cov$ is bilinear: 

$$
Cov(aU,bV) = ab ~ Cov(U,V)
$$

* Variance as a special case:

$$
Cov(U,U) = Var(U)
$$

* Cross-product term:

$$
Var(U+V) = Var(U) + Var(V) + 2 Cov(U,V)
$${#eq-crossprod}

* "Short cut" formula: 

$$
Cov(U,V) = E(UV) - (EU) (EV)
$${#eq-covuv} 

### Covariance matrices

The above was a review of the notion of covariance between two scalar
random variables. We now turn to defining covariance for a random
vector, which will turn out to be a matrix.

The relations between the various components of a random vector $X$ are
often characterized by the *covariance matrix* of $X$, whose entries
consist of scalar covariances between pairs of components of a random
vector.  [The notation is somewhat overloaded.  "Cov" refers both to the
covariance between two random variables, say height and weight, and to
the covariance matrix of a random vector. But it will always be clear
from context which one is being discussed.  ]{.column-margin} It is
defined as follows for a $k$-component random vector $X$. The
covariance matrix, denoted by $Cov(X)$, is a $k \times k$ matrix, and
for $1 \leq i,j \leq k$, its row $i$, column $j$ element is

$$
Cov(X)_{ij} = Cov(X_i,X_j)
$$

As an example, here is data on major league baseball players:

```{r}
library(qeML) 
data(mlb1) 
head(mlb1) 
hwa <- mlb1[,-1] 
cov(hwa) 
cor(hwa)
```

Again, this is sample data. We find that the *sample estimate*
of the covariance between height and weight is 25.61130.

We'll be discussing more of this later, but what about that
negative correlation between height and age? It's near 0, and this could
be a sampling artifact, but another possibility is that in this sport,
shorter players do not survive as well.

Properties of the matrix version of covariance:

* Matrix form of definition: 

  $$ Cov(X) = E[(X - X) (X - EX)'] 
  $${#eq-matrixdef}
 
  (Note the dimensions: $X$ is a column vector, say $k \times 1$,
  so $(X - X) (X - EX)'$ is $k \times k$.  The expected value is
  then of that size as well.)

* For statistically independent random vectors $Q$ and $W$ of the same length,

$$
Cov(Q+W) = Cov(Q) + Cov(W) 
$${#eq-indepcov}

* For any nonrandom scalar $c$, and $Q$ a random vector, we have 

$$
Cov(cQ) = c^2 Cov(Q)
$$

* Say we have a random vector $X$, of length $k$, and a nonrandom matrix
  $A$ of size $m \times k$. Then $A X$ is a new random vector $Y$ of $m$
  components. It turns out that 

  $$
  Cov(Y) = A Cov(X) A'
  $${#eq-acova}

  The proof is straightforward but tedious, and will be omittted.

* $Cov(X)$ is a symmetric matrix. This follows from the symmetry of
  the definition.

* The diagonal elements of $Cov(X)$ are the variances of the random
  variables $X_i$. This follows from $Cov(U,U) = Var(U)$ for scalar $U$.

* If $X$ is a vector of length 1, i.e. a number, then

$$
Cov(X) = Var(X)
$$

* For any length-$k$ column vector $a$,

$$
Var(a'X) = a' ~ Cov(X) ~ a
$${#eq-quadform}

* Since variance is nonnegative, we thus see that  $Cov(X)$ is *nonnegative
  definite*, meaning that for any length-$k$ column vector $a$

$$
a' Cov(X) a \geq 0
$${#eq-covnndef}

* More generally, for any compatible constant vectors $a$ and $b$,

$$
Cov(a'X,b'X) = a' Cov(X) b
$$

* For any constant (i.e. nonrandom) $m \times k$ matrix $A$,

$$
Cov(AX) = A Cov(X) A'
$${#eq-covax} 

### Cross-covariance

The matrix $Cov(X)$ represents the covariance of a vector $X$
*with itself*. But we can also speak of the covariance of one vector $X$
with another vector $Y$, termed the *cross-covariance* between them.
Say $X$ and $Y$ are of lengths $m$ and $n$. Then $Cov(X,Y)$ will be of
size $m \times n$, with

$$
Cov(X,Y)_{ij} = Cov(X_i,Y_j)
$$

where the latter covariance is scalar.

## The Multivariate Normal Distribution Family{#sec-mvn}

The familiar "bell-shaped curve" refers to the *normal* (or *Gaussian*)
family, whose densities have the form

$$
\frac{1}{\sigma \sqrt{2 \pi}}
e^{-0.5 (\frac{t-\mu}{\sigma})^2}
$$

The values of $\mu$ and $\sigma$ are the mean and standard deviation.
But what if we have a random vector, say of length $k$? Is there a
generalized normal family? 

### Example: k = 2

The answer is yes.  Here is an example for $k = 2$:

![3D bell density](Bell.png)

### General form

Well, then, what is the form of the $k$-dimensional density function?
Just as the univariate normal family is parameterized by mean and
variance, the multivariate one is parameterized via mean *vector* $\mu$
and covariance *matrix* $\Sigma$. The form is 

$$
(2\pi)^{-k/2} \det(\Sigma)^{-k/2}
e^{-0.5 (t-\mu)'(\Sigma)^{-1}(t-\mu)}
$${#eq-mvndensity}

Note the intuition:

* Instead of $1/\sigma^2$, i.e. instead of dividing by variance,
we "divide by $\Sigma$," intuitively viewing matrix inverse as a
"reciprocal" of a matrix.

* In other words, covariance matrices operate roughly like
generalized variances.

* Instead of squaring the scalar $t - \mu$, we "square" it in the vector
case by peforming a $w'w$ operation, albeit with $\Sigma^{-1}$ in the
middle. 

Clearly, we should not stretch these analogies very far, but they do
help our intuition here.

### Properties

::: {#thm-conditMVnormal}

If a random vector is multivariate (MV) normally distributed, then the
conditional distribution of any one of its components $Y$, given the
others $X_{others} = t$ (note that $t$ is a vector if $k > 2$) has the
following properties:  

* It has a (univariate) normal distribution.

* Its mean $E(Y | X_{others} = t)$ is linear in $t$.

* Its variance $Var(Y | X_{others} = t)$ does not involve $t$.

These of course are the classical assumptions of linear regression
models: normality, linearity and homoskedasticity. They actually come
from the MV normal model.

More generally: Denote the mean vector and covariance matrix a random vector $X$
by $\mu$ and $\Sigma$.  Partition $X$ as

$$
\left (
\begin{array}{r}
X_1 \\
X_2 \\
\end{array}
\right )
$$

and partition $\mu$ and $\Sigma$ similarly. The conditional distribution
of $X_1$ given $X_2 = t$ is multivariate normal with these paramters:

$$
E(X_1 | X_2 = t) =
\mu_1 + \Sigma_{12} \Sigma_{22}^{-1} (t - \mu_2)
$${#eq-condmean}

$$
Cov(X_1 | X_2 = t) = \Sigma_{11} - \Sigma_{12} \Sigma_{22}^{-1} \Sigma_{21}
$${#eq-condcov}

:::

::: {.proof}
This comes out of writing down the conditional density (overall density
divided by marginal), and then doing some algebra.

$\square$

::: 

Again, note the absence of $t$ in @eq-condcov. 

Note too that the conditional covariance matrix, $\Sigma_{11}$ of $X_1$
given $X_2$,

$$
\Sigma_{11} - \Sigma_{12} \Sigma_{22}^{-1} \Sigma_{21}
$$

[Of course, this is just a rough intuitive view. One matrix is not
"smaller" than another. One way to make this mathematically rigorous
is to say that the matrix $B$ is smaller than (or equal to) the 
matrix $A$ if $A-B$ is nonnegative definite, i.e. $u'(A-B)u \geq 0$
for all vectors $u$. Since in this case $A-B$ is a covariance matrix,
thus nonnegative definite (see @eq-covnndef), our intuitive statement
would become rigorous under this definition.]{.column-margin}
is "smaller" than the unconditional one, $\Sigma_{11}$.  $X_1$ varies
less when we know something about $X_2$.

::: {#thm-xnormalthenaxnormal}

If $X$ is a multivariate-normal random vector, then so is $AX$ for any
conformable nonrandom matrix $A$.

::: 

::: {.proof}

Again, perform direct evaluation of the density.

$\square$
::: 

::: {#thm-mvuvnormal}

## Cramer-Wold Theorem

A random vector $X$ has a multivariate normal distribution if and only
if $w'X$ has a univariate normal distribution for all conformable
nonrandom vectors $w$.

:::

::: {.proof}

"Only if" follows from above. "If" part too complex to present here.

$\square$
::: 

::: {#thm-mvclt}

## The Multivariate Central Limit Theorem

Let $X_1, X_2,...$ be a sequence on statistically independent random
vectors, with common distribution having mean vector $\mu$ and
covariance matrix $\Sigma$, but not necessarily MV normal. Write

$$
\bar{X} = \frac{X_1+...+X_n}{n}
$$

Then the distribution of the random vector

$$
W_n = \sqrt{n} (\bar{X} - \mu)
$$

goes to multivariate normal with the 0 vector as mean and[The usual form
would involve the "square root" of a matrix, but we will not discuss
that concept until a later chapter.]{.column-margin}
covariance matrix $\Sigma$.

:::

::: {.proof}

@thm-mvuvnormal reduces the problem to the univariate case, where we
know the Central Limit Theorem holds.

$\square$

:::

## Multinomial Random Vectors Have Approximate Multivariate Normal Distributions

Recall that a *multinomial* random vector is the mathematial [To be
consistent, view the binomial case as tabulating both successes and
failures, e.g. both heads and tails in coin flips. Then from that
viewpoint, the binomial case is multinomial with $k=2$.]{.column-margin}
analog of an R factor variable -- a categorical variable with $k$
levels/categories. Just as a binomial random variable represents the
number of "successes" in $n$ "trials," a multinomial random vector
represents the numbers of successes in each of the $k$ categories.

Let's write such a random vector as

$$
X = 
\left (
\begin{array}{r}
N_1 \\
... \\
N_k \\
\end{array}
\right )
$$

Let $p_i$ be the probability of a trial having outcome $i=1,...,k$. Note
the following:

* $N_1+...+N_k = n$

* The marginal distribution of $N_i$ is binomial, with success
  probability $p_i$ and $n$ trials.

So for instance if we roll a fair die 10 times, then $N_i$ is the
number of trials in which the roll's outcome was $i$ dots
and $p_i = 1/6$ , $i=1,2,3,4,5,6$.

Let's find $Cov(X)$. Define the *indicator* vector 

$$
I_i = 
\left (
\begin{array}{r}
I_{i1} \\
... \\
I_{ik}  \\
\end{array}
\right )
$$

Here $I_{ij}$ is 1 or 0, depending on whether trial $i$ resulted in
category $j$. (A 1 "indicates" that caregory $j$ occurred.)

The key is that

$$
X = \sum_{i=1}^n I_i
$${#eq-sumindic}

For instance, in the die-rolling example, the first component on the
right-hand side is the number of rolls in which we got 1 dot, and that
is by definition the same as $N_1$, the first component of $X$.

So there we have it -- $X$ is a sum of independent, identically
distributed random vectors, so by the Multivariate Central Limit
Theorem, $X$ has an approximate multivariate normal distribution.
Now, what are the mean vector and covariance matrix in that
distribution?

From our discussion above, we know that

$$
EX = 
\left (
\begin{array}{r}
np_1 \\
... \\
np_k \\
\end{array}
\right )
$$

What about $Cov(X)$? Again, recognizing that @eq-sumindic is a sum of
independent, identically distributed terms, @eq-indepcov tells us that 

$$
Cov(X) = Cov(\sum_{i=1}^n I_i) = \sum_{i=1}^n Cov(I_i) = n Cov(I_1),
$$

that last equality reflecting that the $I_i$ are identically
distributed (the trials all have the same probabilistic behavior).

Now to evaluate that covariance matrix, consider two specific elements
$I_{1j}$ and $I_{1m}$ of $I_1$. Recall, those elements are equal to 1 or 0,
depending on whether the first trial results in Categories j and m,
respectively. Then what is $Cov(I_{1j},I_{1m})$? From @eq-covuv, we have 

$$
Cov(I_{1j},I_{1m}) = E(I_{1j} I_{1m}) - (EI_{1j}) (EI_{1m})
$${#eq-covjm}

Consider the two cases:

* $i=j$: Here $E(I_{1j}^2) = E(I_{1j}) = p_j$. Thus 

$$
Cov(I_{1j},I_{1m}) = p_j (1-p_j)
$$

* $i \neq j$: Each $I_r$ consists of one 1 and $k-1$ 0s. Thus
$E(I_{1j} I_{1m}) = 0$ and 

$$
Cov(I_{1j},I_{1m}) = -p_j p_m
$$

## Your Turn

❄️  **Your Turn:** In @eq-mvndensity with $k = 2$, write $t = (t_1,t_2)$,
and consider the quantity in the exponent,

$$
(t-\mu)'(\Sigma)^{-1}(t-\mu)
$$

Say we set this quantity to some constant $c$, then graph the locus of
points $t$ in the $t_1,t_2$ plane. What geometric figure would we get?

❄️  **Your Turn:** In @thm-conditMVnormal, suppose 

$$
\mu =
\left (
\begin{array}{r}
1.5 \\
8.0 \\
\end{array}
\right )
$$

and

$$
\Sigma =
\left (
\begin{array}{rr}
5.2 & 6.2 \\
6.2 & 20.1  \\
\end{array}
\right )
$$

Find the regression line 

$$
\textrm{ mean Y } = a + bX
$$

❄️  **Your Turn:** Show that in the scalar context,

$$
Cov(X,Y) = E(XY) - EX ~ EY
$$

❄️  **Your Turn:** Show that in the vector context,

$$
Cov(X) = E(X X') - (EX) (EX)'
$$

❄️  **Your Turn:** Suppose

$$
W = X' \beta + \alpha S
$$

for a random vector $X$, a scalar random variable $S$, a nonrandom
vector $\beta$ and a nonrandom scalar $\alpha$. Show that

$$
Cov(W,S) = \beta' Cov(X,S) + \alpha Var(S)
$$




