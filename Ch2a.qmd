

```{r}
#| include: false
library(dsld)
library(qeML)
```

{{< pagebreak >}}

# Covariance Matrices, MV Normal Distribution, Delta Method {#sec-covar}

::: {.callout-note}

## Goals of this chapter:  

A central entity in multivariate analysis is that of the *covariance
matrix*. In this chapter we define the term, list its many properties,
and show its role in the multivariate analog of the normal distribution
family. We close the chapter with an application to the *delta method*,
one of the most useful simple tools in statistics.

:::

## Random Vectors

You are probably familiar with the concept of a random variable, but of
even greater importance is random *vectors*.

Say we are jointly modeling height, weight, age, systolic blood pressure
and cholesterol, and are especially interested in relations between
these quantities.  We then have the random vector

$$
 X
=
 \left (
 \begin{array}{r}
 X_1 \\
 X_2 \\
 X_3 \\
 X_4 \\
 X_5 \\
  \end{array}
 \right )
=
 \left (
 \begin{array}{r}
 \textrm{height} \\
 \textrm{weight} \\
 \textrm{age} \\
 \textrm{bp} \\
 \textrm{chol} \\
  \end{array}
 \right )
$$

### Sample vs. population

We may observe $n$ realizations of $X$ in the form of sample data, say
on $n = 100$ people. In the statistics world, we treat this data as a
random sample from some population, say all Americans. Usually, we are
just given the data rather then having actual random sampling, but this
view recognizes that there are a lot more people out there than our
data.

We speak of estimating population quantities. For instance, we can
estimate the population value E(X1), i.e. mean of $X_1$ throughout the
population, by the sample analog,

$$
\frac{1}{n} \sum_{i=1}^n X_{1j}
$$

where $X_{ij}$ denotes the value of $X_i$ for the $j^{th}$ person in our
sample.

By contrast, this view is rarely taken in the machine learning
community. The data is the data, and the fact that it is a small subset
of a much larger group is irrelevant. They will often allude to the
randomness of the data by mentioning the "data generating mechanism."



## Covariance{#sec-cov}

### Scalar covariance

Recall first the notion in statistics of *covariance*:  Given a pair of
random variables $U$ and $V$, their covariance is defined by

$$
Cov(U,V) = E[(U - EU)(V - EV)]
$$

Loosely speaking, this measures the degree to which the two random
variables vary together. Consider for instance human height $H$ and $W$.
Taller people tend to also be heavier. Say we sample many people from a
population. Most of those who are taller than average, i.e. $H > EH$,
will also be heavier than average, $W > EW$, making $(H - EH)(W - EW) >
0$.  Similarly, shorter people tend to be lighter, i.e. we often have $H
< EH$ and $W < EW$, but then we still have $(H - EH)(W - EW) > 0$. So,
one way or the other, usually $(H - EH)(W - EW) > 0$, and though there
will be a number of exceptions, they will be rare enough so that 

$E[(H - EH)(W - EW)] > 0$.[Of course, the *magnitude* of $(H - EH)(W - EW)$
plays a role too.]{.column-margin} 

In other words, $Cov(H,W) > 0$.  Similarly if $U$ is often large when
$V$ is small, and vice versa, we will likely have $Cov(H,W) < 0$.

If this sounds like correlation to you, then your hunch is correct.
Covariance will indeed later lead to the concept of correlation, but
that intuition will serve us now.

Note some properties of scalar covariance. 

* Symmetry: 

$$
Cov(U,V) = Cov(V,U)
$$

* $Cov$ is bilinear: 

$$
Cov(aU,bV) = ab Cov(U,V)
$$

* Variance as a special case:

$$
Cov(U,U) = Var(U)
$$

* Cross-product term:

$$
Var(U+V) = Var(U) + Var(V) + 2 Cov(U,V)
$$

* "Short cut" formula: 

$$
Cov(U,V) = E(UV) - (EU) (EV)
$${#eq-covuv} 

### Covariance matrices

The relations between the various components of $X$ are often
characterized by the *covariance matrix* of $X$, whose entries consist
of scalar covariances between pairs of components of a random vector.
[The notation is somewhat overloaded.  "Cov" refers both to the
covariance between two random variables, say height and weight, and to
the covariance matrix of a random vector. But it will always
be clear from context which one is being discussed.  ]{.column-margin}
It is defined as follows for a $k$-component random vector $X$ . The
covariance matrix, denoted by $Cov(X)$, is a $k \times k$ matrix,
and for $1 \leq i,j \leq k$, its row $i$, column $j$ element is

$$
Cov(X)_{ij} = Cov(X_i,X_j)
$$

As an example, here is data on major league baseball players:

```{r}
library(qeML) 
data(mlb1) 
head(mlb1) 
hwa <- mlb1[,-1] 
cov(hwa) 
cor(hwa)
```
Again, we'll be discussing more of this later, but what about that
negative correlation between height and age? It's near 0, and this could
be a sampling artifact, but another possibility is that in this sport,
shorter players do not survive as well.

Properties of the matrix version of covariance:

* Matrix form of definition: 

  $$ Cov(X) = E[(X - X) (X - EX)'] 
  $${#eq-matrixdef}
 
  (Note the dimensions: $X$ is a column vector, say $k \times 1$,
  so $(X - X) (X - EX)'$ is $k \times k$.  The expected value is
  then of that size as well.)

* For statistically independent random vectors $Q$ and $W$ of the same length,

$$
Cov(Q+W) = Cov(Q) + Cov(W) 
$${#eq-indepcov}

* For any nonrandom scalar $c$, and $Q$ a random vector, we have 

$$
Cov(cQ) = c^2 Cov(Q)
$$

* Say we have a random vector $X$, of length $k$, and a nonrandom matrix
  $A$ of size $m \times k$. Then $A X$ is a new random vector $Y$ of $m$
  components. It turns out that 

  $$
  Cov(Y) = A Cov(X) A'
  $${#eq-acova}

  The proof is straightforward but tedious, and will be omittted.

* $Cov(X)$ is a symmetric matrix. This follows from the symmetry of
  the definition.

* The diagonal elements of $Cov(X)$ are the variances of the random
  variables $X_i$. This follows from $Cov(U,U) = Var(U)$ for scalar $U$.

* If $X$ is a vector of length 1, i.e. a number, then

$$
Cov(X) = Var(X)
$$

* For any length-$k$ column vector $a$,

$$
Var(a'X) = a' ~ Cov(X) ~ a
$${#eq-quadform}

* Thus $Cov(X)$ is *nonnegative definite*, meaning that for any length-$k$
column vector $a$

$$
a' Cov(X) a \geq 0
$$

* More generally, for any compatible constant vectors $a$ and $b$,

$$
Cov(a'X,b'X) = a' Cov(X) b
$$

* For any constant (i.e. nonrandom) $m \times k$ matrix $A$,

$$
Cov(AX) = A Cov(X) A'
$${#eq-covax} 

### Cross-covariance

The matrix $Cov(X)$ represents the covariance of a vector $X$
*with itself*. But we can also speak of the covariance of one vector $X$
with another vector $Y$, termed the *cross-covariance* between them.
Its definition is a natural extension of covariance matrices:

$$
Cov(X,Y) = E[(X - EX) (Y- EY)]
$$

Say $X$ and $Y$ are of lengths $m$ and $n$. Then $Cov(X,Y)$ will be of
size $m \times n$, with

$$
Cov(X,Y)_{ij} = Cov(X_i,Y_j)
$$

where the latter covariance is scalar.

## The Multivariate Normal Distribution Family{#sec-mvn}

The familiar "bell-shaped curve" refers to the *normal* (or *Gaussian*)
family, whose densities have the form

$$
\frac{1}{\sigma \sqrt{2 \pi}}
e^{-0.5 (\frac{t-\mu}{\sigma})^2}
$$

The values of $\mu$ and $\sigma$ are the mean and standard deviation.
But what if we have a random vector, say of length $k$? Is there a
generalized normal family? 

### Example: k = 2

The answer is yes.  Here is an example for $k = 2$:

![3D bell density](Bell.png)

### General form

Well, then, what is the form of the $k$-dimensional density function?
Just as the univariate normal family is parameterized by mean and
variance, the multivariate one is parameterized via mean vector $\mu$
and covariance matrix $\Sigma$. The form is 

$$
(2\pi)^{-k/2} \det(\Sigma)^{-k/2}
e^{-0.5 (t-\mu)'(\Sigma)^{-1}(t-\mu)}
$${#eq-mvndensity}

Note the intuition:

* Instead of $1/\sigma^2$, i.e. instead of dividing by variance,
we "divide by $\Sigma$," intuitively viewing matrix inverse as a
"reciprocal" of a matrix.

* In other words, covariance matrices operate roughly like
generalized variances.

* Instead of squaring the scalar $t - \mu$, we "square" it in the vector
case by peforming a $w'w$ operation, albeit with $\Sigma^{-1}$ in the
middle. 

Clearly, we should not stretch these analogies very far, but they do
help our intuition here.

### Properties

::: {#thm-conditMVnormal}

If a random vector is MV normally distributed, then the conditional
distribution of any one of its components $Y$, given the others
$X_{others} = t$ (note that $t$ is a vector if $k > 2$) has the
following properties:  

* It has a (univariate) normal distribution.

* Its mean $E(Y | X_{others}) = t$ is linear in $t$.

* Its variance $Var(Y | X_{others} = t)$ does not involve $t$.

These of course are the classical assumptions of linear regression
models. They actually come from the MV normal model.

Specifics: Denote the mean vector and covariance matrix a random vector $X$
by $\mu$ and $\Sigma$.  Partition $X$ as

$$
\left (
\begin{array}{r}
X_1 \\
X_2 \\
\end{array}
\right )
$$

and partition $\mu$ and $\Sigma$ similarly. The conditional distribution
of $X_1$ given $X_2 = t$ is multivariate normal with these paramters:

$$
E(X_1 | X_2 = t) =
\mu_1 + \Sigma_{12} \Sigma_{22}^{-1} (t - \mu_2)
$${#eq-condmean}

$$
Cov(X_1 | X_2 = t) = \Sigma_{11} - \Sigma_{12} \Sigma_{22}^{-1} \Sigma_{21}
$${#eq-condcov}

:::

Note the absence of $t$ in @eq-condcov. Note too that the unconditional
covariance matrix, $\Sigma_{11}$ becomes "smaller" when we know
something about $X_2$.

::: {.proof}
This comes out of writing down the conditional density (overall density
divided by marginal), and then doing some algebra.

$\square$

::: 

::: {#thm-xnormalthenaxnormal}

If $X$ is a multivariate-normal random vector, then so is $AX$ for any
conformable nonrandom matrix $A$.

::: 

::: {.proof}

Again, perform direct evaluation of the density.

$\square$
::: 

::: {#thm-mvuvnormal}

## Cramer-Wold Theorem

A random vector $X$ has a multivariate normal distribution if and only
if $w'X$ has a univariate normal distribution for all conformable
nonrandom vectors $w$.

:::

::: {.proof}

"Only if" follows from above. "If" part too complex to present here.

% NM
$\square$
::: 

::: {#thm-mvclt}

## The Multivariate Central Limit Theorem

Let $X_1, X_2,...$ be a sequence on statistically independent random
vectors, with common distribution multivariate normal with mean vector
$\mu$ and covariance matrix $\Sigma$. Write

$$
\bar{X} = \frac{X_1+...+X_n}{n}
$$

Then the distribution of the random vector

$$
W_n = \sqrt{n} (\bar{X} - \mu)
$$

goes to multivariate normal with the 0 vector as mean and[The usual form
would involve the "square root" of a matrix, but we will not discuss
that concept until a later chapter.]{.column-margin}
covariance matrix $\Sigma$.

:::

::: {.proof}

@thm-mvuvnormal reduces the problem to the univariate case, where we
know the Central Limit Theorem holds.

$\square$

:::

## Multinomial Random Vectors Have Approximate Multivariate Normal Distributions

Recall that a *multinomial* random vector is the mathematial
analog of an R factor variable -- a categorical variable with $k$
levels/categories. Just as a binomial random variable represents the
number of "successes" in $n$ "trials," a multinomial random vector
represents the numbers of successes in each of the $k$ categories.

Let's write such a random vector as

$$
X = 
\left (
\begin{array}{r}
N_1 \\
... \\
N_k \\
\end{array}
\right )
$$

Let $p_i$ be the probability of a trial having outcome $i-1,...,k$. Note
the following:

* $N_1+...+N_k = n$

* The marginal distribution of $N_i$ is binomial, with success
  probability $p_i$ and $n$ trials.

So for instance if we roll a fair die 10 times, then $N_i$ is the
number of trials in which the roll's outcome was $i$ dots
and $p_i = 1/6$ , $i=1,2,3,4,5,6$.

Let's find $Cov(X)$. Define the *indicator* vector 

$$
I_i = 
\left (
\begin{array}{r}
I_{i1} \\
... \\
I_{ik}  \\
\end{array}
\right )
$$

Here $I_{ij}$ is 1 or 0, depending on whether trial $i$ resulted in
category $j$. (A 1 "indicates" that caregory $j$ occurred.)

The key is that

$$
X = \sum_{i=1}^n I_i
$${#eq-sumindic}

For instance, in the die-rolling example, the first component on the
right-hand side is the number of rolls in which we got 1 dot, and that
is by definition the same as $N_1$, the first component of $X$.

So there we have it -- $X$ is a sum of independent, identically
distributed random vectors, so by the Multivariate Central Limit
Theorem, $X$ has an approximate multivariate normal distribution.
Now, what are the mean vector and covariance matrix in that
distribution?

From our discussion above, we know that

$$
EX = 
\left (
\begin{array}{r}
np_1 \\
... \\
np_k \\
\end{array}
\right )
$$

What about $Cov(X)$? Again, recognizing that @eq-sumindic is a sum of
independent terms, @eq-indepcov tells us that 

$$
Cov(X) = Cov(\sum_{i=1}^n I_i) = \sum_{i=1}^n Cov(I_i) = n Cov(I_1),
$$

that last equality reflecting that the $I_i$ are identically
distributed (the trials all have the same probabilistic behavior).

Now to evaluate that covariance matrix, consider two specific elements
$I_{1j}$ and $I_{im}$ of $I_1$. Recall, those elements are equal to 1 or 0,
depending on whether the first trial results in Categories j and m,
respectively. Then what is $Cov(I_{1j},I_{1m})$? From @eq-covuv, we have 

$$
Cov(I_{1j},I_{1m}) = E(I_{1j} I_{1m}) - (EI_{1j}) (EI_{1m})
$${#eq-covjm}

Consider the two cases:

* $i=j$: Here $E(I_{1j}^2) = E(I_{1j}) = p_j$. Thus 

$$
Cov(I_{1j},I_{1m}) = p_j (1-p_j)
$$

* $i \neq j$: Each $I_s$ consists of one 1 and $k-1$ 0s. Thus
$E(I_{1j} I_{1m}) = 0$ and 

$$
Cov(I_{1j},I_{1m}) = -p_j p_m
$$



## The Delta Method

This is one of the most useful simple tools in statistics.  

### Review: confidence intervals, standard errors

To set the stage, let's review the statistical concepts of 
*confidence interval* and *standard error*. Say we have
an estimator $\widehat{\theta}$ of some population parameter $\theta$,
e.g.\ $\bar{X}$ for a population mean $\mu$.

* Loosely speaking, the term *standard error* of is our estimate of
  $\sqrt{Var(\widehat{\theta})}$.  More precisely, suppose that
  $\widehat{\theta}$ is asymptotically normal.  The standard error is an
  estimate of the standard deviation of that normal distribution. For
  this reason, It is customary to write $AVar(\widehat{\theta})$ 
  rather than $Var(\widehat{\theta})$.

  This can be used to form a confidence interval (see below), but 
  also stands on its own as an indication of the accuracy of 
  $\widehat{\theta}$.

* A, say 95%, confidence interval for $\mu$ is then

$$
\widehat{\theta} \pm 1.96 \widehat{\theta}
$$

  The 95% figure means that of all possible samples of the given size
  from the population, 95% of the resulting confidence intervals will
  contain $\theta$.

### Delta method: motivating example

Now, for the delta method, as a first example, say we are estimating a
population mean $\mu$ and are also interested in estimating $\log(\mu)$. 

We will probably use the sample mean $\bar{X}$ to estimate $\mu$, and
thus use $W = \log{\bar{X}}$ to estimate $\log(\mu)$. 
[If we just need to form a confidence
interval for $\log(\mu)$, we can form a CI for $\mu$ and then take the
log of both endpoints. But again, standard errors are of interest in
their own right.]{.column-margin}
But how do we obtain a standard error for $W$?

### Use of the Central Limit Theorem

The Central Limit Theorem tells as that $\bar{X}$ is asymptotically
normally distributed. But what about $\log{\bar{X}}$?

From calculus, we know that a smooth function $f$ can be written as a
Taylor series,

$$
f(x) = f(x_0) + f'(x_0) (x-x_0) + f''(x_0) (x-x_0)^2 /2 + ...
$$

where here "'" denotes derivative rather than matrix transpose.

In our case here, setting $f(t) = \log{t}$, $x_0 = \mu$ and $x =
\bar{X}$, we have

$$
W = \log{\mu} + \log'({\mu}) (\bar{X}-\mu) + \log''({\mu}) (\bar{X}-\mu)^2 /2 + ...
$$

and $\log'(t) = 1/t$ and so on.

The key point is that as n grows, $\bar{X}-\mu$ goes to 0, and
$(\bar{X}-\mu)^2$ goes to 0 even faster. Using theorems from probability
theory, one can show that, in the sense of distribution, 

$$
W \approx log(\mu) + log'(\mu) (\bar{X}-\mu)
$$

In other words, $W$ has an approximate normal distribution that
has mean $log(\mu)$ and variance

$$
\frac{1}{\mu^2} \sigma^2/n
$$

where $\sigma^2$ is the population variance $Var(X)$ . We estimate the
latter by the usual $S^2$ quantity, and thus have our standard error,

$$
\textrm{s.e.}(W) = \frac{S}{\bar{X} \sqrt{n}}
$$

### Use of the Multivariate Central Limit Theorem 

Now, what if the function $f$ has two arguments instead of one? The
above linear approximation is now

$$
f(v,w) \approx f(v_0,w_0) + f_1(v_0,w_0) (v-v_0) + f_2(v_0,w_0)(w-w_0)
$${#eq-twotaylor}

where $f_1$ and $f_2$ are partial derivatives,[A *partial derivative* of
a function of more than one variable is the derivative with respect to
one of those variables. E.g. $\partial/\partial v ~ vw^2 =
w^2$ and $\partial/\partial w ~ vw^2 =
2vw$.]{.column-margin}

$$
f_1(v,w) = \frac{\partial}{\partial v} f(v,w)
$$

$$
f_2(v,w) = \frac{\partial}{\partial w} f(v,w)
$$

So if we are estimating, for instance, a population quantity
$(\alpha,\beta)'$ by $(Q,R)'$, standard error of the latter is

$$
\sqrt{
f_1^2 (Q,R) AVar(Q) +
f_2^2 (Q,R) AVar(R) +
2 f_1(Q,R) f_2(Q,R) ACov(Q,R)
}
$$

As usual, use of matrix notation can help clean up messy expressions
like this. The *gradient* of $f$, say in the two-argument case as above,
is the vector

$$
\nabla f =
\left (
\begin{array}{r}
f_1 (v_0,w_0) \\
f_2 (v_0,w_0) \\
\end{array}
\right )
$$

so that @eq-twotaylor can be written as

$$
f(v,w) \approx f(v_0,w_0) + (\nabla f)' 
 \left (
 \begin{array}{r}
 v-v_0 \\
 w-w_0 \\
 \end{array}
 \right )
$$

Then from @eq-quadform, 

$$
AVar[f(Q,R)] = (\nabla f)' AV(Q,R) (\nabla f)
$${#eq-deltaquad}

### Example: ratio of two means

Say our sample data consists of mother-daughter pairs,

$$
\left (
\begin{array}{r}
M \\
D \\
\end{array}
\right )
$$

representing the heights of mother and daughter. Denote the population mean
vector by

$$
\nu =
\left (
\begin{array}{r}
\mu_M \\
\mu_D \\
\end{array}
\right )
$$

We might be interested in the ratio $\omega = \mu_D / \mu_M$.  Our
estimator will be $\widehat{\omega} = \bar{D} / \bar{M}$, the ratio of
the sample means.

So take $Q = \bar{D}$ and $R =  \bar{M}$. Then in @eq-deltaquad, with
$f(q,r) = q/r$

$$
\nabla{f} =
 \left (
 \begin{array}{r}
 1/r \\
 -q/r^2  \\
 \end{array}
 \right )
$$

which in our application here we would approximate by

$$
\nabla{f} =
 \left (
 \begin{array}{r}
 1/\bar{M} \\
 -\bar{D}/\bar{M}^2  \\
 \end{array}
 \right )
$$

As to $AVar(v,w)$ in @eq-deltaquad, we would use the multivariate analog
of the usual $S^2$ in the univariate case, taking advantage of
@eq-matrixdef. One must be careful, though, making sure we choose the
appropriate quantity for $AVar(v,w)$.

For instance, in our ratio example here, $(Q,R)'$ is $(\bar{M},\bar{D}$.
To obtain $AVar(v,w)$, let's first look at the covariance matrix $\Sigma$,

$$
\Sigma = E[(M - EM) (D - ED)']
$$

This is the average of $(M - EM) (D - ED)$ in the population. The sample
analog average is[The reader may recall that in estmating a variance,
it is customary to divide by $n-1$ instead of $n$, due to unbiasedness.
The same is true for the multivariate analog of variance, i.e.
covariance matrices, but we will use $n$ to retain the sample analog
theme.]{.column-margin}

$$
\widehat{\Sigma} = 
\frac{1}{n}
\sum_{i=1}^n (M_i - \bar{M}) (D_i - \bar{D})'
$$

where $(M_i,D_i)$ represents the $i^{th}$ mother-daughter pair in our
dataset.

But $\widehat{{\Sigma}}$ is not our $AVar(v,w)$ here, because we need,
for instance, the variance of $\bar{M}$ rather than the variance of $M$.
Recall that the former is $1/n$ times the latter. So in this case we
have

$$
AVar(v,w) = \frac{1}{n} \widehat{\Sigma}
$$

So now we can obtain a standard error for $\widehat{\omega}$: 

$$
\sqrt{
\frac{1}{n}
\nabla f' \widehat{\Sigma} \nabla f
}
$$

from which we can form a confidence interval.

In R functions to do parametric regression modeling, Maximum Likelihood
Estimation and so on, $AVar(v,w)$ is available from the function's
return value.

## Example: Iranian Churn Data

Here we predict whether a telecom customer will move to another
provider. Here we illustrate how to obtain $AVar(v,w)$.

```{r}
data(IranianChurn)
glmOut <- glm(Exited ~ ., data = iranChurn, family = binomial)
vcov(glmOut)
```

There are several categorical variables here, so after expansion to
dummies, $AVar(v,w)$ is $12 \times 12$. This is the covariance
matrix for the vector of estimated logistic regression coefficients
$\widehat{\beta}$. There are many different functions $f(\beta)$ that
might be of interest.


## Example: Mother/Daughter Height Data

```{r}
library(WackyData)
data(Heights)
head(heights)
m <- heights[,1]
d <- heights[,2]
meanm <- mean(m)
meand <- mean(d)
fDel <- matrix(c(1/meanm,-meand/meanm^2),ncol=1)
n <- length(m)
sigma <- (1/n) * cov(cbind(m,d))
se <- sqrt(t(fDel) %*% sigma %*% fDel)
se
meanmd <-meanm / meand
meanmd
c(meanmd - 1.96*se, meanmd + 1.96*se)
```

## Regarding Those Pesky Derivatives

Though finding expressions for the derivatives in the above example was
not onerous, the function $f$ can be rather complex, with the
expressions for its derivatives even more complicated. Typically such
tedious and error-prone operations can be avoided, by having the
software calculate approximate derivatives.

Recall the definition of derivative:

$$
f'(x) = \lim_{w \rightarrow 0}
\frac{f(x+w) - f(x)}{w}
$$

So an aproximate value of $f'(x)$ is obtained by choosing some small
value of $w$ and evaluating 

$$
\frac{f(x+w) - f(x)}{w}
$$

Though of course there is an issue with one's choice of $w$, the point
is that one can code the software to find approximate derivatives
automatically using this device. *This is very common in Data Science
libraries.* 

For example, the R package **numDeriv** will compute numerical
derivatives.

## Your Turn

❄️  **Your Turn:** In the mother/daughter data, find the estimated
covariance between the two heights.

❄️  **Your Turn:** In @eq-mvndensity with $k = 2$, write $t = (t_1,t_2)$,
and consider the quantity in the exponent,

$$
(t-\mu)'(\Sigma)^{-1}(t-\mu)
$$

Say we set this quantity to some constant $c$, then graph the locus of
points $t$ in the $t_1,t_2$ plane. What geometric figure would we get?

❄️  **Your Turn:** In @thm-conditMVnormal, suppose 

$$
\mu =
\left (
\begin{array}{r}
1.5 \\
8.0 \\
\end{array}
\right )
$$

and

$$
\Sigma =
\left (
\begin{array}{rr}
5.2 & 6.2 \\
6.2 & 20.1  \\
\end{array}
\right )
$$

Find the regression line 

$$
\textrm{ mean Y } = a + bX
$$

❄️  **Your Turn:** Show that in the scalar context,

$$
Cov(X,Y) = E(XY) - EX ~ EY
$$

❄️  **Your Turn:** Show that in the vector context,

$$
Cov(X) = E(X X') - (EX) (EX)'
$$

❄️  **Your Turn:** Suppose

$$
W = X \beta + \alpha S
$$

for a random vector $X$, a scalar random variable $S$, a nonrandom
vector $\beta$ and a nonrandom scalar $\alpha$. Show that

$$
Cov(W,S) = \beta' Cov(X,S) + \alpha Var(S)
$$




