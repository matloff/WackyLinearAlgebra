

```{r}
#| include: false
library(dsld)
library(qeML)
```

{{< pagebreak >}}

# Covariance Matrices, Multivariate Normal Distribution, Delta Method

::: {.callout-note}

## Goals of this chapter:  

A central entity in multivariate analysis is that of the *covariance
matrix*. In this chapter we define the term, list its many properties,
and show its role in the multivariate analog of the normal distribution
family. We close the chapter with an application to the *delta method*,
one of the most useful simple tools in statistics.

:::

## Covariance Matrices{#sec-cov}

Recall the notion in statistics of *covariance*:  Given a pair of random
variables $U$ and $V$, their covariance is defined by

$$
Cov(U,V) = E[(U - EU)(V - EV)]
$$

Loosely speaking, this measures the degree to which the two random
variables vary together. Consider for instance human height $H$ and $W$.
Taller people tend to also be heavier. Say we sample many people from a
population. Most of those who are taller than average, i.e. $H > EH$
will also be heavier than average, $W > EW$, making $(H - EH)(W - EW) >
0$.  Similarly, shorter people tend to be lighter, but then we still
have $(H - EH)(W - EW) > 0$. So, usually $(H - EH)(W - EW) > 0$, and
though there will be a number of exceptions, they will be rare enough so
that $E[(U - EU)(V - EV)] > 0$.[Of course, the *magnitude* of $(H -
EH)(W - EW)$ plays a role too.]{.column-margin}

$V$ is usually large -- meaning
above its mean $EV$ -- when $U$ is large (i.e.\ above *its*
mean), and they are usually both small together. Then $U - EU$ and $V - EV$
are usually of the same sign, thus have a positive product. Then
$Cov(U,V) > 0$. If on the other hand, one is usually small when the
other is large and vice versa, $Cov(U,V) < 0$. This will later lead to
the concept of correlation, but that intuition will serve us now.

Note some properties of scalar covariance. 

* $Cov$ is bilinear, i.e. $Cov(aU,bV) = ab Cov(U,V)$

* $Cov(U,U) = Var(U)$

* $Var(U+V) = Var(U) + Var(V) + 2 Cov(U,V)$ 

* $Cov(U,V) = E(UV) - (EU) (EV)${#eq-covuv} 

The relations between the various components of $X$ are often
characterized by the *covariance matrix* of $X$, whose entries consist
of scalar covariances between pairs of components of a random vector.
[The definition is somewhat overloaded.  "Cov" refers both to the
covariance between two random variables, say height and weight, and to
the covariance of a random vector, which is a matrix. But it will always
be clear from context which one is being discussed.  ]{.column-margin}
It is defined as follows for a $k$-component random vector. The
covariance matrix, denoted by $Cov(X)$, is a $k \textrm{ x } k$ matrix,
and for $1 \leq i,j \leq k$,

$$
Cov(X_i,X_j) = E[(X_i - EX_i) (X_j - EX_j)]
$$

As an example, here is data on major league baseball players:

```{r}
library(qeML) 
data(mlb1) 
head(mlb1) 
hwa <- mlb1[,-1] 
cov(hwa) 
cor(hwa)
```
Again, we'll be discussing more of this later, but what about that
negative correlation between height and age? It's near 0, and this could
be a sampling artifact, but another possibility is that in this sport,
shorter players do not survive as well.

Properties of the matrix version of covariance:

* Matrix form of definition: 

$$ Cov(X) = E[(X - X) (X - EX)'] 
$${#eq-matrixdef}
 
  (Note the dimensions: $X$ is a column vector, say $k \textrm{ x } 1$,
  so $(X - X) (X - EX)'$ is $q \textrm{ x } q$.  The expected value is
  then of that size as well.)

* For statistically independent random vectors $Q$ and $W$ of the same length,

$$
Cov(Q+W) = Cov(Q) + Cov(W) 
$${#eq-indepcov}

* For any nonrandom scalar $c$, and $Q$ a random vector, we have $Cov(cQ) = c^2 Cov(Q)$.

* Say we have a random vector $X$, of length $k$, and a nonrandom matrix
  $A$ of sise $m \textrm{x} k$. Then $A X$ is a new random vector $Y$ of $m$
  components. It turns out that 

  $$
  Cov(Y) = A Cov(X) A'
  $${#eq-acova}

  The proof is straightforward but tedious, and it will be omittted.

* $Cov(X)$ is a symmetric matrix. This follows from the symmmetry of
the definition.

* The diagonal elements of $Cov(X)$ are the variances of the random
variables $X_i$. This follows from the definition of the variance of a
random variable.

* If $X$ is a vector of length 1, i.e. a number, then

$$
Cov(X) = Var(X)
$$

* For any length-$k$ column vector $a$,

$$
Var(a'X) = a' ~ Cov(X) ~ a
$${#eq-quadform}

* Thus $Cov(X)$ is *nonnegative definite*, meaning that for any length-$k$
column vector $a$

$$
a' Cov(X) a \geq 0
$$


## The Multivariate Normal Distribution Family{#sec-mvn}

The familiar "bell-shaped curve" refers to the *normal* (or *Gaussian*)
family, whose densities have the form

$$
\frac{1}{\sigma \sqrt{2 \pi}}
e^{-0.5 (\frac{t-\mu}{\sigma})^2}
$$

The values of $\mu$ and $\sigma$ are the mean and standard deviation.
But what if we have a random vector, say of length $k$? Is there a
generalized normal family? 

### Example: k = 2

The answer is yes.  Here is an example for $k = 2$:

![3D bell density](Bell.png)

### General form

Well, then, what is the form of the $k$-dimensional density function?
Just as the univariate normal family is parameterized by mean and
variance, the multivariate one is parameterized via mean vector $\mu$
and covariance matrix $\Sigma$. The form is 

$$
(2\pi)^{-k/2} \det(\Sigma)^{-k/2}
e^{-0.5 (t-\mu)'(\Sigma)^{-1}(t-\mu)}
$$

Note the intuition:

* Instead of $1/\sigma^2$, i.e. instead of dividing by variance,
we "divide by $\Sigma$," intuitively viewing matrix inverse as a
"reciprocal" of a matrix.

* In other words, covariance matrices operate roughly like
generalized variances.

* Instead of squaring the scalar $t - \mu$, we "square" it in the vector
case by peforming a $w'w$ operation, albeit with $\Sigma^{-1}$ in the
middle. 

Clearly, we should not stretch these analogies very far, but they do
help our intuition here.

### Properties

::: {#thm-conditMVnormal}

If a random vector is MV normally distributed, then the conditional
distribution of any one of its components $Y$, given the others
$X_{others} = t$ (note that $t$ is a vector if $k > 2$) has the
following properties:

* It has a (univariate) normal distribution.

* Its mean $E(Y | X_{others}) = t$ linear in $t$.

* Its variance $Var(Y | X_{others}) = t$ does not involve $t$.

These of course are the classical assumptions of linear regression
models. They actually come from the MV normal model.

:::

::: {.proof}
This comes out of writing down the conditional density (overall density
divided by marginal), and then doing some algebra.

$\square$

::: 

::: {#thm-xnormalthenaxnormal}

If $X$ is a multivariate-normal random vector, then so is $AX$ for any
conformable nonrandom matrix $A$.

::: 

::: {.proof}

Again, perform direct evaluation of the density.

$\square$
::: 

::: {#thm-mvuvnormal}

A random vector $X$ has a multivariate normal distribution if and only
if $w'X$ has a univariate normal distribution for all conformable
nonrandom vectors $w$.

:::

::: {#thm-mvclt}

## The Multivariate Central Limit Theorem

Let $X_1, X_2,...$ be a sequence on statistically independent random
vectors, with common distribution multivariate normal with mean vector
$\mu$ and covariance matrix $\Sigma$. Write

$$
\bar{X} = \frac{X_1+...+X_n}{n}
$$

Then the distribution of the random vector

$$
W_n = \sqrt{n} (\bar{X} - \mu)
$$

goes to multivariate normal with the 0 vector as mean and[The usual form
would involve the "square root" of a matrix, but we will not discuss
that concept until our chapter on inner product spaces.]{.column-margin}
covariance matrix $\Sigma$.

:::
`
::: {.proof}

@thm-mvuvnormal reduces the problem to the univariate case, where we
know the Central Limit Theorem holds.

$\square$

:::

## Multinomial Random Vectors Have Approximate Multivariate Normal Distributions

Recall that a *multinomial* random vector is the mathematial
analog of an R factor variable -- a categorical variable with $k$
levels/categories. Just as a binomial random variable represents the
number of "successes" in $n$ "trials," a multinomial random vector
represents the numbers of successes in each of the $k$ categories.

Let's write such a random vector as

$$
X = 
\left (
\begin{array}{r}
N_1 \\
... \\
N_k \\
\end{array}
\right )
$$

Let $p_i$ be the probability of a trial having outcome $i-1,...,k$. Note
the following:

* $N_1+...+N_k = n$

* The marginal distribution of $N_i$ is binomial, with success
  probability $p_i$ and $n$ trials.

So for instance if we roll a fair die 10 times, then $N_i$ is the
number of trials in which the roll's outcome was $i$ dots
and $p_i = 1/6$ , $i=1,2,3,4,5,6$.

Let's find $Cov(X)$. Define the *indicator* vector 

$$
I_i = 
\left (
\begin{array}{r}
I_{i1} \\
... \\
I_{ik}  \\
\end{array}
\right )
$$

Here $I_{ij}$ is 1 or 0, depending on whether trial $i$ resulted in
category $j$. (A 1 "indicates" that caregory $j$ occurred.)

The key is that

$$
X = \sum_{i=1}^n I_i
$${#eq-sumindic}

For instance, in the die-rolling example, the first component on the
right-hand side is the number of rolls in which we got 1 dot, and that
is by definition the same as $N_1$, the first component of $X$.

So there we have it -- $X$ is a sum of independent, identically
distributed random vectors, so by the Multivariate Central Limit
Theorem, $X$ has an approximate multivariate normal distribution.
Now, what are the mean vector and covariance matrix in that
distribution?

From our discussion above, we know that

$$
EX = 
\left (
\begin{array}{r}
np_1 \\
... \\
np_k \\
\end{array}
\right )
$$

What about $Cov(X)$? Again, recognizing that @eq-sumindic is a sum of
independent terms, @eq-indepcov tells us that 

$$
Cov(X) = Cov(\sum_{i=1}^n I_i) = \sum_{i=1}^n Cov(I_i) = n Cov(I_1),
$$

that last equality reflecting that the $I_i$ are identically
distributed (the trials all have the same probabilistic behavior).

Now to evaluate that covariance matrix, consider two specific elements
$I_{1j}$ and $I_{im}$ of $I_1$. Recall, that element is equal to 1 or 0,
depending on whether the first trial results in Categories j and m,
respectively. Then what is $Cov(I_{1j},I_{1m})$? From @eq-covuv, we have 

$$
Cov(I_{1j},I_{1m}) = E(I_{1j} I_{1m}) - (EI_{1j}) (EI_{1m})
$${#eq-covjm}

Consider the two cases:

* $i=j$: Here $E(I_{1j}^2) = E(I_{1j}) = p_j$. Thus 

$$
Cov(I_{1j},I_{1m}) = p_j (1-p_j)
$$

* $i \neq j$: Each $I_s$ consists of one 1 and $k-1$ 0s. Thus
$E(I_{1j} I_{1m}) = 0$ and 

$$
Cov(I_{1j},I_{1m}) = -p_j p_m
$$



## The Delta Method

This is one of the most useful simple tools in statistics.  

### Review: confidence intervals, standard errors

To set the stage, let's review the statistical concepts of 
*confidence interval* and *standard error*. Say we have
an estimator $\widehat{\theta}$ of some population parameter $\theta$,
e.g.\ $\bar{X}$ for a population mean $\mu$.

* Loosely speaking, the term *standard error* of is our estimate of
  $\sqrt{Var(\widehat{\theta}})$.  More precisely, suppose that
  $\widehat{\theta}$ is asymptotically normal.  The standard error is an
  estimate of the standard deviation of that normal distribution. For
  this reason, It is customary to write $AVar(\widehat{\theta})$ 
  rather than $Var(\widehat{\theta})$.

  This can be used to form a confidence interval (see below), but 
  also stands on its own as an indication of the accuracy of 
  $\widehat{\theta}$.

* A, say 95%, confidence interval for $\mu$ is then

$$
\widehat{\theta} \pm 1.96 \textrm{ s.e.}(\widehat{\theta})
$$

  The 95% figure means that of all possible samples of the given size
  from the population, 95% of the resulting confidence intervals will
  contain $\theta$.

### Delta method: motivating example

Now, for the delta method, as a first example, say we are estimating a
population mean $\mu$ and are also interested in estimating $\log(\mu)$. 

We will probably use the sample mean $\bar{X}$ to estimate $\mu$, and
thus use $W = \log{\bar{X}}$ to estimate $\log(\mu)$. 
[If we just need to form a confidence
interval for $\log(\mu)$, we can form a CI for $\mu$ and then take the
log of both endpoints. But again, standard errors are of interest in
their own right.]{.column-margin}
But how do we obtain a standard error for $W$?

### Use of the Central Limit Theorem

The Central Limit Theorem tells as that $\bar{X}$ is asymptotically
normally distributed. But what about $\log{\bar{X}}$?

From calculus, we know that a smooth function $f$ can be written as a
Taylor series,

$$
f(x) = f(x_0) + f'(x_0) (x-x_0) + f''(x_0) (x-x_0)^2 /2 + ...
$$

where here "'" denotes derivative.

In our case here, setting $f(t) = \log{t}$, $x_0 = \mu$ and $x =
\bar{X}$, we have

$$
W = \log{\mu} + \log'({\mu}) (\bar{X}-\mu) + \log''({\mu}) (\bar{X}-\mu)^2 /2 + ...
$$

and $\log'(t) = 1/t$ and so on.

The key point is that as n grows, $\bar{X}-\mu$ goes to 0, and
$(\bar{X}-\mu)^2$ goes to 0 even faster. Using theorems from probability
theory, one can show that, in the sense of distribution, 

$$
W \approx log(\mu) + log'(\mu) (\bar{X}-\mu)
$$

In other words, $W$ has an approximate normal distribution that
has mean $log(\mu)$ and variance

$$
\frac{1}{\mu^2} \sigma^2/n
$$

where $\sigma^2$ is the population variance. We estimate the latter by
the usual $S^2$ quantity, and thus have our standard error,

$$
\textrm{s.e.}(W) = \frac{S}{\bar{X} \sqrt{n}}
$$

## Use of the Multivariate Central Limit Theorem

Now, what if the function $f$ has two arguments instead of one? The
above linear approximation is now

$$
f(v,w) \approx f(v_0,w_0) + f_1(v_0,w_0) (v-v_0) + f_2(v_0,w_0)(w-w_0)
$${#eq-twotaylor}

where $f_1$ and $f_2$ are partial derivatives,[A *partial derivative* of
a function of more than one variable is the derivative with respect to
one of those variables. E.g. $\partial/\partial v ~ vw^2 =
w^2$ and $\partial/\partial w ~ vw^2 =
2vw$.]{.column-margin}

$$
f_1(v,w) = \frac{\partial}{\partial v} f(v,w)
$$

$$
f_2(v,w) = \frac{\partial}{\partial w} f(v,w)
$$

So if we are estimating, for instance, a population quantity
$(\alpha,\beta)'$ by $(Q,R)'$, its standard error is

$$
\sqrt{
f_1^2 (Q,R) AVar(Q) +
f_2^2 (Q,R) AVar(R) +
2 f_1(Q,R) f_2(Q,R) ACov(Q,R)
}
$$

As usual, use of matrix notation can help clean up messy expressions
like this. The *gradient* of $f$, say in the two-argument case as above,
is the vector

$$
\nabla f =
\left (
\begin{array}{r}
f_1 (v_0,w_0) \\
f_2 (v_0,w_0) \\
\end{array}
\right )
$$

so that @eq-twotaylor can be written as

$$
f(v,w) = f(v_0,w_0) + (\nabla f)' 
 \left (
 \begin{array}{r}
 v-v_0 \\
 w-w_0 \\
 \end{array}
 \right )
$$

Then from @eq-quadform, 

$$
AVar[f(v,w)] = (\nabla f)' AV(v,w) (\nabla f)
$${#eq-deltaquad}

*Example: Ratio of Two Means*

Say our sample data consists of mother-daughter pairs,

$$
\left (
\begin{array}{r}
M \\
D \\
\end{array}
\right )
$$

representing the heights of mother and daughter. Denote the population mean
vector by

$$
\nu =
\left (
\begin{array}{r}
\mu_M \\
\mu_D \\
\end{array}
\right )
$$

We might be interested in the ratio $\omega = \mu_D / \mu_M$.  Our
estimator will be $\widehat{\omega} = \bar{D} / \bar{M}$, the ratio of
the sample means.

Then in @eq-deltaquad, with $f(q,r) = q/r$

$$
\nabla{f} =
 \left (
 \begin{array}{r}
 1/r \\
 -q/r^2  \\
 \end{array}
 \right )
$$

which in our application here we would approximate by

$$
\nabla{f} =
 \left (
 \begin{array}{r}
 1/\bar{M} \\
 -\bar{D}/\bar{M}^2  \\
 \end{array}
 \right )
$$

As to $AVar(v,w)$ in @eq-deltaquad, we would use the multivariate analog
of the usual $S^2$ in the univeriate case, taking advantage of
@eq-matrixdef:

$$
\widehat{\Sigma} = 
\frac{1}{n-1}
\sum_{i=1}^n (X_i - \bar{X}) (X_i - \bar{X})'
$$

So now we have the estmated mean and variance, and can obtain a standard
error for $\widehat{\omega}$, from which we can form a confidence
interval.


## Your Turn

❄️  **Your Turn:** Show that in the scalar context,

$$
Cov(X,Y) = E(XY) - EX ~ EY
$$

❄️  **Your Turn:** Show that in the vector context,

$$
Cov(X) = E(X X') - (EX) (EX)'
$$

