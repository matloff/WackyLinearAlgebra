[
  {
    "objectID": "preface.html",
    "href": "preface.html",
    "title": "Preface",
    "section": "",
    "text": "Subtlety in the Title\nWelcome to my magnum opus! :-) I’ve written a number of books, but consider this one to be the most important.\nLet’s start with the title of this book, Powered by Linear Algebra: The central role of matrices and vector spaces in Data Science. It’s important to understand why the title is NOT “Linear Algebra for Data Scientists.” That latter would wrongly /connote that people in Data Science (DS)will first learn linear algebra purely as a branch of math in this book, with no hint of connections to DS, then apply that knowledge in subsequent DS courses. Instead, the goal in the title is to emphasize the fact that:",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "preface.html#subtlety-in-the-title",
    "href": "preface.html#subtlety-in-the-title",
    "title": "Preface",
    "section": "",
    "text": "Linear algebra is absolutely fundamental to the Data Science field. For us data scientists, it is “our” branch of math. Almost every concept in this book is first motivated by a Data Science application. Mastering this branch of math, which is definitely within the reach of all, pays major dividends.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "preface.html#philosophy",
    "href": "preface.html#philosophy",
    "title": "Preface",
    "section": "Philosophy",
    "text": "Philosophy\nI learned very early the difference between knowing the name of something and knowing something – physicist Richard Feynman\n…it felt random. “Follow these steps and you get the result you are looking for.” But why does it work? What possessed you to follow this path as opposed to any other? How might I have come up with this myself? – comment by a reader of a famous linear algebra book\nThis book does not allow rote memorization, merely “knowing the name of something.” The focus on How? and Why? is on every page.\nA fundamental philosophy of my book here is to avoid reader frustration. It’s easy to define, say the dimension of a vector subspace, but that’s definitely not enough. What is the underlying intuition? Why is the concept important, especially in Data Science?\nThe presentation of each concept in this book begins with a problem to be solved, almost always from Data Science, then leading up to a linear algebra solution. Basically, the math sneaks up on the reader, who suddenly realizes they’ve just learned a new general concept! And the reader knows where the concept fits into the Big Picture, and can distill the abstraction into an intuitive summary.\nExamples:\n\nIn Chapter 1, we use Markov chain transition matrices and network graph models (e.g. social networks) to motivate the notion of a matrix and matrix multiplication. An interest in finding the stationary distribution of a Markov chain then leads to the concept of matrix inverses. (Linear models are presented later, after groundwork of matrix rank is laid.)\nThe chapter on matrix rank starts with a dataset right off the bat, and shows that R’s linear model function lm fails if categorical variables are fully specified. This motivates the notion of rank, and the dataset dovetails with the theory throughout the chapter, which culminates in a proof that row rank equals column rank.\nThe chapter on eigenanalysis begins with explaining the goals of PCA (with a real dataset). We derive the first PC as a constrained maximization of variance, and behold! – the solution turns out to have the form \\(Ax = \\lambda x\\)! So eigenanalysis comes from solving a Data Science problem. PCA is later covered in detail in the following chapter, but with this motivation we develop the properties of eigenvalues and eigenvectors in the current chapter.\n\nFurthermore, our presentation of the linear algebra connections to Data Science goes far beyond the “obvious” ones of linear models, PCA and SVD. We discussion recommender systems, for instance, not only in connection to SVD and the like, but also as an application of shrinkage estimators.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "preface.html#who-is-this-book-for",
    "href": "preface.html#who-is-this-book-for",
    "title": "Preface",
    "section": "Who Is This Book For?",
    "text": "Who Is This Book For?\nOf course the book should work very well as a classroom textbook. If a Data Science or Statistics program requires linear algebra offered by a Math Department, the mathematical content of this book should be similar to that math course, but with much better student motivation due to the Data Science emphasis of the book. The “applications first” approach is key to that motivational power.\nActually, the applications-centered nature of the book should make teaching the course more rewarding for instructors as well, and the use of Quarto enables easy conversion to Powerpoint by instructors.\nI also hope the book’s emphasis on the How? and Why? especially appeals to do-it-yourselfers, those whose engagement in self-study is motivated by intellectual curiosity rather than a course grade.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "preface.html#prerequisite-background",
    "href": "preface.html#prerequisite-background",
    "title": "Preface",
    "section": "Prerequisite Background",
    "text": "Prerequisite Background\nBasic data science:\n\nCalculus.\nSome exposure to R is recommended, but the text can be read without it.\nBasics of random variables, expected value and variance.\n\nFor a quick, painless introduction to R, see my fasteR tutorial, say the first 8 lessons.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "preface.html#the-role-of-math-and-r",
    "href": "preface.html#the-role-of-math-and-r",
    "title": "Preface",
    "section": "The Role of Math (and R)",
    "text": "The Role of Math (and R)\nEarlier in this Preface, I said, “If a Data Science or Statistics program requires linear algebra offered by a Math Department, the mathematical content of this book should be similar to that math course…” So, the mathematics is definitely here, but one might also say, this book is “mathematical but not overly theoretical.”\nTheorems are mainly limited to results with practical importance. Among the Your Turn exercises at the end of each chapter, the ones requiring proofs are usually simple, of the “one liner” type. But the subject matter is indeed mathematical. Students are indeed expected to read and understand proofs, just as with the Mathematics Department course.\nThe goal is to develop in the reader mathematical skill and intuition into this powerful tool, rather than coding of linear algebra methods. Thus the many R examples are meant to make the mathematical concepts concrete, not as an “how to do linear algebra in R” book. In the software context, the Feynman quote above might be, “There is a difference between knowing how to use code libraries for something and knowing the core nature of that thing.” That said, the code examples do serve a vital role.\nThe applied nature of the book is a double-edged sword. It is has high value as a motivator, but understanding applications is actually more challenging than a purely mathematical treatment, not less so. Math is more crisply-defined, while applications can be “fuzzy.” Among the Your Turn exercises, many of the applied ones are somewhat open-ended, and they tend to be wordier than the theory ones.\nIn other words, the book is intended to arm students with usable practical insights, rather than merely satisfying some curricular requirement that will be quickly forgotten. Hence the needs for (a) developing student intuition and (b) nonpassive learning are paramount (as they should be in any Data Science course).",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "preface.html#r-packages-used",
    "href": "preface.html#r-packages-used",
    "title": "Preface",
    "section": "R Packages Used",
    "text": "R Packages Used\nqeML\nglmnet\nigraph\ndsld\nnetworkdata\npracma\nregclass\nWackyData",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "preface.html#data-availabilty",
    "href": "preface.html#data-availabilty",
    "title": "Preface",
    "section": "Data Availabilty",
    "text": "Data Availabilty\nThe datasets used are included with the above packages.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "preface.html#web-site",
    "href": "preface.html#web-site",
    "title": "Preface",
    "section": "Web Site",
    "text": "Web Site\ngithub.com/matloff/WackyLinear Algebra",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "preface.html#edition-number",
    "href": "preface.html#edition-number",
    "title": "Preface",
    "section": "Edition Number",
    "text": "Edition Number\nCurrently 1.0.0. Correction of typos etc. will usually increment the third digit.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "preface.html#permission-to-copy",
    "href": "preface.html#permission-to-copy",
    "title": "Preface",
    "section": "Permission to Copy",
    "text": "Permission to Copy\nThis work is licensed under Creative Commons Zero v1.0 Universal.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "preface.html#thanks",
    "href": "preface.html#thanks",
    "title": "Preface",
    "section": "Thanks",
    "text": "Thanks\nI deeply appreciate feedback from: Mike Hannon, Nick Knueppel, Joe Rickert and Noah Perry.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "Ch1.html",
    "href": "Ch1.html",
    "title": "2  Matrices and Vectors",
    "section": "",
    "text": "2.1 A Random Walk Model\nIn this chapter, we will take as our main application Markov chains, a statistical model having wide applications in medicine, economics and so on. It is very simple to explain, thus making it a good choice for introducing matrices.\nLet’s consider a random walk on {1,2,3,4,5} in the number line. Time is numbered 1,2,3,… Our current position is termed our state. The notation Xk = i means that at time k we are in state/position i.\nOur rule will be that at any time k, we flip a coin. If we are currently at position i, we move to either i+1 or i-1, depending on whether the coin landed heads or tails. The exceptions are k = 1 and k = 5, in which case we stay put if tails or move to the adjacent position if heads.\nWe can summarize the probabilities with a matrix, a two-dimensional array:\n\\[\nP_1 =\n\\left (\n\\begin{array}{rrrrr}\n0.5 & 0.5 & 0 & 0 & 0\\\\\n0.5 & 0 & 0.5 & 0 & 0\\\\\n0 & 0.5 & 0 & 0.5 & 0\\\\\n0 & 0 & 0.5 & 0 & 0.5\\\\\n0 & 0 & 0 & 0.5 & 0.5 \\\\\n\\end{array}\n\\right )\n\\]\nFor instance, look at row 2. There are 0.5 values in columns 1 and 3, meaning there is a 0.5 chance of a move 2 \\(\\rightarrow\\) 1, and a 0.5 chance of a move 2 \\(\\rightarrow\\) 3.\nWe use a subscript 1 here in \\(P_1\\), meaning “one step.” We go from, say, state 2 to state 1 in one step with probability 0.5. \\(P_1\\) is called the one-step transition matrix (or simply the transition matrix) for this process.\nNote that each row in a transition matrix must sum to 1. After all, from state i we must go somewhere.\nWhat about the two-step transition matrix \\(P_2\\)? For instance, what should be in the row 3, column 1 position in that matrix? From state 3, we could go to state 1 in two steps, by two tails flips of the coin. The probability of that is \\(0.5^2 = 0.25\\). So the row 3, column 1 element in \\(P_2\\) is 0.25. On the other hand, if from state 3 we flip tails then heads, or heads then tails, we are back to state 3. So, the row 3, column 3 element in \\(P_2\\) is 0.25 + 0.25 = 0.5.\nThe reader should verify the correctness here:\n\\[\nP_2 =\n\\left (\n\\begin{array}{rrrrr}\n0.5 & 0.25 & 0.25 & 0 & 0\\\\\n0.25 & 0.5 & 0 & 0.25 & 0\\\\\n0.25 & 0 & 0.5 & 0 & 0.25\\\\\n0 & 0.25 & 0 & 0.5 & 0.25\\\\\n0 & 0 & 0.25 & 0.25 & 0.5 \\\\\n\\end{array}\n\\right )\n\\]\nWell, finding two-step transition probabilities would be tedious in general, but it turns out that is a wonderful shortcut: Matrix multiplication. We will cover this in the next section, but first a couple of preliminaries.\nThe above random walk is a Markov chain. The Markov Property says that the system “has no memory.” If say we land at position 2, we will go to 1 or 3 with probability 1/2 no matter what the previous history of the system was; it doesn’t matter how we got to state 3. That in turn comes in this example from the independence of the successive coin flips.\nNotation: Individual elements of a matrix are usually written with double subscripts. For instance, a25 will mean the row 2, column 5 element of the matrix \\(A\\). If say \\(A\\) has more than 9 rows, its row 11, column 5 element is denoted by a11,5, using the comma to avoid ambiguity.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Matrices and Vectors</span>"
    ]
  },
  {
    "objectID": "Ch1.html#vectors",
    "href": "Ch1.html#vectors",
    "title": "2  Matrices and Vectors",
    "section": "2.2 Vectors",
    "text": "2.2 Vectors\nMatrices are two-dimensional arrays. One-dimensional arrays are called vectors, either in row or column form, e.g.\n\\[\nu = (12,5,13)\n\\]\nand\n\\[\nu =\n\\left (\n\\begin{array}{r}\n12 \\\\\n5 \\\\\n13 \\\\\n\\end{array}\n\\right )\n\\]\nPlease note:\n\nVectors may also be viewed as one-row or one-column matrices.\nWhen not otherwise stated, the term “vector” will mean column form.\nThe term scalar simply means a number, rather than a matrix or vector. It will be used quite frequently in this book.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Matrices and Vectors</span>"
    ]
  },
  {
    "objectID": "Ch1.html#sec-easyops",
    "href": "Ch1.html#sec-easyops",
    "title": "2  Matrices and Vectors",
    "section": "2.3 Addition and Scalar Multiplication",
    "text": "2.3 Addition and Scalar Multiplication\nVectors of the same length may be summed, in elementwise form, e.g.\n\\[\n\\left (\n\\begin{array}{r}\n12 \\\\\n5 \\\\\n13 \\\\\n\\end{array}\n\\right )\n+\n\\left (\n\\begin{array}{r}\n-3 \\\\\n6 \\\\\n18.2 \\\\\n\\end{array}\n\\right ) =\n\\left (\n\\begin{array}{r}\n9 \\\\\n11 \\\\\n31.2 \\\\\n\\end{array}\n\\right )\n\\]\nSimilarly, two matrices may be added, again in elementwise fashion, provided the number of rows is the same for both, as well as the same condition for number of columns.\nVectors and matrices can be multiplied by scalars, again elementwise, e.g.\n\\[\n0.3\n\\left (\n\\begin{array}{r}\n6 \\\\\n15 \\\\\n\\end{array}\n\\right ) =\n\\left (\n\\begin{array}{r}\n1.8 \\\\\n4.5 \\\\\n\\end{array}\n\\right )\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Matrices and Vectors</span>"
    ]
  },
  {
    "objectID": "Ch1.html#matrix-matrix-multiplication",
    "href": "Ch1.html#matrix-matrix-multiplication",
    "title": "2  Matrices and Vectors",
    "section": "2.4 Matrix-Matrix Multiplication",
    "text": "2.4 Matrix-Matrix Multiplication\nThis is the most fundamental operation in linear algebra. It is defined as follows:\n\nGiven matrix \\(A\\) of \\(k\\) rows and \\(m\\) columns and matrix \\(B\\) of \\(m\\) rows and \\(r\\) columns, the product \\(C = AB\\) is a \\(k \\times m\\) matrix, whose row \\(i\\), column \\(j\\) element is\n\\[\na_{i1} b_{1j} +\na_{i2} b_{2j} + ... +\na_{im} b_{mj}\n\\]\nThis is the “dot product” of row \\(i\\) of A and column \\(j\\) of B: Find the products of the paired elements in the two vectors, then sum.\n\nFor example, set\n\\[\nA = \\left (\n\\begin{array}{rrr}\n5 & 2 & 6 \\\\\n1 & 1 & 8 \\\\\n\\end{array}\n\\right )\n\\]\nand\n\\[\nB = \\left (\n\\begin{array}{rr}\n5 & -1 \\\\\n1 & 0 \\\\\n0 & 8 \\\\\n\\end{array}\n\\right )\n\\]\nLet’s find the row 2, column 2 element of \\(C = AB\\). Again, that means taking the dot product of row 2 of \\(A\\) and \\(column\\) 2 of \\(B\\), which we’ve highlighted below.\n\\[\nA = \\left (\n\\begin{array}{rrr}\n5 & 2 & 6 \\\\\n\\color{red}{1} & \\color{red}{1} & \\color{red}{1} \\\\\n\\end{array}\n\\right )\n\\]\nand\n\\[\nB = \\left (\n\\begin{array}{rr}\n5 & \\color{red}{-1} \\\\\n1 & \\color{red}{0} \\\\\n0 & \\color{red}{8} \\\\\n\\end{array}\n\\right )\n\\]\nThe value in question is then\n1 (-1) + 1 (0) + 1 (8) = 7\nLet’s check it, with R:\n\na &lt;- rbind(c(5,2,6),c(1,1,1))\nb &lt;- cbind(c(5,1,0),c(-1,0,8))\na %*% b\n\n     [,1] [,2]\n[1,]   27   43\n[2,]    6    7\n\n\nThe rbind and cbind functions (“row bind” and “column bind”) are very handy tools for creating matrices. The reader should make sure to check the other elements by hand.\n\n\n\n\n\n\nTip 2.1\n\n\n\nAlways keep in mind that in the matrix product \\(AB\\), the number of rows of \\(B\\) must equal the number of columns of \\(A\\). The two matrices are then said to be conformable.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Matrices and Vectors</span>"
    ]
  },
  {
    "objectID": "Ch1.html#the-identity-matrix",
    "href": "Ch1.html#the-identity-matrix",
    "title": "2  Matrices and Vectors",
    "section": "2.5 The Identity Matrix",
    "text": "2.5 The Identity Matrix\nThe identity matrix \\(I\\) of size \\(n\\) is the \\(n \\times n\\) matrix with 1s on the diagonal and 0s elsewhere. \\(IB = B\\) and \\(AI = A\\) for any conformable \\(A\\) and \\(B\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Matrices and Vectors</span>"
    ]
  },
  {
    "objectID": "Ch1.html#sec-introMCs",
    "href": "Ch1.html#sec-introMCs",
    "title": "2  Matrices and Vectors",
    "section": "2.6 Application to Markov Chain Transition Matrices",
    "text": "2.6 Application to Markov Chain Transition Matrices\nNow let’s return to the question of how to easily compute \\(P_2\\), the two-step transition matrix. It turns out that:\n\nLet P denote the transition matrix of a (finite-state) Markov chain. The k-step transition matrix is \\(P^k\\).\n\nAt first, this may seem amazingly fortuitous, but it makes sense in light of the “and/or” nature of the probability computations involved. Recall our computation for the row 1, column 2 element of \\(P_2\\) above. From state 1, we could either stay at 1 for one flip, then move to 2 on the second flip, or we could go to 2 then return to 1. Each of these has probability 0.5, so the total probability is\n\\[\n(0.5)(0.5) + (0.5)(0.5)\n\\]\nBut this is exactly the form of our “dot product” computation in the definition of matrix multiplication,\n\\[\na_{i1} b_{i1} +\na_{i2} b_{i1} + ... +\na_{m1} b_{m1}\n\\]\nThen \\(P^3\\) stores the 3-step probabilities and so on.\nStatisticians and computer scientists like to look at the asymptotic behavior of systems, meaning what happens to a quantity when time or size or some other value grows. Let’s see where we might be after say, 6 steps:\n\nmatpow &lt;- function(m,k) {\n   nr &lt;- nrow(m)\n   tmp &lt;- diag(nr)  # identity matrix\n   for (i in 1:k) tmp &lt;- tmp %*% m\n   tmp\n}\n\np1 &lt;- rbind(c(0.5,0.5,0,0,0), c(0.5,0,0.5,0,0), c(0,0.5,0,0.5,0), \n   c(0,0,0.5,0,0.5), c(0,0,0,0.5,0.5))\nmatpow(p1,6) \n\n         [,1]     [,2]     [,3]     [,4]     [,5]\n[1,] 0.312500 0.234375 0.234375 0.109375 0.109375\n[2,] 0.234375 0.312500 0.109375 0.234375 0.109375\n[3,] 0.234375 0.109375 0.312500 0.109375 0.234375\n[4,] 0.109375 0.234375 0.109375 0.312500 0.234375\n[5,] 0.109375 0.109375 0.234375 0.234375 0.312500\n\n\nSo for instance if we start at position 2, there is about an 11% chance that we will be at position 3 at time 6. What about time 25?\n\nmatpow(p1,25)\n\n          [,1]      [,2]      [,3]      [,4]      [,5]\n[1,] 0.2016179 0.2016179 0.1993820 0.1993820 0.1980001\n[2,] 0.2016179 0.1993820 0.2016179 0.1980001 0.1993820\n[3,] 0.1993820 0.2016179 0.1980001 0.2016179 0.1993820\n[4,] 0.1993820 0.1980001 0.2016179 0.1993820 0.2016179\n[5,] 0.1980001 0.1993820 0.1993820 0.2016179 0.2016179\n\n\nSo, no matter which state we start in, at time 25 we are about 20% likely to be at any of the states. In fact, as time \\(n\\) goes to infinity, this probability vector becomes exactly (0.20,0.20,0.20,0.20,0.20). It will be shown below that the vector of long-run state probabilities \\(\\nu\\) is the solution of\n\\[\nP \\nu = \\nu\n\\tag{2.1}\\]\nwhere \\(P\\) is the transition matrix for the Markov chain.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Matrices and Vectors</span>"
    ]
  },
  {
    "objectID": "Ch1.html#network-graph-models",
    "href": "Ch1.html#network-graph-models",
    "title": "2  Matrices and Vectors",
    "section": "2.7 Network Graph Models",
    "text": "2.7 Network Graph Models\nThere has always been lots of analysis of “Who is connected to whom,” but activity soared after the advent of Facebook and the film, A Social Network. See for instance Statistical Analysis of Network Data with R by Eric Kolaczy and Gábor Csárdi. As the authors say,\n\nThe oft-repeated statement that “we live in a connected world” perhaps best captures, in its simplicity why networks have come to hold such interest in recent years. From on-line social networks like Facebook to the World Wide Web and the Internet itself, we are surrounded by examples of ways in which we interact with each other. Similarly, we are connected as well at the level of various human institutions (e.g., governments), processes (e.g., economies), and infrastructures (e.g., the global airline network). And, of course, humans are surely not unique in being members of various complex, inter-connected systems. Looking at the natural world around us, we see a wealth of examples of such systems, from entire eco-systems, to biological food webs, to collections of inter-acting genes or communicating neurons.\n\nAnd of course, at the center of it all is a matrix! Here is why:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Matrices and Vectors</span>"
    ]
  },
  {
    "objectID": "Ch1.html#example-karate-club",
    "href": "Ch1.html#example-karate-club",
    "title": "2  Matrices and Vectors",
    "section": "2.8 Example: Karate Club",
    "text": "2.8 Example: Karate Club\nLet’s consider the famous Karate Club dataset:\n\n# remotes::install_github(\"schochastics/networkdata\") \nlibrary(networkdata)\ndata(karate)\nlibrary(igraph)\nplot(karate)\n\n\n\n\n\n\n\n\nThere is a link between node 13 and node 4, meaning that club members 13 and 4 are friends.This graph is undirected, as friendship is mutual. Many graphs are directed, but we will assume undirected here.\nSpecifically, the adjacency matrix has row i, column j element as 1 or 0, according to whether a link exists between nodes i and j.\n\nadjK &lt;- as_adjacency_matrix(karate)\nadjK\n\n34 x 34 sparse Matrix of class \"dgCMatrix\"\n                                                                         \n [1,] . 1 1 1 1 1 1 1 1 . 1 1 1 1 . . . 1 . 1 . 1 . . . . . . . . . 1 . .\n [2,] 1 . 1 1 . . . 1 . . . . . 1 . . . 1 . 1 . 1 . . . . . . . . 1 . . .\n [3,] 1 1 . 1 . . . 1 1 1 . . . 1 . . . . . . . . . . . . . 1 1 . . . 1 .\n [4,] 1 1 1 . . . . 1 . . . . 1 1 . . . . . . . . . . . . . . . . . . . .\n [5,] 1 . . . . . 1 . . . 1 . . . . . . . . . . . . . . . . . . . . . . .\n [6,] 1 . . . . . 1 . . . 1 . . . . . 1 . . . . . . . . . . . . . . . . .\n [7,] 1 . . . 1 1 . . . . . . . . . . 1 . . . . . . . . . . . . . . . . .\n [8,] 1 1 1 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n [9,] 1 . 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 . 1 1\n[10,] . . 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\n[11,] 1 . . . 1 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n[12,] 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n[13,] 1 . . 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n[14,] 1 1 1 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\n[15,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 1\n[16,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 1\n[17,] . . . . . 1 1 . . . . . . . . . . . . . . . . . . . . . . . . . . .\n[18,] 1 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n[19,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 1\n[20,] 1 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\n[21,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 1\n[22,] 1 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n[23,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 1\n[24,] . . . . . . . . . . . . . . . . . . . . . . . . . 1 . 1 . 1 . . 1 1\n[25,] . . . . . . . . . . . . . . . . . . . . . . . . . 1 . 1 . . . 1 . .\n[26,] . . . . . . . . . . . . . . . . . . . . . . . 1 1 . . . . . . 1 . .\n[27,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 . . . 1\n[28,] . . 1 . . . . . . . . . . . . . . . . . . . . 1 1 . . . . . . . . 1\n[29,] . . 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 . 1\n[30,] . . . . . . . . . . . . . . . . . . . . . . . 1 . . 1 . . . . . 1 1\n[31,] . 1 . . . . . . 1 . . . . . . . . . . . . . . . . . . . . . . . 1 1\n[32,] 1 . . . . . . . . . . . . . . . . . . . . . . . 1 1 . . 1 . . . 1 1\n[33,] . . 1 . . . . . 1 . . . . . 1 1 . . 1 . 1 . 1 1 . . . . . 1 1 1 . 1\n[34,] . . . . . . . . 1 1 . . . 1 1 1 . . 1 1 1 . 1 1 . . 1 1 1 1 1 1 1 .\n\nadjK[13,4]\n\n[1] 1\n\n\nAccordingly, row 13, column 4 does have a 1 entry.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Matrices and Vectors</span>"
    ]
  },
  {
    "objectID": "Ch1.html#the-role-of-matrix-multiplication-in-network-graph-models",
    "href": "Ch1.html#the-role-of-matrix-multiplication-in-network-graph-models",
    "title": "2  Matrices and Vectors",
    "section": "2.9 The Role of Matrix Multiplication in Network Graph Models",
    "text": "2.9 The Role of Matrix Multiplication in Network Graph Models\nAs is the case with Markov transition matrices, powers of an adjacency matrix can yield valuable information. In the Markov case, multiplication gives us sums of paired products, computing probabilities. What about the network graph case?\nHere products are of the form 0x0, 0x1, 1x0 or 1x1. If there is a nonzero entry m in row i, column j of the square of the adjacency matrix, that means there were m 1x1 products in that sum, which would correspond to m paths. Let’s look into this.\n\nadjK2 &lt;- adjK %*% adjK\n\nWe see that adjK2[11,1] is 2. Inspection of adjK shows that its row 11, columns 6 and 7 are 1s, and that rows 6 and 7, column 1 are 1s as well. So there are indeed two two-hop paths from node 11 to node 1, specifically \\(11 \\rightarrow 6 \\rightarrow 1\\) and \\(11 \\rightarrow 7 \\rightarrow 1\\). Thus the 2 we see in adjK2[11,1] was correct.\nIn other words, \\(A^k\\) for a network adjacency matrix \\(A\\) shows the number of paths from each node to each of the others.\nActually, what is typically of interest is connectivity rather than number of paths. For any given pair of nodes, is there a multihop path between them? Or does the graph break down to several “islands” of connected nodes?\nAgain consider the Karate Club data.\n\nu &lt;- matpow(adjK,33)\nsum(u == 0)\n\n[1] 0\n\n\nSo, in that graph representing paths of 33 links, there are no 0s. In this graph, no pair of nodes has 0 paths between them. The graph is connected.\nMaking this kind of analysis fully correct requires paying attention to things such as cycles. The details are beyond the scope of this book.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Matrices and Vectors</span>"
    ]
  },
  {
    "objectID": "Ch1.html#sec-recsys",
    "href": "Ch1.html#sec-recsys",
    "title": "2  Matrices and Vectors",
    "section": "2.10 Recommender Systems",
    "text": "2.10 Recommender Systems\nIf you inquire about some item at an online store, the software will also present you with some related items that it thinks would be of interest to you. How does the software make this guess?\nClearly, the full answer is quite complex. But we can begin to see the process by looking at some real data.\n\nsite &lt;- 'http://files.grouplens.org/datasets/movielens/ml-100k/u.data'\nq &lt;- read.table(site)\nnames(q) &lt;- c('user','movie','rating','userinfo')\nhead(q)\n\n  user movie rating  userinfo\n1  196   242      3 881250949\n2  186   302      3 891717742\n3   22   377      1 878887116\n4  244    51      2 880606923\n5  166   346      1 886397596\n6  298   474      4 884182806\n\n\nWe see for instance that user 22 gave movie 242 a rating of 1. If we want to know some characteristics of this user, his/her ID is 878887116, which we can find in the file u.user at the above URL. Other files tell us more about this movie, e.g. its genre, and so on.\nLet’s explore the data a bit:\nHow many users and movies are in this dataset?\n\nlength(unique(q$user))\n\n[1] 943\n\nlength(unique(q$movie))\n\n[1] 1682\n\n\nHow many other users rated movie number 242?\n\nsum(q$movie == 242)\n\n[1] 117\n\n\nDid user 22 rate movie 234, for instance?\n\nwhich(q$user == 22 & q$movie == 234)\n\ninteger(0)\n\n\nNow we can begin to see a solution to the recommender problem. Say we wish to guess whether user 22 would like movie 234. We could look for other users who have rated many of the same movies as user 22, then focus on the ones who rated movie 234. We could average those ratings to obtain a predicted rating for movie 234 by user 22.We could also incorporate the characteristics of user 22 and the others, which may improve our prediction accuracy, but we will not pursue that here.\nIn order to assess interuser similarity of the nature described above, we might form a matrix \\(S\\), as follows. There would be 943 rows, one for each user, and 1682 columns, one for each movie. The element in row \\(i\\), column \\(j\\) would be the rating user \\(i\\) gave to movie \\(j\\). Most of the matrix would be 0s.\nThe point of constructing \\(S\\) is that determining the similarity of users becomes a matter of measuring similarity of rows of \\(S\\). This paves the way to exploiting the wealth of matrix-centric methodology we will develop in this book.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Matrices and Vectors</span>"
    ]
  },
  {
    "objectID": "Ch1.html#matrix-algebra",
    "href": "Ch1.html#matrix-algebra",
    "title": "2  Matrices and Vectors",
    "section": "2.11 Matrix Algebra",
    "text": "2.11 Matrix Algebra\n\n2.11.1 Other basic operations\nMatrix multiplication may seem odd at first, but other operations are straightforward.\nAddition: We just add corresponding elements. For instance,\n\\[\nA = \\left (\n\\begin{array}{rrr}\n5 & 2 & 6 \\\\\n1 & 2.6 & -1.2 \\\\\n\\end{array}\n\\right )\n\\]\n\\[\nB = \\left (\n\\begin{array}{rrr}\n0 & 20 & 6 \\\\\n3 & 5.8 & 1 \\\\\n\\end{array}\n\\right )\n\\]\n\\[\nA+B = \\left (\n\\begin{array}{rrr}\n5 & 22 & 12 \\\\\n4 & 8.4 & -0.2 \\\\\n\\end{array}\n\\right )\n\\]\nWe do have to make sure the addends match in terms of numbers of rows and columns, 2 and 3 in the example here.\nScalar multiplication: Again, this is simply elementwise. E.g. with A as above,\n\\[\n1.5 A = \\left (\n\\begin{array}{rrr}\n7.5 & 3 & 9 \\\\\n1.5 & 3.9 & -1.8 \\\\\n\\end{array}\n\\right )\n\\]\nDistributive property:\nFor matrices A, B and C of suitable conformability (A and B match in numbers of rows and columns, and their common number of columns matches the number of rows in C), we have\n(A+B) C = AC + BC\n\n\n2.11.2 Matrix transpose\nThis is a very simple but very important operation: We merely exchange rows and columns of the given matrix. For instance, with A as above, its transpose (signified with “’”), is\n\\[\nA' = \\left (\n\\begin{array}{rr}\n5 & 1 \\\\\n2 & 2.6 \\\\\n6 & -1.2 \\\\\n\\end{array}\n\\right )\n\\]\nSome books use the notation \\(A^t\\) or \\(A^T\\) instead of \\(A'\\). The R function for transpose is t().\nIt can be shown that if \\(A\\) and \\(B\\) are conformable, then\n\\[\n(AB)' = B'A'\n\\]\nFor some matrices \\(C\\), we have \\(C' = C\\). \\(C\\) is then termed symmetric.\nWe will often write a row vector in the form (a,b,c,…). So (5,1,88) means the 1x3 matrix with those elements. If we wish this to be a column vector, we use transpose, so that for instance (5,1,88)’ means a 3x1 matrix.\n\n\n2.11.3 Trace of a square matrix\nThe trace of a square matrix \\(A\\) is the sum of its diagonal elements, \\(tr(A) = \\sum_{i=1}^n A_{ii}\\). This measure has various properties, some obvious (trace of the sum is sum of the traces), and some less so, such as:\n\nTheorem 2.1 Suppose \\(A\\) and \\(B\\) are square matrices of the same size. Then\n\\[\ntr(AB) = tr(BA)\n\\tag{2.2}\\]\n\n\nProof. See Your Turn problem below.\n\nAnd furthermore:\n\nTheorem 2.2 Trace is invariant under circular shifts, e.g. \\(UVW\\), \\(VWU\\) and \\(WUV\\) all have the same trace.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Matrices and Vectors</span>"
    ]
  },
  {
    "objectID": "Ch1.html#linear-combinations-of-rows-and-columns-of-a-matrix",
    "href": "Ch1.html#linear-combinations-of-rows-and-columns-of-a-matrix",
    "title": "2  Matrices and Vectors",
    "section": "3.1 Linear Combinations of Rows and Columns of a Matrix",
    "text": "3.1 Linear Combinations of Rows and Columns of a Matrix\nNote that the above expression Equation 3.3,\n\\[\n10 \\left (\n\\begin{array}{r}\n5 \\\\\n1 \\\\\n\\end{array}\n\\right )\n+\n2 \\left (\n\\begin{array}{r}\n2 \\\\\n2.6 \\\\\n\\end{array}\n\\right )\n+\n1 \\left (\n\\begin{array}{r}\n6 \\\\\n-1.2 \\\\\n\\end{array}\n\\right ),\n\\]\nis a sum of scalar products of vectors, which is called a linear combination of those vectors. The quantities 10, 2 and 1 are the coefficients in that linear combination.\nIn view of the fact that that linear combination worked out to be \\(Av\\), we have that:\n\nTheorem 3.1 The product \\(Av\\) of a matrix times a column vector is equal to a linear combination of the columns of the matrix, with coefficients equal to the column vector.\n\nSimilarly,\n\nTheorem 3.2 The product \\(wA\\) of a row vector and a matrix is equal to a linear combination of the rows of the matrix, with the coefficients coming from the row vector.\n\nTo further illustrate all this, write the above matrix Equation 3.1 as\n\\[\nA =\n\\left (\n\\begin{array}{rr}\nA_{11} & A_{21} \\\\\n\\end{array}\n\\right )\n\\tag{3.4}\\]\nwhere\n\\[\nA_{11} =\n\\left (\n\\begin{array}{rr}\n5 & 2 \\\\\n1 & 2.6 \\\\\n\\end{array}\n\\right )\n\\]\nand\n\\[\nA_{12} =\n\\left (\n\\begin{array}{r}\n6 \\\\\n-1.2 \\\\\n\\end{array}\n\\right )\n\\]\nSymbolically, in Equation 3.4, \\(A\\) now looks like a 1x2 “matrix.” Similarly, rewriting Equation 3.2}, we have\n\\[\nv =\n\\left (\n\\begin{array}{r}\nv_{11} \\\\\nv_{21} \\\\\n\\end{array}\n\\right )\n\\]\nwhere\n\\[\nv_{11} =\n\\left (\n\\begin{array}{r}\n10 \\\\\n2 \\\\\n\\end{array}\n\\right )\n\\]\nand \\(v_{21} = 1\\) (a 1x1 matrix), \\(v\\) looks to be 2x1.\nSo, again pretending, treat the product \\(Av\\) as the multiplication of a 1 x 2 “matrix” and a 2 x 1 “vector”, yielding a 1 x 1 result,\n\\[\nA_{11} v_{11} + A_{12} v_{21}\n\\]\nBut all that pretending actually does give the correct answer!\n\\[\nA_{11} v_{11} + A_{12} v_{21} =\n\\left (\n\\begin{array}{rr}\n5 & 2 \\\\\n1 & 2.6 \\\\\n\\end{array}\n\\right )\n\\left (\n\\begin{array}{r}\n10 \\\\\n2 \\\\\n\\end{array}\n\\right )\n+\n\\left (\n\\begin{array}{r}\n6 \\\\\n-1.2 \\\\\n\\end{array}\n\\right )\n1\n=\n\\left (\n\\begin{array}{r}\n60 \\\\\n14 \\\\\n\\end{array}\n\\right )\n\\]\nwhich is the true value of \\(Av\\).\nWe can extend that reasoning further. Say \\(A\\) and \\(B\\) are matrices of sizes \\(m \\times n\\) and \\(n \\times k\\), and consider the product \\(AB\\). Partition \\(B\\) by its columns,\n\\[\nB = (B^{(1)},B^{(2)},..., B^{(k)})\n\\]\nNow pretending that \\(A\\) is a \\(1 \\times 1\\) “matrix” and \\(B\\) is a \\(1 \\times k\\) “matrix”, we have\n\\[\nAB = (AB^{(1)},AB^{(2)},..., AB^{(k)})\n\\]\nIn other words,\n\nTheorem 3.3 In the product \\(AB\\), column \\(j\\) is a linear combination of the columns of \\(A\\), and the coefficients in that linear combination are the elements of column \\(j\\) of \\(B\\).\nA similar result holds for the rows of the product.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Matrices and Vectors</span>"
    ]
  },
  {
    "objectID": "Ch1.html#your-turn",
    "href": "Ch1.html#your-turn",
    "title": "2  Matrices and Vectors",
    "section": "3.2 Your Turn",
    "text": "3.2 Your Turn\n❄️ Your Turn: Fill in the blank with a term from this chapter: The adjacency matrix of an undirected graph is necessarily ________.\n❄️ Your Turn: Consider the Karate Club dataset.\n\nWhich members is member 6 linked with?\nWhich member is linked to the most number of members? How about the least number?\nFind an example of a triad, i.e. a set of 3 members who are all linked to each other?\n\n❄️ Your Turn: Write an R function with call form\noutLinks(adj)\nwhere adj is the adjacency matrix of some network graph, possibly directed. The function will return an R list, whose \\(i^{th}\\) element is a vector of all the nodes that have exactly \\(i\\) outgoing links.\n❄️ Your Turn: Consider the matrix\n\\[\nA =\n\\left (\n\\begin{array}{rr}\n3 & -1 \\\\\n0 & 8  \\\\\n4 & 5  \\\\\n1 & 0  \\\\\n0 & 1  \\\\\n\\end{array}\n\\right )\n\\]\nSay we decide to partition it as\n\\[\nA =\n\\left (\n\\begin{array}{r}\nB \\\\\nI \\\\\n\\end{array}\n\\right )\n\\]\nNow consider the product \\(AA'\\). Using partitioning, we would treat \\(B\\) and \\(I\\) numbers and the product as having factors of size \\(2\n\\times 1\\) and \\(1 \\times 2\\). That would give us a \\(2 \\times 2\\) “matrix”\n\\[\nAA' =\n\\left (\n\\begin{array}{r}\nB \\\\\nI \\\\\n\\end{array}\n\\right )\n(B',I) =\n\\left (\n\\begin{array}{rr}\nBB' & B \\\\\nB' & I  \\\\\n\\end{array}\n\\right )\n\\tag{3.5}\\]\nEvaluate \\(AA'\\) and the far-right side of Equation 3.5 to verify that the partitioning did indeed give us the right answer.\n❄️ Your Turn: The long-run probabilities in Section 2.6 turned out to be uniform, with value 0.20 for all five states. In fact, that is usually not the case. Make a small change to \\(P_1\\) – remember to keep the row sums to 1 – and compute a high power to check whether the long-run distribution seems nonuniform.\n❄️ Your Turn: Not every Markov chain, even ones with finitely many states, have long-run distributions. Some chains have periodic states. It may be, for instance, that after leaving state \\(i\\), once can return only after an even number of hops. Modify our example chain here so that states 1 and 5 (and all the others) have that property. Then compute \\(P^n\\) for various large values of \\(n\\) and observe oscillatory behavior, rather than long-run convergence.\n❄️ Your Turn: Consider the following Markov model of a discrete-time, single-server queue:\n\nModel parameters are \\(p\\) (probability of job completion), \\(q\\) (probability of new job arriving) and \\(m\\) (size of the customer waiting area).\nJobs arrive, are served (possibly after queuing) and leave.\nOnly one job can be in service at a time.\nAt each time epoch:\n\nThe job currently in service, if any, will complete with probability \\(p\\).\nAfter a job completion, a job in the queue, if any, will start service.\nA new job will arrive with probability \\(q\\). If the server is free, this new job will start service, rather than going to the waiting room. If the queue is not full when this job arrives, it will join the queue; otherwise, the job is discarded.\n\nThe system is memoryless in the Markov sense. If a job has been in service for many epochs now, the probability that it finishes in the next epoch is still \\(p\\).\nThe current state is the number of jobs in the system, taking on the values 0,1,2,..,m+1; that last state means m jobs in the queue and 1 in service.\n\nFor instance, say p = 0.4, q = 0.2, m = 5, Suppose the current state is 3, so there is a job in service and two jobs in the queue. Our next state will be 2 with probability (0.4) (0.8); it will be 3 with probability (0.4) (0.2), and so on.\nAnalyze this system for the case given above. Find the approximate long-run distribution, and also the proportion of jobs that get discarded.\n❄️ Your Turn: Prove Equation 2.2. Hint: Write out the left-hand side as a double sum. Reverse the order of summation, and work toward the right-hand side.\n❄️ Your Turn: Write out the details of the “similar result” in the statement of Theorem 3.3.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Matrices and Vectors</span>"
    ]
  },
  {
    "objectID": "Ch2.html",
    "href": "Ch2.html",
    "title": "3  Matrix Inverse",
    "section": "",
    "text": "3.1 A Further Look at Markov Chains\nTo motivate our discussion of matrix inverse, we first revisit the topic of Markov chains.\nSuppose \\(X_0\\), our state at time 0, is random. Let \\(f\\) denote its distribution, i.e. its list of probabilities: \\(f_i = P(X_0 = i)\\), i = 1,…,k, where k is the number of states in the chain. What about \\(X_1\\), the state at time 1? Let’s find an expression for \\(g\\), the distribution of \\(X_1\\).\n\\[\ng_j = P(X_1 = j)\n= \\sum_{i=1}^k P(X_0 = i) P(X_1 = j | X_0 = i)\n= \\sum_{i=1}^k f_i a_{ij}\n\\]\nwhere \\(a_{ij}\\) is the row \\(i\\), column \\(j\\) element of the chain’s transition matrix \\(P\\).\nPutting this is more explicit matrix terms,\n\\[\ng' = (g_1,...,g_k)'\n=\n(f_1 a_{11} + ... + f_k a_{k1},\n...,\nf_1 a_{1k} + ... + f_k a_{kk})\n\\]\nUsing partitioning, we see that that last expression is\n\\[\nf'P\n\\]\nso we have the nice compact relation for the distribution of \\(X_1\\) in terms of the distribution of \\(X_0\\).\n\\[\ng' = f'P\n\\]\nAnd setting \\(h\\) to the distribution of \\(X_2\\), the same reasoning gives us\n\\[\nh' = g'P\n\\]\nLet \\(d_r\\) denote the distribution of \\(X_r\\). Generalizing the above reasoning gives us\n\\[\nd_r' = d_{r-1}' P\n\\]\nFor convenience, let’s take transposes (recalling that \\((AB)' =\nB'A'\\)):\n\\[\nd_r = P' d_{r-1}\n\\tag{3.1}\\]\nNow suppose our chain has a long-run distribution \\(\\nu\\), as in {Section 2.6}, so that\n\\[\n\\lim_{r \\rightarrow \\infty} d_r = \\nu\n\\]\nApplying this to Equation 3.1, we have\n\\[\n\\nu = P' \\nu\n\\tag{3.2}\\]\nSince P is known, this provides us with a way to compute \\(\\nu\\). All we need to do is solve Equation 3.2. Well, how do we do that? It turns out that use of matrix inverses will solve our problem.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Matrix Inverse</span>"
    ]
  },
  {
    "objectID": "Ch2.html#a-further-look-at-markov-chains",
    "href": "Ch2.html#a-further-look-at-markov-chains",
    "title": "3  Matrix Inverse",
    "section": "",
    "text": "Note that we used the Markov property, “memorylessness.” Once we reach time 1, “time starts over,” regradless of the previous history, i.e. regardless of where we were at time 0.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Matrix Inverse</span>"
    ]
  },
  {
    "objectID": "Ch2.html#definition",
    "href": "Ch2.html#definition",
    "title": "3  Matrix Inverse",
    "section": "3.2 Definition",
    "text": "3.2 Definition\n\nFor any square matrix \\(A\\), its inverse \\(B\\) (if it exists) is a square matrix of the same size such that\n\\[\nAB = BA = I\n\\]\nwhere \\(I\\) is the identity matrix of that size.\n\nAs hinted, many matrices do not have inverses. For instance, if \\(A\\) consists of all 0s, there is no way to get \\(I\\) for \\(AB\\).\nIn very rough terms, it sometimes helps the intuition to think of an inverse as the “reciprocal” of the matrix.\nWe will often speak of the inverse of \\(A\\). In fact, if \\(A\\) is invertible, its inverse is unique.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Matrix Inverse</span>"
    ]
  },
  {
    "objectID": "Ch2.html#sec-markovsolve",
    "href": "Ch2.html#sec-markovsolve",
    "title": "3  Matrix Inverse",
    "section": "3.3 Example: Computing Long-Run Markov Distribution",
    "text": "3.3 Example: Computing Long-Run Markov Distribution\nNow let us return to Equation 2.1, which expresses the vector of long-run state probabilities for a Markov chain with transition matrix P and stationary distribution \\(\\nu\\), How can we use matrix inverses to solve this equation?\n\\[\n\\nu = P' \\nu\n\\tag{3.3}\\]\nRewrite it using the identity matrix:\n\\[\n(I - P') \\nu = 0\n\\]\nFor the random walk chain in Chapter 1, we hadIn that particular model, P’ = P, but for most chains this is not the case.\n\\[\nP =\n\\left (\n\\begin{array}{rrrrr}\n0.5 & 0.5 & 0 & 0 & 0\\\\\n0.5 & 0 & 0.5 & 0 & 0\\\\\n0 & 0.5 & 0 & 0.5 & 0\\\\\n0 & 0 & 0.5 & 0 & 0.5\\\\\n0 & 0 & 0 & 0.5 & 0.5 \\\\\n\\end{array}\n\\right )\n\\tag{3.4}\\]\nWith \\(\\nu = (\\nu_1,\\nu_2,\\nu_3,\\nu_4,\\nu_5)'\\), the equation to be solved, \\((I-P') \\nu = \\nu\\), is\n\\[\n\\left (\n\\begin{array}{rrrrr}\n0.5 & -0.5 & 0 & 0 & 0 \\\\\n-0.5 & 1 & -0.5 & 0 & 0 \\\\\n0 & -0.5 & 1 & -0.5 & 0 \\\\\n0 & 0 & -0.5 & 1 & -0.5 \\\\\n0 & 0 & 0 & -0.5 & 0.5 \\\\\n\\end{array}\n\\right )\n\\left (\n\\begin{array}{r}\n\\nu_1 \\\\\n\\nu_2 \\\\\n\\nu_3 \\\\\n\\nu_4 \\\\\n\\nu_5 \\\\\n\\end{array}\n\\right ) =\n\\left (\n\\begin{array}{r}\n0 \\\\\n0 \\\\\n0 \\\\\n0 \\\\\n0 \\\\\n\\end{array}\n\\right )\n\\]\nIf we perform the matrix multiplication, we have an ordinary system of linear equations:\n\\[\n\\begin{array}{r}\n0.5 \\nu_1 - 0.5 \\nu_2 = 0 \\\\\n-0.5 \\nu_1 + \\nu_2 - 0.5 \\nu_3 = 0 \\\\\n-0.5 \\nu_2 + \\nu_3 - 0.5 \\nu_4 = 0 \\\\\n-0.5 \\nu_3 + \\nu_4 - 0.5 \\nu_5 = 0 \\\\\n-0.5 \\nu_4 + 0.5 \\nu_5 = 0 \\\\\n\\end{array}\n\\tag{3.5}\\]\nThis is high school math, and we could solve the equations that way. But this is literally what linear algebra was invented for, solving systems of equations! We will use matrix inverse.\nBut first, we have a problem to solve: The only solution to the above system is with all \\(\\nu_i = 0\\). We need an equation involving a nonzero quantity.\nBut we do have such an equation. The vector \\(\\nu\\) is a stationary distribution for a Markov chain, i.e. the set of long-run probabilities, and thus it must sum to 1.0. Let’s replace the last row by that relation:\n\\[\n\\left (\n\\begin{array}{rrrrr}\n0.5 & -0.5 & 0 & 0 & 0 \\\\\n-0.5 & 1 & -0.5 & 0 & 0 \\\\\n0 & -0.5 & 1 & -0.5 & 0 \\\\\n0 & 0 & -0.5 & 1 & -0.5 \\\\\n1 & 1 & 1 & 1 & 1 \\\\\n\\end{array}\n\\right )\n\\left (\n\\begin{array}{r}\n\\nu_1 \\\\\n\\nu_2 \\\\\n\\nu_3 \\\\\n\\nu_4 \\\\\n\\nu_5 \\\\\n\\end{array}\n\\right ) =\n\\left (\n\\begin{array}{r}\n0 \\\\\n0 \\\\\n0 \\\\\n0 \\\\\n1 \\\\\n\\end{array}\n\\right )\n\\tag{3.6}\\]\nMore compactly,\n\\[\nG \\nu = q\n\\]\nIf our matrix \\(G\\) is invertible,  we can premultiply both sides of our equation above, yielding\\(I-P'\\) might not be invertible, as there may not be a long-run distribution.\n\\[\nG^{-1} q = G^{-1} G \\nu = \\nu\n\\]\nSo, we have obtained our solution for the stationary distribution \\(\\nu\\),\n\\[\n\\nu = G^{-1} q\n\\]\nWe can evaluate it numerically via the R solve function, which finds matrix inverse:\n\nG &lt;-\nrbind(c(0.5,-0.5,0,0,0), c(-0.5,1,-0.5,0,0), c(0,-0.5,1,-0.5,0),\n   c(0,0,-0.5,1,-0.5), c(1,1,1,1,1))\nG\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]  0.5 -0.5  0.0  0.0  0.0\n[2,] -0.5  1.0 -0.5  0.0  0.0\n[3,]  0.0 -0.5  1.0 -0.5  0.0\n[4,]  0.0  0.0 -0.5  1.0 -0.5\n[5,]  1.0  1.0  1.0  1.0  1.0\n\nGinv &lt;- solve(G)\n# check the inverse\nGinv %*% G  # yes, get I (of course with some roundoff error)\n\n              [,1]         [,2]          [,3]          [,4]          [,5]\n[1,]  1.000000e+00 1.942890e-16 -1.387779e-16  1.942890e-16  8.326673e-17\n[2,]  1.942890e-16 1.000000e+00  3.053113e-16 -2.775558e-17  8.326673e-17\n[3,] -2.775558e-17 0.000000e+00  1.000000e+00  0.000000e+00  2.775558e-17\n[4,] -1.665335e-16 4.996004e-16 -3.608225e-16  1.000000e+00 -2.775558e-17\n[5,] -1.665335e-16 7.216450e-16 -4.996004e-16  5.551115e-17  1.000000e+00\n\nnu &lt;- Ginv %*% c(0,0,0,0,1)  # recall that q = c(0,0,0,0,1)\nnu\n\n     [,1]\n[1,]  0.2\n[2,]  0.2\n[3,]  0.2\n[4,]  0.2\n[5,]  0.2\n\n\nThis confirms our earlier speculation in Section 2.6 based on powers of \\(P\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Matrix Inverse</span>"
    ]
  },
  {
    "objectID": "Ch2.html#sec-matalg",
    "href": "Ch2.html#sec-matalg",
    "title": "3  Matrix Inverse",
    "section": "3.4 Matrix Algebra",
    "text": "3.4 Matrix Algebra\nSeveral properties to note:\n\nIf the inverses of \\(A\\) and \\(B\\) exist, and \\(A\\) and \\(B\\) are conformable, then \\((AB)^{-1}\\) exists and is equal to \\(B^{-1} A^{-1}\\).\nProof: Consider the product \\((AB) (B^{-1} A^{-1})\\). The \\(B\\) factors give us \\(I\\), leaving \\(A A^{-1}\\), which too is \\(I\\).\n\\((A')^{-1}\\) exists and is equal to \\((A^{-1})'\\).\nProof: Follows immediately from \\(A A^{-1} = I\\) and the fact that \\((UV)' = V'U'\\).\nIf \\(A\\) is invertible and symmetric, then \\((A^{-1})'\\) is also symmetric.\nProof: For convenience, let \\(B\\) denote \\(A^{-1}\\). Then\n\\[\nAB = I\n\\]\nAgain using the fact that the transpose of a product is the reverse product of the transposes, we have\n\\[\nI = I' = (AB)' = B'A'\n\\]\nBut since \\(A' = A\\), we have\n\\[\nI = B' A\n\\]\nIn other words, not only is \\(B\\) the inverse of \\(A\\), \\(B'\\) is too! So, \\(B = B'\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Matrix Inverse</span>"
    ]
  },
  {
    "objectID": "Ch2.html#computation-of-the-matrix-inverse",
    "href": "Ch2.html#computation-of-the-matrix-inverse",
    "title": "3  Matrix Inverse",
    "section": "3.5 Computation of the Matrix Inverse",
    "text": "3.5 Computation of the Matrix Inverse\nFinding the inverse of a large matrix – in data science applications, the number of rows and columns \\(n\\) can easily be hundreds or more – can be computationally challenging. The run time is proportional to \\(n^3\\), and roundoff error can be an issue. Sophisticated algorithms have been developed, such as the QR decompositions. So in R, we should use, say, qr.solve rather than solve if we are working with sizable matrices, or even use methods that do not directly compute the inverse.\nThe classic “pencil and paper” method for matrix inversion is instructive, and will be presented here.\n\n3.5.1 Pencil-and-paper computation\nNote: Some readers will notice some similarity here with elementary methods they learned in high school, but actually the treatment here is much more sophisticated. It will play an important practical and theoretical role in Chapter 6.\nThe basic idea follows the pattern the reader learned for solving systems of linear equations, but with the added twist of involving some matrix multiplication.\nLet’s take as our example\n\\[\nA =\n\\left (\n\\begin{array}{rr}\n4 & 7 \\\\\n-8 & 15  \\\\\n\\end{array}\n\\right )\n\\]\nWe aim to transform this to the 2x2 identity matrix, via a sequence of row operations.\n\n\n3.5.2 Use of elementary matrices\nLet’s multiply row 1 by 1/4, to put 1 in the first element:\n\\[\n\\left (\n\\begin{array}{rr}\n1 & \\frac{7}{4} \\\\\n-8 & 15  \\\\\n\\end{array}\n\\right )\n\\]\nIn matrix terms, that operation is equivalent to premultiplying \\(A\\) by\n\\[\nE_1=\n\\left (\n\\begin{array}{rr}\n\\frac{1}{4} & 0  \\\\\n0 & 1  \\\\\n\\end{array}\n\\right )\n\\]\nWe then add 8 times row 1 to row 2, yielding\n\\[\n\\left (\n\\begin{array}{rr}\n1 & \\frac{7}{4} \\\\\n0 & 29  \\\\\n\\end{array}\n\\right )\n\\]\nThe premultiplier for this operation is\n\\[\nE_2=\n\\left (\n\\begin{array}{rr}\n1 & 0  \\\\\n8 & 1  \\\\\n\\end{array}\n\\right )\n\\]\nMultiply row 2 by 1/29:\n\\[\n\\left (\n\\begin{array}{rr}\n1 & \\frac{7}{4} \\\\\n0 & 1  \\\\\n\\end{array}\n\\right )\n\\]\ncorresponding to\n\\[\nE_3=\n\\left (\n\\begin{array}{rr}\n1 & 0  \\\\\n8 & 1/29  \\\\\n\\end{array}\n\\right )\n\\]\nAnd finally, add -7/4 row 2 to row 1.\n\\[\n\\left (\n\\begin{array}{rr}\n1 & 0 \\\\\n0 & 1  \\\\\n\\end{array}\n\\right )\n\\]\nThe premultiplier is\n\\[\nE_4 =\n\\left (\n\\begin{array}{rr}\n1 & \\frac{7}{4} \\\\\n0 & 1  \\\\\n\\end{array}\n\\right )\n\\]\nNow, how does that give use \\(A^{-1}\\)? The method your were taught probably set up the partioned matrix \\((A,I)\\). The row operations that transformed \\(A\\) to \\(I\\) also transformed \\(I\\) to \\(A^{-1}\\). Here’s why:\nAs noted, the row operations are such that\n\\[\nE_4 E_3 E_2 E_1 A\n\\]\ngive us the final transformed result, i.e. the matrix \\(I\\):\n\\[\n(E_4 E_3 E_2 E_1) A = I\n\\]\nAha! We have found the inverse of \\(A\\) – it’s \\(E_4 E_3 E_2 E_1\\).\nNote too thatRecall that the inverse of a product (if it exists is the reverse product of the inverses.\n\\[\nA = (E_4 E_3 E_2 E_1)^{-1} I = E_4^{-1} E_3^{-1} E_2^{-1} E_1^{-1}\n\\]\nSo apparently the inverses of the elementary matrices \\(E_i\\) also exist, and in fact we can obtain them easily:\nEach \\(E_i^{-1}\\) simply “undoes” its partner. \\(E_1\\), for instance, multiplies the row 1, column 1 element by 1/4, which is “undone” by multiplying that element by 4,\n\\[\nE_1^{-1} =\n\\left (\n\\begin{array}{rr}\n4 & 0 \\\\\n0 & 1  \\\\\n\\end{array}\n\\right )\n\\]\nAlso, to undo the operation of adding 8 times row 1 to row 2, we add -8 times row 1 to row 2:\n\\[\nE_2^{-1} =\n\\left (\n\\begin{array}{rr}\n1 & 0 \\\\\n-8 & 1 \\\\\n\\end{array}\n\\right )\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Matrix Inverse</span>"
    ]
  },
  {
    "objectID": "Ch2.html#sec-noinverse",
    "href": "Ch2.html#sec-noinverse",
    "title": "3  Matrix Inverse",
    "section": "3.6 Nonexistent Inverse",
    "text": "3.6 Nonexistent Inverse\nSuppose our matrix \\(A\\) had been slightly different:\n\\[\nA =\n\\left (\n\\begin{array}{rr}\n4 & 7 \\\\\n-8 & -14  \\\\\n\\end{array}\n\\right )\n\\]\nThis would have led to\n\\[\n\\left (\n\\begin{array}{rr}\n1 & \\frac{7}{4} \\\\\n0 & 0  \\\\\n\\end{array}\n\\right )\n\\]\nThis cannot lead to \\(I\\), indicating that \\(A^{-1}\\) does not exist, and the matrix is said to be singular. And it’s no coincidence that row 2 of \\(A\\) is double row 1. This has many implications, as will be seen in our chapter on vector spaces.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Matrix Inverse</span>"
    ]
  },
  {
    "objectID": "Ch2.html#determinants",
    "href": "Ch2.html#determinants",
    "title": "3  Matrix Inverse",
    "section": "3.7 Determinants",
    "text": "3.7 Determinants\nThis is a topic that is quite straightforward and traditional, even old-fashioned – in fact, too old-fashioned, according to mathematician Sheldon Axler. The theme of his book, Linear Algebra Done Right, is that determinants are overemphasized. He relegates the topic to the very end of the book. Yet determinants do appear often in applied linear algebra settings. Moreover, they will be convenient to use in explaining concepts in this book on linear algebra in Data Science.\nBut why place the topic in this particular chapter? The answer lies in the fact that earlier in this chapter we had the proviso “If \\((A'A)^{-1}\\) exists.” The following property of determinants is then relevant:\n\nA square matrix \\(G\\) is invertible if and only if \\(det(G) \\neq 0\\).\n\nThere are better ways to ascertain invertibility than this, but it is conceptually helpful. Determinants play a similar role in the topic of eigenvectors in Chapter 11.\n\n3.7.1 Definition\nThe standard definition is one of the ugliest in all of mathematics. Instead we will define the term using one of the methods for calculating determinants.\n\nConsider an \\(r \\textrm{ x } r\\) matrix \\(G\\). For \\(r = 2\\), write \\(G\\) as\n\\[\nG =\n\\left (\n\\begin{array}{rr}\na & b \\\\\nc & d  \\\\\n\\end{array}\n\\right )\n\\]\nand define \\(\\det(G)\\) to be \\(ad -bc\\). For \\(r &gt; 2\\), define submatrices as follows.\n\\(G_j\\) is the \\((r-1) \\textrm{ x } (r-1)\\) submatrix obtained by removing row 1 and column \\(j\\) from \\(G\\). Then \\(\\det(G)\\) is defined recursively as\n\\[\n\\sum_{i=1}^r (-1)^{i+1} \\det(G_i)\n\\]\n\nActually, we can alternatively remove row \\(i\\) instead of row 1. If \\(i\\) is an odd number, the same recursive formula holds, but for even \\(i\\), replace \\((-1)^{i+1}\\) by \\((-1)^i\\).\nFor instance, consider\n\\[\nM =\n\\left (\n\\begin{array}{rrr}\n5 & 1 & 0 \\\\\n3 & -1 & 7 \\\\\n  0 & 1 & 1 \\\\\n  \\end{array}\n\\right )\n\\]\nThe \\(\\det(M) = 5(-1 - 7) - 1(3 - 0) + 0 = -43\\).\nA glimpse at the classical definition:\nUsing the same approach as in the last computation, we would find that the determinant of a general \\(3 \\textrm{ x } 3\\) matrix\n\\[\n\\left (\n\\begin{array}{rrr}\na & b & c \\\\\nd & e & f \\\\\ng & h & i \\\\\n  \\end{array}\n\\right )\n\\]\nis\n\\[\naei + bfg + cdh - afh - bdi - ceg\n\\]\nEach term here is involves a product of 3 of the elements of the matrix. In general, the determinant involves sums and differences of permuted products of distinct elements of the matrix, as we see above. The formation of the terms in general, and the determination of + and - signs, is done in complex but precise manner that we will not present here. But the reader should at least keep in mind that each term is a product of \\(n\\) elements of the matrix, a fact that will be relevant in the sequel.\n\n\n3.7.2 Properties\nWe state these without proof:\n\n\\(G^{-1}\\) exists if and only if \\(\\det(G) \\neq 0\\)\n\\(\\det(GH) = \\det(G) \\det(H)\\)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Matrix Inverse</span>"
    ]
  },
  {
    "objectID": "Ch2.html#your-turn",
    "href": "Ch2.html#your-turn",
    "title": "3  Matrix Inverse",
    "section": "3.8 Your Turn",
    "text": "3.8 Your Turn\n❄️ Your Turn: In Section 3.5.2, find the inverses of \\(E_3\\) and \\(E_4\\) using similar reasoning, and thus find \\(A^{-1}\\).\n❄️ Your Turn: In Equation 3.7, consider the variant\n\\[\nP =\n\\left (\n\\begin{array}{rrrrr}\n0.5 & 0.5 & 0 & 0 & 0\\\\\n0.5 & 0 & 0.5 & 0 & 0\\\\\n0 & 0.5 & 0 & 0.5 & 0\\\\\n0 & 0 & 0.5 & 0 & 0.5\\\\\n0 & 0 & 0 & 1.0 & 0.0 \\\\\n\\end{array}\n\\right )\n\\tag{3.7}\\]\nHere, the walker at state 5 immediately “bounces back” to state 4, rather than remaining at state 5 for one or more epochs.\nIn the original chain, we found that \\(\\nu = (0.2,0.2,0.2,0.2,0.2)'\\). Speculate as to the effect on \\(\\nu_5\\) of the above change in model. Then investigate to determine if our speculation was correct.\n❄️ Your Turn: Consider a Markov chain with transition probability matrix\n\\[\nP =\n\\left (\n\\begin{array}{rr}\n0 & 1 \\\\\n1 & 0  \\\\\n\\end{array}\n\\right )\n\\]\nThis chain is termed periodic, with period 2. It alternates between states 1 and 2, and thus a long-run distribution \\(\\nu\\) does not exist. That would suggest the \\(I-P'\\) noninvertible. Confrim this.\n❄️ Your Turn: If you are familiar with recursive calls, write a function \\(\\verb+dt(a)+\\) to compute the determinant of a square matrix \\(A\\).\n❄️ Your Turn: Prove the assertions in Section 3.4. Note that for the identity matrix \\(I\\), \\(I' = I\\).\n❄️ Your Turn: The determinant of a 3 x 3 matrix\n\\[\nM =\n\\left (\n\\begin{array}{rrr}\na & b & d \\\\\nd & e & f \\\\\ng & h & i \\\\\n\\end{array}\n\\right )\n\\]\nis\n\\[\naei+bfg+cdh-ceg-bdi-afh\n\\]\nSuppose the elements of \\(M\\) are independent random variables with uniform distributions on (0,1). Argue that P(M is invertible) = 1.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Matrix Inverse</span>"
    ]
  },
  {
    "objectID": "Ch2a.html",
    "href": "Ch2a.html",
    "title": "4  Covariance Matrices, MV Normal Distribution, Delta Method",
    "section": "",
    "text": "4.1 Random Vectors\nYou are probably familiar with the concept of a random variable, but of even greater importance is random vectors.\nSay we are jointly modeling height, weight, age, systolic blood pressure and cholesterol, and are especially interested in relations between these quantities. We then have the random vector\n\\[\nX\n=\n\\left (\n\\begin{array}{r}\nX_1 \\\\\nX_2 \\\\\nX_3 \\\\\nX_4 \\\\\nX_5 \\\\\n  \\end{array}\n\\right )\n=\n\\left (\n\\begin{array}{r}\n\\textrm{height} \\\\\n\\textrm{weight} \\\\\n\\textrm{age} \\\\\n\\textrm{bp} \\\\\n\\textrm{chol} \\\\\n  \\end{array}\n\\right )\n\\]",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Covariance Matrices, MV Normal Distribution, Delta Method</span>"
    ]
  },
  {
    "objectID": "Ch2a.html#random-vectors",
    "href": "Ch2a.html#random-vectors",
    "title": "4  Covariance Matrices, MV Normal Distribution, Delta Method",
    "section": "",
    "text": "4.1.1 Sample vs. population\nWe may observe \\(n\\) realizations of \\(X\\) in the form of sample data, say on \\(n = 100\\) people. In the statistics world, we treat this data as a random sample from some population, say all Americans. Usually, we are just given the data rather then having actual random sampling, but this view recognizes that there are a lot more people out there than our data.\nWe speak of estimating population quantities. For instance, we can estimate the population value E(X1), i.e. mean of \\(X_1\\) throughout the population, by the sample analog,\n\\[\n\\frac{1}{n} \\sum_{i=1}^n X_{1j}\n\\]\nwhere \\(X_{ij}\\) denotes the value of \\(X_i\\) for the \\(j^{th}\\) person in our sample.\nBy contrast, this view is rarely taken in the machine learning community. The data is the data, and the fact that it is a small subset of a much larger group is irrelevant. They will often allude to the randomness of the data by mentioning the “data generating mechanism.”",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Covariance Matrices, MV Normal Distribution, Delta Method</span>"
    ]
  },
  {
    "objectID": "Ch2a.html#sec-cov",
    "href": "Ch2a.html#sec-cov",
    "title": "4  Covariance Matrices, MV Normal Distribution, Delta Method",
    "section": "4.2 Covariance",
    "text": "4.2 Covariance\n\n4.2.1 Scalar covariance\nRecall first the notion in statistics of covariance: Given a pair of random variables \\(U\\) and \\(V\\), their covariance is defined by\n\\[\nCov(U,V) = E[(U - EU)(V - EV)]\n\\]\nLoosely speaking, this measures the degree to which the two random variables vary together. Consider for instance human height \\(H\\) and \\(W\\). Taller people tend to also be heavier. Say we sample many people from a population. Most of those who are taller than average, i.e. \\(H &gt; EH\\), will also be heavier than average, \\(W &gt; EW\\), making \\((H - EH)(W - EW) &gt;\n0\\). Similarly, shorter people tend to be lighter, i.e. we often have \\(H\n&lt; EH\\) and \\(W &lt; EW\\), but then we still have \\((H - EH)(W - EW) &gt; 0\\). So, one way or the other, usually \\((H - EH)(W - EW) &gt; 0\\), and though there will be a number of exceptions, they will be rare enough so that\n\\(E[(H - EH)(W - EW)] &gt; 0\\).Of course, the magnitude of \\((H - EH)(W - EW)\\) plays a role too.\nIn other words, \\(Cov(H,W) &gt; 0\\). Similarly if \\(U\\) is often large when \\(V\\) is small, and vice versa, we will likely have \\(Cov(H,W) &lt; 0\\).\nIf this sounds like correlation to you, then your hunch is correct. Covariance will indeed later lead to the concept of correlation, but that intuition will serve us now.\nNote some properties of scalar covariance.\n\nSymmetry:\n\n\\[\nCov(U,V) = Cov(V,U)\n\\]\n\n\\(Cov\\) is bilinear:\n\n\\[\nCov(aU,bV) = ab Cov(U,V)\n\\]\n\nVariance as a special case:\n\n\\[\nCov(U,U) = Var(U)\n\\]\n\nCross-product term:\n\n\\[\nVar(U+V) = Var(U) + Var(V) + 2 Cov(U,V)\n\\]\n\n“Short cut” formula:\n\n\\[\nCov(U,V) = E(UV) - (EU) (EV)\n\\tag{4.1}\\]\n\n\n4.2.2 Covariance matrices\nThe relations between the various components of \\(X\\) are often characterized by the covariance matrix of \\(X\\), whose entries consist of scalar covariances between pairs of components of a random vector.  It is defined as follows for a \\(k\\)-component random vector \\(X\\) . The covariance matrix, denoted by \\(Cov(X)\\), is a \\(k \\times k\\) matrix, and for \\(1 \\leq i,j \\leq k\\), its row \\(i\\), column \\(j\\) element isThe notation is somewhat overloaded. “Cov” refers both to the covariance between two random variables, say height and weight, and to the covariance matrix of a random vector. But it will always be clear from context which one is being discussed.\n\\[\nCov(X)_{ij} = Cov(X_i,X_j)\n\\]\nAs an example, here is data on major league baseball players:\n\nlibrary(qeML) \ndata(mlb1) \nhead(mlb1) \n\n        Position Height Weight   Age\n1        Catcher     74    180 22.99\n2        Catcher     74    215 34.69\n3        Catcher     72    210 30.78\n4  First_Baseman     72    210 35.43\n5  First_Baseman     73    188 35.71\n6 Second_Baseman     69    176 29.39\n\nhwa &lt;- mlb1[,-1] \ncov(hwa) \n\n           Height    Weight        Age\nHeight  5.3542814  25.61130 -0.8239233\nWeight 25.6113038 433.60211 12.9110576\nAge    -0.8239233  12.91106 18.6145019\n\ncor(hwa)\n\n            Height    Weight         Age\nHeight  1.00000000 0.5315393 -0.08252974\nWeight  0.53153932 1.0000000  0.14371113\nAge    -0.08252974 0.1437111  1.00000000\n\n\nAgain, we’ll be discussing more of this later, but what about that negative correlation between height and age? It’s near 0, and this could be a sampling artifact, but another possibility is that in this sport, shorter players do not survive as well.\nProperties of the matrix version of covariance:\n\nMatrix form of definition:\n\\[ Cov(X) = E[(X - X) (X - EX)']\n\\tag{4.2}\\]\n(Note the dimensions: \\(X\\) is a column vector, say \\(k \\times 1\\), so \\((X - X) (X - EX)'\\) is \\(k \\times k\\). The expected value is then of that size as well.)\nFor statistically independent random vectors \\(Q\\) and \\(W\\) of the same length,\n\n\\[\nCov(Q+W) = Cov(Q) + Cov(W)\n\\tag{4.3}\\]\n\nFor any nonrandom scalar \\(c\\), and \\(Q\\) a random vector, we have\n\n\\[\nCov(cQ) = c^2 Cov(Q)\n\\]\n\nSay we have a random vector \\(X\\), of length \\(k\\), and a nonrandom matrix \\(A\\) of size \\(m \\times k\\). Then \\(A X\\) is a new random vector \\(Y\\) of \\(m\\) components. It turns out that\n\\[\nCov(Y) = A Cov(X) A'\n\\tag{4.4}\\]\nThe proof is straightforward but tedious, and will be omittted.\n\\(Cov(X)\\) is a symmetric matrix. This follows from the symmetry of the definition.\nThe diagonal elements of \\(Cov(X)\\) are the variances of the random variables \\(X_i\\). This follows from \\(Cov(U,U) = Var(U)\\) for scalar \\(U\\).\nIf \\(X\\) is a vector of length 1, i.e. a number, then\n\n\\[\nCov(X) = Var(X)\n\\]\n\nFor any length-\\(k\\) column vector \\(a\\),\n\n\\[\nVar(a'X) = a' ~ Cov(X) ~ a\n\\tag{4.5}\\]\n\nThus \\(Cov(X)\\) is nonnegative definite, meaning that for any length-\\(k\\) column vector \\(a\\)\n\n\\[\na' Cov(X) a \\geq 0\n\\]\n\nFor any constant (i.e. nonrandom) \\(m \\times k\\) matrix \\(A\\),\n\n\\[\nCov(AX) = A Cov(X) A'\n\\tag{4.6}\\]\n\n\n4.2.3 Cross-covariance\nThe matrix \\(Cov(X)\\) represents the covariance of a vector \\(X\\) with itself. But we can also speak of the covariance of one vector \\(X\\) with another vector \\(Y\\), termed the cross-covariance between them. Its definition is a natural extension of covariance matrices:\n\\[\nCov(X,Y) = E[(X - EX) (Y- EY)]\n\\]\nSay \\(X\\) and \\(Y\\) are of lengths \\(m\\) and \\(n\\). Then \\(Cov(X,Y)\\) will be of size \\(m \\times n\\), with\n\\[\nCov(X,Y)_{ij} = Cov(X_i,Y_j)\n\\]\nwhere the latter covariance is scalar.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Covariance Matrices, MV Normal Distribution, Delta Method</span>"
    ]
  },
  {
    "objectID": "Ch2a.html#sec-mvn",
    "href": "Ch2a.html#sec-mvn",
    "title": "4  Covariance Matrices, MV Normal Distribution, Delta Method",
    "section": "4.3 The Multivariate Normal Distribution Family",
    "text": "4.3 The Multivariate Normal Distribution Family\nThe familiar “bell-shaped curve” refers to the normal (or Gaussian) family, whose densities have the form\n\\[\n\\frac{1}{\\sigma \\sqrt{2 \\pi}}\ne^{-0.5 (\\frac{t-\\mu}{\\sigma})^2}\n\\]\nThe values of \\(\\mu\\) and \\(\\sigma\\) are the mean and standard deviation. But what if we have a random vector, say of length \\(k\\)? Is there a generalized normal family?\n\n4.3.1 Example: k = 2\nThe answer is yes. Here is an example for \\(k = 2\\):\n\n\n\n3D bell density\n\n\n\n\n4.3.2 General form\nWell, then, what is the form of the \\(k\\)-dimensional density function? Just as the univariate normal family is parameterized by mean and variance, the multivariate one is parameterized via mean vector \\(\\mu\\) and covariance matrix \\(\\Sigma\\). The form is\n\\[\n(2\\pi)^{-k/2} \\det(\\Sigma)^{-k/2}\ne^{-0.5 (t-\\mu)'(\\Sigma)^{-1}(t-\\mu)}\n\\tag{4.7}\\]\nNote the intuition:\n\nInstead of \\(1/\\sigma^2\\), i.e. instead of dividing by variance, we “divide by \\(\\Sigma\\),” intuitively viewing matrix inverse as a “reciprocal” of a matrix.\nIn other words, covariance matrices operate roughly like generalized variances.\nInstead of squaring the scalar \\(t - \\mu\\), we “square” it in the vector case by peforming a \\(w'w\\) operation, albeit with \\(\\Sigma^{-1}\\) in the middle.\n\nClearly, we should not stretch these analogies very far, but they do help our intuition here.\n\n\n4.3.3 Properties\n\nTheorem 4.1 If a random vector is MV normally distributed, then the conditional distribution of any one of its components \\(Y\\), given the others \\(X_{others} = t\\) (note that \\(t\\) is a vector if \\(k &gt; 2\\)) has the following properties:\n\nIt has a (univariate) normal distribution.\nIts mean \\(E(Y | X_{others}) = t\\) is linear in \\(t\\).\nIts variance \\(Var(Y | X_{others} = t)\\) does not involve \\(t\\).\n\nThese of course are the classical assumptions of linear regression models. They actually come from the MV normal model.\nSpecifics: Denote the mean vector and covariance matrix a random vector \\(X\\) by \\(\\mu\\) and \\(\\Sigma\\). Partition \\(X\\) as\n\\[\n\\left (\n\\begin{array}{r}\nX_1 \\\\\nX_2 \\\\\n\\end{array}\n\\right )\n\\]\nand partition \\(\\mu\\) and \\(\\Sigma\\) similarly. The conditional distribution of \\(X_1\\) given \\(X_2 = t\\) is multivariate normal with these paramters:\n\\[\nE(X_1 | X_2 = t) =\n\\mu_1 + \\Sigma_{12} \\Sigma_{22}^{-1} (t - \\mu_2)\n\\tag{4.8}\\]\n\\[\nCov(X_1 | X_2 = t) = \\Sigma_{11} - \\Sigma_{12} \\Sigma_{22}^{-1} \\Sigma_{21}\n\\tag{4.9}\\]\n\nNote the absence of \\(t\\) in Equation 4.9. Note too that the uncoditional covariance matrix, \\(\\Sigma_{11}\\) becomes “smaller” when we know something about \\(X_2\\).\n\nProof. This comes out of writing down the conditional density (overall density divided by marginal), and then doing some algebra.\n\\(\\square\\)\n\n\nTheorem 4.2 If \\(X\\) is a multivariate-normal random vector, then so is \\(AX\\) for any conformable nonrandom matrix \\(A\\).\n\n\nProof. Again, perform direct evaluation of the density.\n\\(\\square\\)\n\n\nTheorem 4.3 (Cramer-Wold Theorem) A random vector \\(X\\) has a multivariate normal distribution if and only if \\(w'X\\) has a univariate normal distribution for all conformable nonrandom vectors \\(w\\).\n\n\nProof. “Only if” follows from above. “If” part too complex to present here.\n% NM \\(\\square\\)\n\n\nTheorem 4.4 (The Multivariate Central Limit Theorem) Let \\(X_1, X_2,...\\) be a sequence on statistically independent random vectors, with common distribution multivariate normal with mean vector \\(\\mu\\) and covariance matrix \\(\\Sigma\\). Write\n\\[\n\\bar{X} = \\frac{X_1+...+X_n}{n}\n\\]\nThen the distribution of the random vector\n\\[\nW_n = \\sqrt{n} (\\bar{X} - \\mu)\n\\]\ngoes to multivariate normal with the 0 vector as mean and covariance matrix \\(\\Sigma\\).The usual form would involve the “square root” of a matrix, but we will not discuss that concept until our chapter on inner product spaces.\n\n\nProof. Theorem 4.3 reduces the problem to the univariate case, where we know the Central Limit Theorem holds.\n\\(\\square\\)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Covariance Matrices, MV Normal Distribution, Delta Method</span>"
    ]
  },
  {
    "objectID": "Ch2a.html#multinomial-random-vectors-have-approximate-multivariate-normal-distributions",
    "href": "Ch2a.html#multinomial-random-vectors-have-approximate-multivariate-normal-distributions",
    "title": "4  Covariance Matrices, MV Normal Distribution, Delta Method",
    "section": "4.4 Multinomial Random Vectors Have Approximate Multivariate Normal Distributions",
    "text": "4.4 Multinomial Random Vectors Have Approximate Multivariate Normal Distributions\nRecall that a multinomial random vector is the mathematial analog of an R factor variable – a categorical variable with \\(k\\) levels/categories. Just as a binomial random variable represents the number of “successes” in \\(n\\) “trials,” a multinomial random vector represents the numbers of successes in each of the \\(k\\) categories.\nLet’s write such a random vector as\n\\[\nX =\n\\left (\n\\begin{array}{r}\nN_1 \\\\\n... \\\\\nN_k \\\\\n\\end{array}\n\\right )\n\\]\nLet \\(p_i\\) be the probability of a trial having outcome \\(i-1,...,k\\). Note the following:\n\n\\(N_1+...+N_k = n\\)\nThe marginal distribution of \\(N_i\\) is binomial, with success probability \\(p_i\\) and \\(n\\) trials.\n\nSo for instance if we roll a fair die 10 times, then \\(N_i\\) is the number of trials in which the roll’s outcome was \\(i\\) dots and \\(p_i = 1/6\\) , \\(i=1,2,3,4,5,6\\).\nLet’s find \\(Cov(X)\\). Define the indicator vector\n\\[\nI_i =\n\\left (\n\\begin{array}{r}\nI_{i1} \\\\\n... \\\\\nI_{ik}  \\\\\n\\end{array}\n\\right )\n\\]\nHere \\(I_{ij}\\) is 1 or 0, depending on whether trial \\(i\\) resulted in category \\(j\\). (A 1 “indicates” that caregory \\(j\\) occurred.)\nThe key is that\n\\[\nX = \\sum_{i=1}^n I_i\n\\tag{4.10}\\]\nFor instance, in the die-rolling example, the first component on the right-hand side is the number of rolls in which we got 1 dot, and that is by definition the same as \\(N_1\\), the first component of \\(X\\).\nSo there we have it – \\(X\\) is a sum of independent, identically distributed random vectors, so by the Multivariate Central Limit Theorem, \\(X\\) has an approximate multivariate normal distribution. Now, what are the mean vector and covariance matrix in that distribution?\nFrom our discussion above, we know that\n\\[\nEX =\n\\left (\n\\begin{array}{r}\nnp_1 \\\\\n... \\\\\nnp_k \\\\\n\\end{array}\n\\right )\n\\]\nWhat about \\(Cov(X)\\)? Again, recognizing that Equation 4.10 is a sum of independent terms, Equation 4.3 tells us that\n\\[\nCov(X) = Cov(\\sum_{i=1}^n I_i) = \\sum_{i=1}^n Cov(I_i) = n Cov(I_1),\n\\]\nthat last equality reflecting that the \\(I_i\\) are identically distributed (the trials all have the same probabilistic behavior).\nNow to evaluate that covariance matrix, consider two specific elements \\(I_{1j}\\) and \\(I_{im}\\) of \\(I_1\\). Recall, those elements are equal to 1 or 0, depending on whether the first trial results in Categories j and m, respectively. Then what is \\(Cov(I_{1j},I_{1m})\\)? From Equation 4.1, we have\n\\[\nCov(I_{1j},I_{1m}) = E(I_{1j} I_{1m}) - (EI_{1j}) (EI_{1m})\n\\tag{4.11}\\]\nConsider the two cases:\n\n\\(i=j\\): Here \\(E(I_{1j}^2) = E(I_{1j}) = p_j\\). Thus\n\n\\[\nCov(I_{1j},I_{1m}) = p_j (1-p_j)\n\\]\n\n\\(i \\neq j\\): Each \\(I_s\\) consists of one 1 and \\(k-1\\) 0s. Thus \\(E(I_{1j} I_{1m}) = 0\\) and\n\n\\[\nCov(I_{1j},I_{1m}) = -p_j p_m\n\\]",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Covariance Matrices, MV Normal Distribution, Delta Method</span>"
    ]
  },
  {
    "objectID": "Ch2a.html#the-delta-method",
    "href": "Ch2a.html#the-delta-method",
    "title": "4  Covariance Matrices, MV Normal Distribution, Delta Method",
    "section": "4.5 The Delta Method",
    "text": "4.5 The Delta Method\nThis is one of the most useful simple tools in statistics.\n\n4.5.1 Review: confidence intervals, standard errors\nTo set the stage, let’s review the statistical concepts of confidence interval and standard error. Say we have an estimator \\(\\widehat{\\theta}\\) of some population parameter \\(\\theta\\), e.g. \\(\\bar{X}\\) for a population mean \\(\\mu\\).\n\nLoosely speaking, the term standard error of is our estimate of \\(\\sqrt{Var(\\widehat{\\theta})}\\). More precisely, suppose that \\(\\widehat{\\theta}\\) is asymptotically normal. The standard error is an estimate of the standard deviation of that normal distribution. For this reason, It is customary to write \\(AVar(\\widehat{\\theta})\\) rather than \\(Var(\\widehat{\\theta})\\).\nThis can be used to form a confidence interval (see below), but also stands on its own as an indication of the accuracy of \\(\\widehat{\\theta}\\).\nA, say 95%, confidence interval for \\(\\mu\\) is then\n\n\\[\n\\widehat{\\theta} \\pm 1.96 \\widehat{\\theta})\n\\]\nThe 95% figure means that of all possible samples of the given size from the population, 95% of the resulting confidence intervals will contain \\(\\theta\\).\n\n\n4.5.2 Delta method: motivating example\nNow, for the delta method, as a first example, say we are estimating a population mean \\(\\mu\\) and are also interested in estimating \\(\\log(\\mu)\\).\nWe will probably use the sample mean \\(\\bar{X}\\) to estimate \\(\\mu\\), and thus use \\(W = \\log{\\bar{X}}\\) to estimate \\(\\log(\\mu)\\).  But how do we obtain a standard error for \\(W\\)?If we just need to form a confidence interval for \\(\\log(\\mu)\\), we can form a CI for \\(\\mu\\) and then take the log of both endpoints. But again, standard errors are of interest in their own right.\n\n\n4.5.3 Use of the Central Limit Theorem\nThe Central Limit Theorem tells as that \\(\\bar{X}\\) is asymptotically normally distributed. But what about \\(\\log{\\bar{X}}\\)?\nFrom calculus, we know that a smooth function \\(f\\) can be written as a Taylor series,\n\\[\nf(x) = f(x_0) + f'(x_0) (x-x_0) + f''(x_0) (x-x_0)^2 /2 + ...\n\\]\nwhere here “’” denotes derivative rather than matrix transpose.\nIn our case here, setting \\(f(t) = \\log{t}\\), \\(x_0 = \\mu\\) and \\(x =\n\\bar{X}\\), we have\n\\[\nW = \\log{\\mu} + \\log'({\\mu}) (\\bar{X}-\\mu) + \\log''({\\mu}) (\\bar{X}-\\mu)^2 /2 + ...\n\\]\nand \\(\\log'(t) = 1/t\\) and so on.\nThe key point is that as n grows, \\(\\bar{X}-\\mu\\) goes to 0, and \\((\\bar{X}-\\mu)^2\\) goes to 0 even faster. Using theorems from probability theory, one can show that, in the sense of distribution,\n\\[\nW \\approx log(\\mu) + log'(\\mu) (\\bar{X}-\\mu)\n\\]\nIn other words, \\(W\\) has an approximate normal distribution that has mean \\(log(\\mu)\\) and variance\n\\[\n\\frac{1}{\\mu^2} \\sigma^2/n\n\\]\nwhere \\(\\sigma^2\\) is the population variance \\(Var(X)\\) . We estimate the latter by the usual \\(S^2\\) quantity, and thus have our standard error,\n\\[\n\\textrm{s.e.}(W) = \\frac{S}{\\bar{X} \\sqrt{n}}\n\\]\n\n\n4.5.4 Use of the Multivariate Central Limit Theorem\nNow, what if the function \\(f\\) has two arguments instead of one? The above linear approximation is now\n\\[\nf(v,w) \\approx f(v_0,w_0) + f_1(v_0,w_0) (v-v_0) + f_2(v_0,w_0)(w-w_0)\n\\tag{4.12}\\]\nwhere \\(f_1\\) and \\(f_2\\) are partial derivatives,A partial derivative of a function of more than one variable is the derivative with respect to one of those variables. E.g. \\(\\partial/\\partial v ~ vw^2 =\nw^2\\) and \\(\\partial/\\partial w ~ vw^2 =\n2vw\\).\n\\[\nf_1(v,w) = \\frac{\\partial}{\\partial v} f(v,w)\n\\]\n\\[\nf_2(v,w) = \\frac{\\partial}{\\partial w} f(v,w)\n\\]\nSo if we are estimating, for instance, a population quantity \\((\\alpha,\\beta)'\\) by \\((Q,R)'\\), standard error of the latter is\n\\[\n\\sqrt{\nf_1^2 (Q,R) AVar(Q) +\nf_2^2 (Q,R) AVar(R) +\n2 f_1(Q,R) f_2(Q,R) ACov(Q,R)\n}\n\\]\nAs usual, use of matrix notation can help clean up messy expressions like this. The gradient of \\(f\\), say in the two-argument case as above, is the vector\n\\[\n\\nabla f =\n\\left (\n\\begin{array}{r}\nf_1 (v_0,w_0) \\\\\nf_2 (v_0,w_0) \\\\\n\\end{array}\n\\right )\n\\]\nso that Equation 4.12 can be written as\n\\[\nf(v,w) \\approx f(v_0,w_0) + (\\nabla f)'\n\\left (\n\\begin{array}{r}\nv-v_0 \\\\\nw-w_0 \\\\\n\\end{array}\n\\right )\n\\]\nThen from Equation 4.5,\n\\[\nAVar[f(Q,R)] = (\\nabla f)' AV(Q,R) (\\nabla f)\n\\tag{4.13}\\]\n\n\n4.5.5 Example: ratio of two means\nSay our sample data consists of mother-daughter pairs,\n\\[\n\\left (\n\\begin{array}{r}\nM \\\\\nD \\\\\n\\end{array}\n\\right )\n\\]\nrepresenting the heights of mother and daughter. Denote the population mean vector by\n\\[\n\\nu =\n\\left (\n\\begin{array}{r}\n\\mu_M \\\\\n\\mu_D \\\\\n\\end{array}\n\\right )\n\\]\nWe might be interested in the ratio \\(\\omega = \\mu_D / \\mu_M\\). Our estimator will be \\(\\widehat{\\omega} = \\bar{D} / \\bar{M}\\), the ratio of the sample means.\nSo take \\(Q = \\bar{D}\\) and \\(R =  \\bar{M}\\). Then in Equation 4.13, with \\(f(q,r) = q/r\\)\n\\[\n\\nabla{f} =\n\\left (\n\\begin{array}{r}\n1/r \\\\\n-q/r^2  \\\\\n\\end{array}\n\\right )\n\\]\nwhich in our application here we would approximate by\n\\[\n\\nabla{f} =\n\\left (\n\\begin{array}{r}\n1/\\bar{M} \\\\\n-\\bar{D}/\\bar{M}^2  \\\\\n\\end{array}\n\\right )\n\\]\nAs to \\(AVar(v,w)\\) in Equation 4.13, we would use the multivariate analog of the usual \\(S^2\\) in the univariate case, taking advantage of Equation 4.2. One must be careful, though, making sure we choose the appropriate quantity for \\(AVar(v,w)\\).\nFor instance, in our ratio example here, \\((Q,R)'\\) is \\((\\bar{M},\\bar{D}\\). To obtain \\(AVar(v,w)\\), let’s first look at the covariance matrix \\(\\Sigma\\),\n\\[\n\\Sigma = E[(M - EM) (D - ED)']\n\\]\nThis is the average of \\((M - EM) (D - ED)\\) in the population. The sample analog average isThe reader may recall that in estmating a variance, it is customary to divide by \\(n-1\\) instead of \\(n\\), due to unbiasedness. The same is true for the multivariate analog of variance, i.e. covariance matrices, but we will use \\(n\\) to retain the sample analog theme.\n\\[\n\\widehat{\\Sigma} =\n\\frac{1}{n}\n\\sum_{i=1}^n (M_i - \\bar{M}) (D_i - \\bar{D})'\n\\]\nwhere \\((M_i,D_i)\\) represents the \\(i^{th}\\) mother-daughter pair in our dataset.\nBut \\(\\widehat{{\\Sigma}}\\) is not our \\(AVar(v,w)\\) here, because we need, for instance, the variance of \\(\\bar{M}\\) rather than the variance of \\(M\\). Recall that the former is \\(1/n\\) times the latter. So in this case we have\n\\[\nAVar(v,w) = \\frac{1}{n} \\widehat{\\Sigma}\n\\]\nSo now we can obtain a standard error for \\(\\widehat{\\omega}\\):\n\\[\n\\sqrt{\n\\frac{1}{n}\n\\nabla f' \\widehat{\\Sigma} \\nabla f\n}\n\\]\nfrom which we can form a confidence interval.\nIn R functions to do parametric regression modeling, Maximum Likelihood Estimation and so on, \\(AVar(v,w)\\) is available from the function’s return value.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Covariance Matrices, MV Normal Distribution, Delta Method</span>"
    ]
  },
  {
    "objectID": "Ch2a.html#example-iranian-churn-data",
    "href": "Ch2a.html#example-iranian-churn-data",
    "title": "4  Covariance Matrices, MV Normal Distribution, Delta Method",
    "section": "4.6 Example: Iranian Churn Data",
    "text": "4.6 Example: Iranian Churn Data\nHere we predict whether a telecom customer will move to another provider. Here we illustrate how to obtain \\(AVar(v,w)\\).\n\ndata(IranianChurn)\nglmOut &lt;- glm(Exited ~ ., data = iranChurn, family = binomial)\nvcov(glmOut)\n\n                   (Intercept)   CreditScore GeographyGermany GeographySpain\n(Intercept)       5.993100e-02 -5.051370e-05    -1.589878e-04  -1.388436e-03\nCreditScore      -5.051370e-05  7.859301e-08    -2.683572e-07  -2.454056e-07\nGeographyGermany -1.589878e-04 -2.683572e-07     4.579770e-03   1.678887e-03\nGeographySpain   -1.388436e-03 -2.454056e-07     1.678887e-03   4.989715e-03\nGenderMale       -1.350817e-03  2.156023e-07     2.008595e-05  -2.200094e-05\nAge              -2.685656e-04 -7.969688e-09     6.042109e-06  -2.107579e-06\nTenure           -4.049072e-04  1.614151e-09    -6.645863e-06   3.828636e-06\nBalance          -2.936332e-08  1.033099e-13    -1.358788e-08  -2.156656e-11\nNumOfProducts    -3.775016e-03 -8.271320e-08    -3.655500e-04  -3.357316e-05\nHasCrCard1       -2.595873e-03  2.066409e-07    -7.611739e-05   3.603876e-05\nIsActiveMember1   5.010389e-04 -3.102862e-07    -1.008754e-04  -3.954646e-05\nEstimatedSalary  -2.301952e-08  1.070775e-12    -7.124867e-11  -8.778366e-11\n                    GenderMale           Age        Tenure       Balance\n(Intercept)      -1.350817e-03 -2.685656e-04 -4.049072e-04 -2.936332e-08\nCreditScore       2.156023e-07 -7.969688e-09  1.614151e-09  1.033099e-13\nGeographyGermany  2.008595e-05  6.042109e-06 -6.645863e-06 -1.358788e-08\nGeographySpain   -2.200094e-05 -2.107579e-06  3.828636e-06 -2.156656e-11\nGenderMale        2.968982e-03 -4.697135e-06 -7.714322e-06 -9.609423e-10\nAge              -4.697135e-06  6.633235e-06 -2.330915e-07  4.769766e-11\nTenure           -7.714322e-06 -2.330915e-07  8.751351e-05 -1.419504e-11\nBalance          -9.609423e-10  4.769766e-11 -1.419504e-11  2.644146e-13\nNumOfProducts     5.853316e-05  2.887014e-06 -2.575910e-06  6.486034e-09\nHasCrCard1       -8.288460e-06  1.405830e-07 -1.356629e-05  6.276417e-10\nIsActiveMember1   2.637202e-05 -3.924088e-05  1.978922e-05 -5.129968e-10\nEstimatedSalary   1.976874e-10  1.824674e-11 -7.850909e-11 -3.430272e-15\n                 NumOfProducts    HasCrCard1 IsActiveMember1 EstimatedSalary\n(Intercept)      -3.775016e-03 -2.595873e-03    5.010389e-04   -2.301952e-08\nCreditScore      -8.271320e-08  2.066409e-07   -3.102862e-07    1.070775e-12\nGeographyGermany -3.655500e-04 -7.611739e-05   -1.008754e-04   -7.124867e-11\nGeographySpain   -3.357316e-05  3.603876e-05   -3.954646e-05   -8.778366e-11\nGenderMale        5.853316e-05 -8.288460e-06    2.637202e-05    1.976874e-10\nAge               2.887014e-06  1.405830e-07   -3.924088e-05    1.824674e-11\nTenure           -2.575910e-06 -1.356629e-05    1.978922e-05   -7.850909e-11\nBalance           6.486034e-09  6.276417e-10   -5.129968e-10   -3.430272e-15\nNumOfProducts     2.221635e-03 -3.379445e-06   -3.685755e-05   -4.030670e-10\nHasCrCard1       -3.379445e-06  3.521179e-03    6.930480e-05    3.401694e-11\nIsActiveMember1  -3.685755e-05  6.930480e-05    3.327629e-03    2.676211e-10\nEstimatedSalary  -4.030670e-10  3.401694e-11    2.676211e-10    2.243567e-13\n\n\nThere are several categorical variables here, so after expansion to dummies, \\(AVar(v,w)\\) is \\(12 \\times 12\\). This is the covariance matrix for the vector of estimated logistic regression coefficients \\(\\widehat{\\beta}\\). There are many different functions \\(f(\\beta)\\) that might be of interest.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Covariance Matrices, MV Normal Distribution, Delta Method</span>"
    ]
  },
  {
    "objectID": "Ch2a.html#example-motherdaughter-height-data",
    "href": "Ch2a.html#example-motherdaughter-height-data",
    "title": "4  Covariance Matrices, MV Normal Distribution, Delta Method",
    "section": "4.7 Example: Mother/Daughter Height Data",
    "text": "4.7 Example: Mother/Daughter Height Data\n\nlibrary(WackyData)\ndata(Heights)\nhead(heights)\n\n  Mheight Dheight\n1    59.7    55.1\n2    58.2    56.5\n3    60.6    56.0\n4    60.7    56.8\n5    61.8    56.0\n6    55.5    57.9\n\nm &lt;- heights[,1]\nd &lt;- heights[,2]\nmeanm &lt;- mean(m)\nmeand &lt;- mean(d)\nfDel &lt;- matrix(c(1/meanm,-meand/meanm^2),ncol=1)\nn &lt;- length(m)\nsigma &lt;- (1/n) * cov(cbind(m,d))\nse &lt;- sqrt(t(fDel) %*% sigma %*% fDel)\nse\n\n            [,1]\n[1,] 0.001097201\n\nmeanmd &lt;-meanm / meand\nmeanmd\n\n[1] 0.9796356\n\nc(meanmd - 1.96*se, meanmd + 1.96*se)\n\n[1] 0.9774850 0.9817861",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Covariance Matrices, MV Normal Distribution, Delta Method</span>"
    ]
  },
  {
    "objectID": "Ch2a.html#regarding-those-pesky-derivatives",
    "href": "Ch2a.html#regarding-those-pesky-derivatives",
    "title": "4  Covariance Matrices, MV Normal Distribution, Delta Method",
    "section": "4.8 Regarding Those Pesky Derivatives",
    "text": "4.8 Regarding Those Pesky Derivatives\nThough finding expressions for the derivatives in the above example was not onerous, the function \\(f\\) can be rather complex, with the expressions for its derivatives even more complicated. Typically such tedious and error-prone operations can be avoided, by having the software calculate approximate derivatives.\nRecall the definition of derivative:\n\\[\nf'(x) = \\lim_{w \\rightarrow 0}\n\\frac{f(x+w) - f(x)}{w}\n\\]\nSo an aproximate value of \\(f'(x)\\) is obtained by choosing some small value of \\(w\\) and evaluating\n\\[\n\\frac{f(x+w) - f(x)}{w}\n\\]\nThough of course there is an issue with one’s choice of \\(w\\), the point is that one can code the software to find approximate derivatives automatically using this device. This is very common in Data Science libraries.\nFor example, the R package numDeriv will compute numerical derivatives.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Covariance Matrices, MV Normal Distribution, Delta Method</span>"
    ]
  },
  {
    "objectID": "Ch2a.html#your-turn",
    "href": "Ch2a.html#your-turn",
    "title": "4  Covariance Matrices, MV Normal Distribution, Delta Method",
    "section": "4.9 Your Turn",
    "text": "4.9 Your Turn\n❄️ Your Turn: In the mother/daughter data, find the estimated covariance between the two heights.\n❄️ Your Turn: In Equation 4.7 with \\(k = 2\\), write \\(t = (t_1,t_2)\\), and consider the quantity in the exponent,\n\\[\n(t-\\mu)'(\\Sigma)^{-1}(t-\\mu)\n\\]\nSay we set this quantity to some constant \\(c\\), then graph the locus of points \\(t\\) in the \\(t_1,t_2\\) plane. What geometric figure would we get?\n❄️ Your Turn: In Theorem 4.1, suppose\n\\[\n\\mu =\n\\left (\n\\begin{array}{r}\n1.5 \\\\\n8.0 \\\\\n\\end{array}\n\\right )\n\\]\nand\n\\[\n\\Sigma =\n\\left (\n\\begin{array}{rr}\n5.2 & 6.2 \\\\\n6.2 & 20.1  \\\\\n\\end{array}\n\\right )\n\\]\nFind the regression line\n\\[\n\\textrm{ mean Y } = a + bX\n\\]\n❄️ Your Turn: Show that in the scalar context,\n\\[\nCov(X,Y) = E(XY) - EX ~ EY\n\\]\n❄️ Your Turn: Show that in the vector context,\n\\[\nCov(X) = E(X X') - (EX) (EX)'\n\\]\n❄️ Your Turn: Suppose\n\\[\nW = X \\beta + \\alpha S\n\\]\nfor a random vector \\(X\\), a scalar random variable \\(S\\), a nonrandom vector \\(\\beta\\) and a nonrandom scalar \\(\\alpha\\). Show that\n\\[\nCov(W,S) = \\beta' Cov(X,S) + \\alpha Var(S)\n\\]",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Covariance Matrices, MV Normal Distribution, Delta Method</span>"
    ]
  },
  {
    "objectID": "Ch3.html",
    "href": "Ch3.html",
    "title": "5  Linear Statistical Models",
    "section": "",
    "text": "5.1 Linear Regression through the Origin\nThis chapter could have been titled, say, “Optimization, Part I,” since many applications of linear algebra involve minimization or maximization of some kind, and this chapter will involve calculus derivatives. But the statistical applications of linear algebra are equally important.\nLet’s consider the Nile dataset built-in to R. It is a time series, one measurement per year.\nhead(Nile)\n\n[1] 1120 1160  963 1210 1160 1160\n\n# predict current year from previous year?\nn1 &lt;- Nile[-(length(Nile))]\nhead(n1)\n\n[1] 1120 1160  963 1210 1160 1160\n\nn2 &lt;- Nile[-1]\nhead(n2)\n\n[1] 1160  963 1210 1160 1160  813\n\nplot(n1,n2,cex=0.4,xlim=c(0,1400),yli=c(0,1400))\nWe would like to fit a straight line through that data point cloud. We might have two motivations for doing this:\n\\[E(C | V) = \\beta V\\]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Linear Statistical Models</span>"
    ]
  },
  {
    "objectID": "Ch3.html#linear-regression-through-the-origin",
    "href": "Ch3.html#linear-regression-through-the-origin",
    "title": "5  Linear Statistical Models",
    "section": "",
    "text": "The line might serve as nice summary of the data.\nMore formally, let \\(C\\) and \\(V\\) denote the current and previous year’s measurements..Then the model\n\n\nmay be useful. Here the slope \\(\\beta\\) is an unknown value to be estimated from the data. Readers with some background in linear regression models should note that this assumption of a linear trend through the origin is the only assumption we are making here. Nothing on normal distributions etc.\n\n\n\n\n\n\nModel Validity\n\n\n\nThe great statistician George Box once said, “All models are wrong but some are useful.” All data scientists should keep this at the forefronts of their minds.\n\n\n\n5.1.1 Least squares approach\nWe wish to estimate \\(\\beta\\) from our data, which we regard as a sample from the data generating process. Denote our data by \\((C_i,V_i), i =\n1,...,100\\). Let \\(\\widehat{\\beta}\\) denote our estimate. How should we obtain it?This too is something all data scientists should keep at the forefronts of their minds. We are always working with sample data, subject to intersample variations. The quantity \\(\\widehat{\\beta}\\) is a random variable.The “hat” notation ‘^’ is statistical convention for “estimate of”; \\(\\widehat{\\beta}\\) is our estimate of the unknown parameter \\(\\beta\\).\nPretend for a moment that we don’t know, say, \\(C_{28}\\). Using our estimated \\(\\beta\\), our predicted value would be \\(\\widehat{\\beta}\nV_{28}\\). Our squared prediction error would then be \\((C_{28} - \\widehat{\\beta}\nW_{28})^2\\).\nWell, we actually do know \\(C_{28}\\) (and the others in our data), so we can answer the question:\n\nIn our search for a good value of \\(\\widehat{\\beta}\\), we can ask how well we would predict our known data, using that candidate value of \\(\\widehat{\\beta}\\) in our data. Our total squared prediction error would be\n\\[\n\\sum_{i=1}^{100} [C_{i} - \\widehat{\\beta} V_{i} )^2\n\\]\nA natural choice for \\(b\\) would be the value that minimizes this quantity. Why not look at the absolute value instead of the square? The latter makes the math flow well, as will be seen shortly.\n\n\n\n5.1.2 Calculation\nAs noted, our choice for \\(b\\) will be the minimizer of\n\\[\n\\sum_{i=1}^{100} (C_{i} - b V_{i})^2\n\\]\nover all possible values of \\(b\\). We then set \\(\\widehat{\\beta}\\) to that minimizing value of \\(b\\).\nThis is a straightforward calculus problem. Setting\n\\[\n0 = \\frac{d}{db} \\sum_{i=1}^{100} (C_{i} - b V_{i} )^2 =\n-2 \\sum_{i=1}^{100} (C_{i} - b V_{i}) V_i\n\\]\nand solving \\(b\\), we find that\n\\[\nb = \\frac{\\sum_{i=1}^n C_i V_i}{\\sum_{i=1}^nV_i^2}\n\\]\n\n\n5.1.3 R code\n\nlm(n2 ~ n1-1)\n\n\nCall:\nlm(formula = n2 ~ n1 - 1)\n\nCoefficients:\n  n1  \n0.98  \n\n\nThis says, “Fit the model \\(E(C | V) = \\beta V\\) to the data, with the line constrained to pass through the origin.” The constraint is specified by the -1 term.\nWe see that the estimate regression line is\n\\[E(C | V) = 0.98 V\\]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Linear Statistical Models</span>"
    ]
  },
  {
    "objectID": "Ch3.html#linear-regression-model-with-intercept-term",
    "href": "Ch3.html#linear-regression-model-with-intercept-term",
    "title": "5  Linear Statistical Models",
    "section": "5.2 Linear Regression Model with Intercept Term",
    "text": "5.2 Linear Regression Model with Intercept Term\nSay we do not want to constrain the model to pass the line through the origin. Our model is then\n\\[E(C | V) = \\beta_0 + \\beta_1 V\\]\nwhere we now have two unknown parameters to be estimated.\n\n5.2.1 Least-squares estimation, single predictor\nOur sum of squared prediction errors is now\n\\[\n\\sum_{i=1}^{100} [O_{i} - (b_0 + b_1 V_{i}) ]^2\n\\]\nThis means setting two derivatives to 0 and solving. Since the derivatives involve two different quantities to be optimized, \\(b_0\\) and \\(b_1\\), the derivatives are termed partial, and the \\(\\partial\\) symbol is used instead of ‘d’.\n\\[\\begin{align}\n0 &=\n\\frac{\\partial}{\\partial b_0}\\sum_{i=1}^{100} [C_{i} - (b_0 + b_1 V_{i })]^2 \\\\\n&=\n-2 \\sum_{i=1}^{100} [C_{i} - (b_0 + b_1 V_{i}) ]\n\\end{align}\\]\nand\n\\[\\begin{align}\n0 &=\n\\frac{\\partial}{\\partial b_1}\\sum_{i=1}^{100} [C_{i} - (b_0 + b_1 V_{i})]^2 \\\\\n&=\n-2 \\sum_{i=1}^{100} [C_{i} - (b_0 + b_1 V_{i})] V_i\n\\end{align}\\]\nWe could then solve for the \\(b_i\\), but let’s go straight to the general case.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Linear Statistical Models</span>"
    ]
  },
  {
    "objectID": "Ch3.html#least-squares-estimation-general-number-of-predictors",
    "href": "Ch3.html#least-squares-estimation-general-number-of-predictors",
    "title": "5  Linear Statistical Models",
    "section": "5.3 Least-Squares Estimation, General Number of Predictors",
    "text": "5.3 Least-Squares Estimation, General Number of Predictors\n\n5.3.1 Nile example\nAs we have seen, systems of linear equations are natural applications of linear algebra. The equations setting the derivatives to 0 can be written in matrix terms as\n\\[\n\\left (\n\\begin{array}{r}\n\\sum_{i=1}^n C_i \\\\\n\\sum_{i=1}^n C_i V_i \\\\\n\\end{array}\n\\right ) =\n\\left (\n\\begin{array}{rr}\n100 & \\sum_{i=1}^n V_i \\\\\n\\sum_{i=1}^n V_i & b_1 \\sum_{i=1}^n V_i^2 \\\\\n\\end{array}\n\\right )\n\\left (\n\\begin{array}{r}\nb_0 \\\\\nb_1 \\\\\n\\end{array}\n\\right )\n\\tag{5.1}\\]\nActually, that matrix equation can be derived more easily by using matrices to begin with:\nDefine \\(S\\) and \\(T\\):\n\\[\nS =\n\\left (\n\\begin{array}{r}\nC_1 \\\\\nC_2 \\\\\n... \\\\\nC_{100} \\\\\n\\end{array}\n\\right )\n\\]\nand\n\\[\nT =\n\\left (\n\\begin{array}{r}\nV_1 \\\\\nV_2 \\\\\n... \\\\\nV_{100} \\\\\n\\end{array}\n\\right )\n\\]\nThen our linear assumption, \\(E(C | V) = \\beta _0 + \\beta_1 V\\), applied to \\(S\\) and \\(T\\), is\n\\[\nE(S | T) =\nA \\beta\n\\]\nwhere\n\\[\nA =  \n\\left (\n\\begin{array}{rr}\n1 & V_1 \\\\\n1 & V_2 \\\\\n... & ... \\\\\n1 & V_{100} \\\\\n\\end{array}\n\\right )\n\\]\nand \\(\\beta = (\\beta_0,\\beta_1)'.\\)\nOur predicted error vector, using our candidate estimate \\(b\\) of \\(\\beta\\), is very simply expressed:\n\\[\nS - Ab\n\\]\nAnd since for any column vector \\(u\\), the sum of its squared elements is\n\\[\nu'u\n\\]\nour sum of squared prediction errors is\n\\[\n(S - Ab)'(S - Ab)\n\\tag{5.2}\\]\nNow how we will minimize that matrix expression with respect to the vector \\(b\\)? That is the subject of the next section.\n\n\n5.3.2 Matrix derivatives\nThe (column) vector of partial derivatives of a scalar quantity is called the gradient of that quantity. For instance, with\n\\[\nu = 2x + 3y^2 + xy\n\\]\nwe have that its gradient is\n\\[\n\\left (\n\\begin{array}{r}\n2 + y \\\\\n6y + x \\\\\n\\end{array}\n\\right )\n\\]\nWith care, we can compute gradients entirely at the matrix level, using easily derivable properties, without ever resorting to returning to the scalar expressions. Let’s apply them to the case at hand in the last section,\n\\[\n(S - Ab)'(S - Ab)\n\\tag{5.3}\\]\n\n\n5.3.3 Differentiation purely in matrix terms\nIt can be shown that for a column vector \\(a\\),\n\\[\n\\frac{d}{da} a'a = 2a\n\\tag{5.4}\\]\nEquation 5.3 is indeed of the form \\(a'a\\), but the problem here is that \\(a\\) in turn is a function of \\(b\\), This calls for the Chain Rule, which does exist at the matrix level:\nFor example if \\(u = Mv + w\\), with \\(M\\) and \\(w\\) constants (i.e. not functions of \\(v\\), then\n\\[\n\\frac{d}{dv} u'u = 2M'u\n\\]We must keep in mind that we are working with vectors and matrices, so that \\(M'u\\), say \\(r \\times s\\) times \\(s \\times 1\\), is conformable matrix multiplication.\nIn our case at hand, we have \\(M = -A\\) and \\(w = S\\), so that\n\\[\n\\frac{d}{db} [(S - Ab)'(S - Ab) = -2 A'(S - Ab)\n\\]\nSo, set\n\\[\n0 = A'(S - Ab) = A'S - A'A b\n\\]\nyield our minimizing \\(b\\):\n\\[\n\\widehat{\\beta} = (A'A)^{-1} A'S\n\\tag{5.5}\\]\nproviding the inverse exists (more on this in the next chapter).\nLet’s check this with the Nile example:\n\nA &lt;- cbind(1,n1)\nS &lt;- n2\nAp &lt;- t(A)  # R matrix transpose\nsolve(Ap %*% A) %*% Ap %*% S  # R matrix inverse\n\n          [,1]\n   452.7667508\nn1   0.5043159\n\n# check via R\nlm(n2 ~ n1)\n\n\nCall:\nlm(formula = n2 ~ n1)\n\nCoefficients:\n(Intercept)           n1  \n   452.7668       0.5043  \n\n\n\ndet(Ap %*% A) \n\n[1] 277463876\n\n\nNonzero! So \\((A'A)^{-1}\\) does exist, as we saw.\n\n\n5.3.4 The general case\nSay our data consists of \\(n\\) points, each of which is of length \\(p\\). Write the \\(j^{th}\\) element of the \\(i^{th}\\) data point as \\(X_{ij}\\). Then set\n\\[\nA =\n\\left (\n\\begin{array}{rrrr}\n1 & X_{11} & ... & X_{p1} \\\\\n1 & X_{21} & ... & X_{p2} \\\\\n... & ... & ... & ... \\\\\n1 & X_{n1} & ... & X_{np} \\\\\n\\end{array}\n\\right )\n\\tag{5.6}\\]\nContinue to set \\(S\\) to the length-\\(n\\) column vector of our response variable. Our model is\n\\[\nE(S | A) = A \\beta\n\\]\nfor an unknown vector \\(\\beta\\) of length \\(p+1\\). Our estimated of that vector based on our data will be denoted\n\\[\nb = (b_0,b_1,...,b_p)'\n\\]\nThen, using the same reasoning as before, we have the minimizing value of \\(b\\):\n\\[\n\\widehat{\\beta} = (A'A)^{-1} A'S\n\\tag{5.7}\\]\nagain providing that the inverse exists.\n\n\n5.3.5 Example: mlb1 data\nAs an example, let’s take the mlb1 from my qeML (’Quick and Easy Machine Learning package.  The data is on major league baseball players. We will predict weight from height and age.Dataset kindly provided by the UCLA Dept. of Statistics\n\nlibrary(qeML)\ndata(mlb1)\nhead(mlb1)\n\n        Position Height Weight   Age\n1        Catcher     74    180 22.99\n2        Catcher     74    215 34.69\n3        Catcher     72    210 30.78\n4  First_Baseman     72    210 35.43\n5  First_Baseman     73    188 35.71\n6 Second_Baseman     69    176 29.39\n\nourData &lt;- as.matrix(mlb1[,-1]) # must have matrix to enable %*%\nhead(ourData)\n\n  Height Weight   Age\n1     74    180 22.99\n2     74    215 34.69\n3     72    210 30.78\n4     72    210 35.43\n5     73    188 35.71\n6     69    176 29.39\n\nA &lt;- cbind(1,ourData[,c(1,3)])\nAp &lt;- t(A)\nS &lt;- as.vector(mlb1[,3])\nsolve(Ap %*% A) %*% Ap %*% S\n\n               [,1]\n       -187.6381754\nHeight    4.9235994\nAge       0.9115326\n\n# check via R\nlm(Weight ~ .,data=mlb1[,-1])\n\n\nCall:\nlm(formula = Weight ~ ., data = mlb1[, -1])\n\nCoefficients:\n(Intercept)       Height          Age  \n  -187.6382       4.9236       0.9115  \n\n\nSo, if we have a player of known height and age, we would predict the weight to be\n-187.6382 + 4.9236 x height + 0.9115 x age\n\n\n5.3.6 Homogeneous variance case\nIn addition to\n\\[\nE(S | A) = A \\beta\n\\]\nit is often assumed thatIn some applications, \\(A\\) is actually chosen by an experimenter, so that it is not random. But even in the random case, it is standard to condition on \\(A\\). By Equation 8.5, a conditional confidence interval, say, at the 95% level also has the level unconditionally.\n\\[\nCov(S | A) = \\sigma^2 I\n\\]\nwhere \\(\\sigma\\) is an unknown constant to be estimated from the data.\nSo \\(Var(S_i) = \\sigma^2\\) for all \\(i\\). It is usually not a realistic assumption. Say for instance we are predicting human weight from height. There should be more variation in weight among taller people than above shorter people. But it’s a simplifying assumption without really good alternatives, so it is commonly used.\nAnd in that setting, our formulas from Chapter 4 come in handy, as follows.\nRecall that \\(\\widehat{\\beta}\\) is our estimate of the unknown population parameter \\(\\beta\\), based on our random sample data. But that means that \\(\\widehat{\\beta}\\) is a random vector, and thus has a covariance matrix. Using Equation 4.4 and setting \\(R = (A'A)^{-1} A'\\)$, we have\n\\[\nCov(\\widehat{\\beta}|A) = R Cov(S|A) R' = R \\sigma^2 I R' = \\sigma^2 R R' =\n\\sigma^2 (A'A)^{-1}\n\\tag{5.8}\\]\nClassical statistical formulas use this relation to find standard errors for \\(\\widehat{\\beta}_i\\) etc.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Linear Statistical Models</span>"
    ]
  },
  {
    "objectID": "Ch3.html#update-formulas",
    "href": "Ch3.html#update-formulas",
    "title": "5  Linear Statistical Models",
    "section": "5.4 Update Formulas",
    "text": "5.4 Update Formulas\nOne important theme in developing prediction models (linear regression, neural networks etc.) is the avoidance of overfitting, meaning that we fit an overly elaborate model to our data. We simply are estimating too many things for the amount of data we have, “spreading our data too thin.”\nA common example is using too many predictor variables, so that we are estimating a large number of coefficients \\(\\beta_i\\).\nOr we may draw a histogram with too many bins:\n\nlibrary(qeML)\ndata(forest500)  # data on forest ground cover\nhist(forest500$V1,breaks=10)\n\n\n\n\n\n\n\nhist(forest500$V1,breaks=100)\n\n\n\n\n\n\n\n\nIn a histogram, we are estimating the heights of the bins. With 10 bins we obtained a smooth graph, but with 100 bins it became choppy. So again, we are estimating too many things, given the capacity of the data.A histogram is an estimate of the probability density function of the observed random variable. Having more, thus narrower, bins reduces bias but increases variance.\nWith larger datasets, we can use more predictor variables, more histogram bins, and so on. The question then arises is, for instance, How many predictors, how many bins and so on, can we afford to use with our given data?\nThe typical solution is to fit several models of different complexity, then choose the one that predicts the best. But we must do this evaluation on “fresh” data; we should not predict on the same data on which we fitted our model.\nWe thus rely on partitioning our data, into a training set to which we fit our model, and a test set, on which we predict using the fitted model. We may wish to do this several times.\nA special case is the Leaving One Out method, in which the holdout set size is 1. It might go like this, for a dataset d:\nsumErrs = 0\nfor i = 1,...,n  # dataset has n datapoints\n   fit lm to d[-i,]\n   use result to predict d[i,]\n   add prediction error to sumErrs\nreturn sumErrs\nThis can become computationally challenging, as we would need to refit the model each time. Each call to lm involves a matrix inversion (equivalent), and we must do this \\(n\\) times.\nIt would be nice if we could have an “update” formula that would quickly recalculate the model found on the full dataset. Then e would need to perform matrix inversion just once. In the case of linear models, such a formula exists, in the Sherman-Morrison-Woodbury relation:\n\nGiven an invertible matrix \\(B\\) and row vectors \\(u\\) and \\(v\\) having lengths equal to the number of columns of \\(B\\). form the matrix\n\\[\nC = B + uv'\n\\]\nThen \\(C^{-1}\\) exists and is equal toNote that the quantity \\(uv'\\) is a square matrix the size of \\(B\\), so the sum and product make sense.\n\\[\nB^{-1} - \\frac{1}{1+v'B^{-1}u} B^{-1} (uv') B^{-1}\n\\]\n\nNow, how can we apply this to the Leave One Out method? In the matrix \\(A\\) in Equation Equation 5.6, we wish to remove row \\(i\\); call the result \\(A_{-i}\\). Our new version of \\(A'A\\) is then\n\\[\nA_{-i}' A_{-i}\n\\]\nSo our main task is to obtain\n\\[\n(A_{-i}' A_{-i})^{-1}\n\\]\nby updating \\((A'A)^{-1}\\), which we already have from our computation in the full dataset.\nWe can do this as follows. Denote row \\(i\\) of \\(A\\) by \\(a_i\\), and set\n\\[\nu = -a_i, v = a_i\n\\]\nTo show why these choices for \\(u\\) and \\(v\\) work, consider the case in which we delete the last row of \\(A\\). (The analysis would be similar for other cases.) Write the latter as a partitioned matrix,\n\\[\n\\left (\n\\begin{array}{r}\nA_{(-n)} \\\\\na_n \\\\\n\\end{array}\n\\right )\n\\]\nWe pretend it is a \\(2 \\times 1\\) “matrix,” and \\(A'\\) is then “\\(1\n\\times 2\\)”:\n\\[\nA' = (A_{(-n)}' | a_n')\n\\]\nThus\n\\[\nA'A =\nA_{-i}' A_{-i} + a_n a_n'\n\\]\nyielding\n\\[\nA_{-i}' A_{-i} = A'A - a_n a_n'\n\\]\njust what we need for Sherman-Morrison-Woodbury: With \\(B = A'A\\), we have\n\\[\n[A_{-i}' A_{-i}]^{-1} =\nB^{-1} + \\frac{1}{1-a_i'B^{-1}a_i}\nB^{-1} (a_i a_i') B^{-1}\n\\]\nLet’s check it:\n\na &lt;- rbind(c(1,3,2),c(1,0,5),c(1,1,1),c(1,9,-3)) \napa &lt;- t(a) %*% a\napai &lt;- solve(apa)\na2 &lt;- a[-2,]\napa2 &lt;- t(a2) %*% a2\napa2i &lt;- solve(apa2)\n# prepare for S-M-W\nadel &lt;- matrix(a[2,],ncol=1)\nw1 &lt;- 1/(1 - t(adel) %*% apai %*% adel)\nw1 &lt;- as.numeric(w1)\nuvt &lt;- adel %*% t(adel)\nw2 &lt;- apai %*% uvt %*% apai\n# S-M-W says this will be apa2i\napai + w1 * w2\n\n           [,1]       [,2]      [,3]\n[1,]  3.4140625 -0.7109375 -1.015625\n[2,] -0.7109375  0.1640625  0.234375\n[3,] -1.0156250  0.2343750  0.406250\n\napa2i\n\n           [,1]       [,2]      [,3]\n[1,]  3.4140625 -0.7109375 -1.015625\n[2,] -0.7109375  0.1640625  0.234375\n[3,] -1.0156250  0.2343750  0.406250",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Linear Statistical Models</span>"
    ]
  },
  {
    "objectID": "Ch3.html#your-turn",
    "href": "Ch3.html#your-turn",
    "title": "5  Linear Statistical Models",
    "section": "5.5 Your Turn",
    "text": "5.5 Your Turn\n❄️ Your Turn: Due to the Multivariate Central Limit Theorem, many common statistical estimators have approximately normal distributions. In R, functions such as lm, glm, lme and coxph come with an associated function vcov. This gives the approximate covariance  matrix of the computed estimator, e.g. for the estimated coefficients vector in a linear model. This enables formation of approximate confidence intervals for not only individual model parameters but also linear combinations of them.As seen in Equation 5.1, least-squares estimators are composed of various sums, making them asymptotically normal by the Multivariate Central Limit Theorem. Thus we can form approximate confidence intervals even without assuming normal distributions for “Y.”\nWrite an R function with call form\nregFtnCI(lmOut,t,alpha)\nthat returns an approximate \\((1-\\alpha)\\) confidence interval for the conditional mean \\(E(Y | X=t)\\). Here lmOut is the object returned by a call to lm.\n❄️ Your Turn: Show Equation 5.4. Write \\(a = (a_1,...,a_k)'\\) and find the gradient “by hand.” Compare to \\(2a\\).\n❄️ Your Turn: Show that\n\\[\n\\frac{d}{du} u'Qu = 2Qu\n\\]\nfor a constant symmetric matrix \\(Q\\) and a vector \\(u\\). (\\(u'Qu\\) is called a quadratic form.)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Linear Statistical Models</span>"
    ]
  },
  {
    "objectID": "Ch4.html",
    "href": "Ch4.html",
    "title": "6  Matrix Rank",
    "section": "",
    "text": "6.1 Example: Census Data\nIn our computations in the latter part of the last chapter, we added a proviso that \\((A'A)^{-1}\\) exists. In this chapter, we’ll present a counterexample, which will naturally lead into our covering matrix rank, and in the next chapter, the basics of vector spaces.\nThis dataset is also from qeML. It is data for Silicon Valley engineers in the 2000 Census. Let’s focus on just a few columns.\ndata(svcensus) \nhead(svcensus) \n\n       age     educ occ wageinc wkswrkd gender\n1 50.30082 zzzOther 102   75000      52 female\n2 41.10139 zzzOther 101   12300      20   male\n3 24.67374 zzzOther 102   15400      52 female\n4 50.19951 zzzOther 100       0      52   male\n5 51.18112 zzzOther 100     160       1 female\n6 57.70413 zzzOther 100       0       0   male\n\nsvc &lt;- svcensus[,c(1,4:6)] \nhead(svc) \n\n       age wageinc wkswrkd gender\n1 50.30082   75000      52 female\n2 41.10139   12300      20   male\n3 24.67374   15400      52 female\n4 50.19951       0      52   male\n5 51.18112     160       1 female\n6 57.70413       0       0   male\n\nlm(wageinc ~ .,data=svc) \n\n\nCall:\nlm(formula = wageinc ~ ., data = svc)\n\nCoefficients:\n(Intercept)          age      wkswrkd   gendermale  \n   -29384.1        496.7       1372.8      10700.8\nSo, we estimate that, other factors being equal, men about paid close to $11,000 more than women. This is a complex issue, but for our purposes here, how did gender become gendermale, no explicit mention of women?\nLet’s try to force the issue:\nsvc$man &lt;- as.numeric(svc$gender == 'male')\nsvc$woman &lt;- as.numeric(svc$gender == 'female')\nsvc$gender &lt;- NULL\nhead(svc)\n\n       age wageinc wkswrkd man woman\n1 50.30082   75000      52   0     1\n2 41.10139   12300      20   1     0\n3 24.67374   15400      52   0     1\n4 50.19951       0      52   1     0\n5 51.18112     160       1   0     1\n6 57.70413       0       0   1     0\n\nlm(wageinc ~ .,data=svc)\n\n\nCall:\nlm(formula = wageinc ~ ., data = svc)\n\nCoefficients:\n(Intercept)          age      wkswrkd          man        woman  \n   -29384.1        496.7       1372.8      10700.8           NA\nWell, we couldn’t force the issue after all. Why not? We hinted above that \\(A' A\\) may not be invertible. Let’s take a look.\nA &lt;- cbind(1,svc[,-2])\nA &lt;- as.matrix(A)\nApA &lt;- t(A) %*% A\nApA\n\n               1        age  wkswrkd      man    woman\n1        20090.0   794580.7   907240  15182.0   4908.0\nage     794580.7 33956543.6 35869770 600860.8 193719.9\nwkswrkd 907240.0 35869770.5 45252608 692076.0 215164.0\nman      15182.0   600860.8   692076  15182.0      0.0\nwoman     4908.0   193719.9   215164      0.0   4908.0\nIs this matrix invertible? Let’s apply the elementary row operations introduced in Section 3.5.1:\nlibrary(pracma)\nrref(ApA) \n\n        1 age wkswrkd man woman\n1       1   0       0   0     1\nage     0   1       0   0     0\nwkswrkd 0   0       1   0     0\nman     0   0       0   1    -1\nwoman   0   0       0   0     0\nAha! Look at that row of 0s! The row operations process ended prematurely. This matrix will be seen to have no inverse. We say that the matrix has rank 4 – meaning 4 nonzero rows – when it needs to be 5. We also say that the matrix is of nonfull rank.\nThough we have motivated the concept of matrix rank here with a linear model example, and will do so below, the notion pervades all of linear algebra, as will be seen in the succeeding chapters.\nWe still have not formally defined rank, just building intuition, but toward that end, let us first formalize the row operations process.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Matrix Rank</span>"
    ]
  },
  {
    "objectID": "Ch4.html#reduced-row-echelon-form-rref-of-a-matrix",
    "href": "Ch4.html#reduced-row-echelon-form-rref-of-a-matrix",
    "title": "6  Matrix Rank",
    "section": "6.2 Reduced Row Echelon Form (RREF) of a Matrix",
    "text": "6.2 Reduced Row Echelon Form (RREF) of a Matrix\nWe formalize and extend Section 3.5.1.\n\n6.2.1 Elementary row operations\nThese are:\n\nMultiply a row by a nonzero constant.\nAdd a multiple of one row to another.\nSwap two rows.\n\nAgain, each operation can be implemented via pre-multiplying the given matrix by a corresponding elementary matrix. The latter is the result of applying the given operation to the identity matrix \\(I\\). For example, here is the matrix corresponding to swapping rows 2 and 3:\n\\[\n\\left ( \\begin{array}{rrr}\n1 & 0 & 0 \\\\\n0 & 0 & 1 \\\\\n0 & 1 & 0 \\\\\n\\end{array} \\right )\n\\]\nFor example:\n\ne &lt;- rbind(c(1,0,0),c(0,0,1),c(0,1,0))\ne\n\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    0    0    1\n[3,]    0    1    0\n\na &lt;- matrix(runif(12),nrow=3)\na\n\n          [,1]        [,2]      [,3]       [,4]\n[1,] 0.3311482 0.476398621 0.6677385 0.06998726\n[2,] 0.5441880 0.544119692 0.5891712 0.02173528\n[3,] 0.5404404 0.009794929 0.9054202 0.62113846\n\ne %*% a  # matrix mult. is NOT e * a!\n\n          [,1]        [,2]      [,3]       [,4]\n[1,] 0.3311482 0.476398621 0.6677385 0.06998726\n[2,] 0.5404404 0.009794929 0.9054202 0.62113846\n[3,] 0.5441880 0.544119692 0.5891712 0.02173528\n\n\nYes, rows 2 and 3 were swapped.\n\n\n6.2.2 The RREF\nBy applying these operations to a matrix \\(A\\), we can compute its reduced row echelon form \\(A_{rref}\\), which is defined by the following properties:\n\nEach row, if any, that consists of all 0s is at the bottom of the matrix.\nThe first nonzero entry in a row, called the pivot, is a 1.\nEach pivot will be to the right of the pivots in the rows above it.\nEach pivot is the only nonzero entry in its column.\n\nThe reader should verify that the matrix at the end of Section 6.1 has these properties.\n\n\n6.2.3 Recap of Section 3.5.1\n\nEach row operation can be performed via premultiplication by an elementary matrix. Each such matrix is invertible, and its inverse is an elementary matrix.\nThus\n\\[\nB_{rref} = E_k ... E_2 E_1 B\n\\tag{6.1}\\]\nfor a sequence of invertible elementary matrices.\nAnd\n\\(B = (E_1)^{-1} (E_2)^{-1} ... (E_k)^{-1} B_{rref}\\),\nwhere each \\((E_i)^{-1}\\) is itself an elementary row operation.\n\n\n\n6.2.4 Partitioned-matrix view of Reduced Echelon Forms\nIt is much easier to gain insight from RREF by viewing it in partitioned-matrix form:\n\nSay \\(A\\) is of size \\(m \\times n\\). Then \\(A_{rref}\\) has the form\n\\[\n\\left (\n\\begin{array}{rr}\nI_{s} & U \\\\\n0 & 0  \\\\\n\\end{array}\n\\right )\n\\tag{6.2}\\] where \\(I_s\\) is an identity matrix of some size \\(s\\) and \\(U\\) has dimension \\(s \\times (n-s)\\).\n\nThe Column-Reduced Echelon Form is similar:\n\\[\nA_{cref} =\n\\left (\n\\begin{array}{rr}\nI_{t} & 0 \\\\\nV & 0  \\\\\n\\end{array}\n\\right )\n\\tag{6.3}\\]\nwhere \\(I_t\\) is an identity matrix of some size \\(t\\) and \\(V\\) has dimension \\((m-t) \\times t\\).\nClearly, the row rank of \\(A_{rref}\\) is \\(s\\), and its column rank is \\(t\\). We will prove shortly that the \\(s\\) is the column rank of \\(A\\) as well.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Matrix Rank</span>"
    ]
  },
  {
    "objectID": "Ch4.html#formal-definitions",
    "href": "Ch4.html#formal-definitions",
    "title": "6  Matrix Rank",
    "section": "6.3 Formal Definitions",
    "text": "6.3 Formal Definitions\n\nDefinition: A linear combination of vectors \\(v_1,v_2,...v_k\\) is a sum of scalar multiples of the vectors, i.e.\n\\[\na_1 v_1 + ... + a_k v_k\n\\]\nwhere the \\(a_i\\) are scalars.\n\n(The reader may wish to review Section 2.3.)\n\nDefinition: We say a set of vectors is linearly dependent if some linear combination of them (excluding the trivial case in which all the coefficients are 0) equals 0. If no nontrival linear combination of the vectors is 0, we say the vectors are linearly independent.\n\nFor instance, consider the matrix\n\\[\n\\left ( \\begin{array}{rrr}\n1 & 3 & 1 \\\\\n1 & 9 & 4 \\\\\n0 & 8 & 0 \\\\\n\\end{array} \\right )\n\\]\nDenote its columns by \\(c_1\\), \\(c_2\\) and \\(c_3\\). Observe that\n\\[\n(-1) c_1 + (1) c_2 + (-2) c_3 =\n\\left (\n\\begin{array}{r}\n0 \\\\\n0 \\\\\n0 \\\\\n\\end{array}\n\\right )\n\\]\nThen \\(c_1\\), \\(c_2\\) and \\(c_3\\) are not linearly independent.\nSo, here is the formal definition of rank:\n\nDefinition The rank of a matrix \\(B\\) is its maximal number of linearly independent rows.\n\nAs an example involving real data, recall that in the nonfull rank let’s look again at the census example,\n\nhead(A)\n\n  1      age wkswrkd man woman\n1 1 50.30082      52   0     1\n2 1 41.10139      20   1     0\n3 1 24.67374      52   0     1\n4 1 50.19951      52   1     0\n5 1 51.18112       1   0     1\n6 1 57.70413       0   1     0\n\n\nthe sum of the last two columns of \\(A\\) is equal to the first column, so\ncolumn1 - column4 - column5 = 0\nWrite this more fully as an explicit linear combination of the columns of this matrix:\n1 column1 + 0 column2 + 0 column3 + (-1) column4 + (-1) column5 = 0\nSo we have found a linear combination of the columsn of this matrix, with coefficients (1,0,0,-1,-1), that evaluates to 0. Though we have defined rank in terms of rows, one can do so in terms of columns as well:",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Matrix Rank</span>"
    ]
  },
  {
    "objectID": "Ch4.html#row-and-column-rank",
    "href": "Ch4.html#row-and-column-rank",
    "title": "6  Matrix Rank",
    "section": "6.4 Row and Column Rank",
    "text": "6.4 Row and Column Rank\nWe have defined the rank of a matrix to be the number of maximally linearly independent rows. We’ll now call that the row rank, and define the column rank to be the number of maximally linearly independent columns. So for instance the matrix analyzed at the end of the last section is seen to be of nonfull column rank.\nAs will be seen below, the row and column ranks will turn out to be equal. Thus in the sequel, we will use the term rank to refer to their common value. For now, though, the unmodified term “rank” will mean row rank.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Matrix Rank</span>"
    ]
  },
  {
    "objectID": "Ch4.html#sec-rankprops",
    "href": "Ch4.html#sec-rankprops",
    "title": "6  Matrix Rank",
    "section": "6.5 Some Properties of Ranks",
    "text": "6.5 Some Properties of Ranks\nIn Section 6.1, we found the matrix \\(A'A\\) to be noninvertible, as its RREF had a row of 0s. We mentioned a relation to rank, but didn’t formalize it, which we will do now.\n\nTheorem 6.1 Let \\(A\\) be any matrix, and let \\(V\\) and \\(W\\) be square, invertible matrices with sizes conformable with the products below. Then the column rank of \\(VA\\) is equal to that of \\(A\\), and the row rank of \\(AW\\) is equal to that of \\(A\\).\n\n\nProof. Say \\(A\\) has column rank \\(r\\). Then there exist \\(r\\) (and no more than \\(r\\)) linearly independent columns of \\(A\\). For convenience of notation, say these are the first \\(r\\) columns, and call them \\(c_1,...,c_r\\). Then the first \\(r\\) columns of \\(VA\\) are \\(Vc_1,...,Vc_r\\), by matrix partitioning.\nNote that for a vector \\(x\\), \\(Vx = 0\\) if and only if \\(x = 0\\). (Just multiply \\(Vx = 0\\) on the left by \\(V^{-1}\\).) So a linear combination \\(\\lambda_1 c_1+...+\\lambda_r c_r\\) is 0 if and only if the corresponding linear combination \\(\\lambda_1 Vc_1+...+\\lambda_r Vc_r\\) is 0. Thus the vectors \\(Vc_i\\) are linearly independent, and \\(VA\\) has column rank \\(r\\).\nThe argument for the case of \\(AW\\) is identical, this time involving rows of \\(A\\).\n\\(\\square\\)\n\n\nTheorem 6.2 The column rank of a matrix \\(B\\) is equal to the column rank of its RREF, \\(B_{rref}\\), which in turn is \\(s\\) in Equation 6.2.\n\n\nProof. The above Theorem 6.1 says that pre-postmultiplying \\(B\\) will not change its column rank. Then invoke Equation 6.1.\nIt is clear that any column in \\(U\\) in Equation 6.2 can be written as a linear combination of the columns of \\(I_s\\). Thus the column rank of \\(B\\) is \\(s\\).\n\\(\\square\\)\n\n\nLemma 6.1 An elementary row operation on a matrix leaves the row rank unchanged.\n\n\nProof. Let \\(r_i, i=1,...,m\\) denote the rows of the matrix. Consider a linear combination\n\\[\na_1 r_1 + ... + a_m r_m \\neq 0\n\\]\nFor any of the three elementary operations, a slightly modified set of the \\(a_i\\) works (i.e. will be nonzero), using the modified elementary matrix:\n\nSwap rows \\(i\\) and \\(j\\): Swap \\(a_i\\) and \\(a_j\\).\nMultiplying row \\(i\\) by a constant \\(c\\). Since \\(r_i \\rightarrow c r_i\\), set \\(a_i \\rightarrow (1/c) a_i\\).\nAdd c times row \\(j\\) to row \\(i\\): Set \\(a_j \\rightarrow a_j - c a_j\\).\n\n\\(\\square\\)\n\n\nTheorem 6.3 (The row rank and column rank of a matrix are equal.)  \n\n\nProof. The lemma shows that every nonzero linear combination of the rows of \\(A\\) corresponds to one for the rows of \\(A_{rref}\\), and vice versa. Thus the row rank of \\(A\\) is the same as that of \\(A_{rref}\\). But that is \\(s\\) in Equation 6.2, which we found earlier was equal to the column rank of \\(A\\).\n\\(\\square\\)\n\nIn Section 6.1, we found the matrix \\(A'A\\) to not be of full rank. It is surprising that so was \\(A\\):\n\nqr(ApA)$rank  # qr is a built-in R function\n\n[1] 4\n\nqr(A)$rank\n\n[1] 4\n\n\nThis is rather startling. \\(A\\) has over 20,000 rows — yet only 4 linearly independent ones? But it follows from this fact:There are many subsets of 4 rows that are linearly independent. But no sets of 5 or more are linearly independent.\n\nTheorem 6.4 The (common value of) row and column ranks of an \\(m \\times n\\) matrix \\(B\\) must be less than or equal to the minimum of the number of rows and columns of \\(B\\) .\n\n\nProof. The row rank of \\(b\\) equals its column rank. The former is bounded above by \\(m\\) while the latter’s bound is \\(n\\).\n\\(\\square\\)\n\n\nTheorem 6.5 The matrix \\(A'A\\) has the same rank as \\(A\\).\n\n\nProof. Using the column analog of Equation 6.1, we can write\n\\[\nA_{cref} = AF\n\\]\nwhere \\(F\\) is an invertible product of matrices for elementary column operations. Then\n\\[\n(A_{rref})' A_{rref}) = F'A'AF\n\\]\nTheorem 6.1 tells us that the rank of \\(F'A'A\\) has the same rank as \\(A'A\\), and \\(F'A'AF\\) has the same rank as \\(F'A'A\\), in turn the same rank as \\(A'A\\). So we can concentrate on \\((A_{rref})' A_{rref})\\)\nUsing Equation 6.2, we have\n\\[\n(A_{rref})' A_{rref})  =\n\\left (\n\\begin{array}{rr}\nI_s & U \\\\\nU' & U'U  \\\\\n\\end{array}\n\\right )\n\\]\nThe presence of \\(I_s\\) tells us there are at least \\(s\\) linearly independent rows, thus rank at least \\(s\\). So the rank of \\(A'A\\) is at least that of \\(A\\).\nOn the other hand, again pply our knowlege of partitioning.\n\nSay \\(A\\) is \\(m \\times n\\). Write\n\\[\nA'A =\n\\left (\n\\begin{array}{r}\nc_1 \\\\\n... \\\\\nc_n \\\\\n\\end{array}\n\\right )\nA =\n\\left (\n\\begin{array}{r}\nc_1 A \\\\\n... \\\\\nc_n A \\\\\n\\end{array}\n\\right )\n\\]\nwhere the \\(c_i\\) are the columns of \\(A\\), thus the rows of \\(A'\\).\nThus each row \\(c_i A\\) of \\(A'A\\) is a linear combination of the rows of \\(A\\).\nRecall that the rank of \\(A'A\\) is its maximal number of linearly independent rows. We saw above that each row of \\(A'A\\) is a linear combination of the rows of \\(A\\). Thus in turn, each linear combination of the rows of \\(A'A\\) is a linear combination of linear combinations of rows of \\(A\\) – still a linear combination of the rows of \\(A\\)!\nAt most \\(s\\) members of that latter linear combination are linearly independent. In other words, the rank of \\(A'A\\) is at most \\(s\\).\nThus the rank of \\(A'A\\) is also \\(s\\).\n\n\\(\\square\\)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Matrix Rank</span>"
    ]
  },
  {
    "objectID": "Ch4.html#your-turn",
    "href": "Ch4.html#your-turn",
    "title": "6  Matrix Rank",
    "section": "6.6 Your Turn",
    "text": "6.6 Your Turn\n❄️ Your Turn: If the matrix \\(A\\) has a 0 row, then \\(det(A) =  0\\). Explain why.\n❄️ Your Turn: Consider the three basic elementary matrices discussed here: Swap rows \\(i\\) and \\(j\\); multiply row \\(i\\) by a constant \\(b\\); adding \\(c\\) times row \\(i\\) to row \\(j\\), for a constant \\(c\\). Find general formulas for the determinants of the three matrices.\n❄️ Your Turn: Show that a set of vectors is linearly dependent if and only if one of the vectors is a linear combination of the others.\n❄️ Your Turn: Consider an \\(m \\times n\\) matrix \\(A\\) with \\(m \\geq n\\). Then consider the partitioned matrix\n\\[\nB =\n\\left (\n\\begin{array}{r}\nA \\\\\nI \\\\\n\\end{array}\n\\right )\n\\]\nwhere \\(I\\) is the \\(n \\times n\\) identity matrix. Prove that \\(B\\) is of full rank.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Matrix Rank</span>"
    ]
  },
  {
    "objectID": "Ch5a.html",
    "href": "Ch5a.html",
    "title": "7  Vector Spaces",
    "section": "",
    "text": "7.1 Review of a Matrix Rank Property\n\\[\nx^2  \n\\]\nIn the last chapter, we presented the concepts of matrix row and column rank, defined to be the maximal number of linearly independent combinations of the rows or columns of the matrix, respectively. We proved that\nWe can say something stronger:\nSo, not only do the two matrices have the same maximal numbers of linearly independent rows, they also generate the same linear combinations of those rows.\nThe sets \\(\\cal V\\) and \\(\\cal V_{rref}\\) are called the row spaces of the two matrices, and yes, they are examples of vector spaces, as we will now see.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Vector Spaces</span>"
    ]
  },
  {
    "objectID": "Ch5a.html#review-of-a-matrix-rank-property",
    "href": "Ch5a.html#review-of-a-matrix-rank-property",
    "title": "7  Vector Spaces",
    "section": "",
    "text": "For any matrix \\(B\\),\n\\[\n\\textrm{rowrank}(B)\n= \\textrm{colrank}(B)\n= \\textrm{rowrank}(B_{rref})\n= \\textrm{colrank}(B_{rref})\n\\]\n\n\n\nTheorem 7.1 Let \\(\\cal V\\) denote the span of the rows of \\(B\\), i.e. the set of all possible linear combinations of rows of \\(B\\). Define \\(\\cal\nV_{rref}\\) similarly. Then\n\\[\n\\cal V = \\cal V_{rref}\n\\]\nThe analogous result holds for columns.\n\n\nProof. Actually, the theorem follows immediately from our comment following Lemma 6.1: “every nonzero linear combination of the rows of \\(A\\) corresponds to one for the rows of \\(A_{rref}\\), and vice versa.” This showed a one-to-one correspondence between the two sets of linear combinations.\n\\(\\square\\)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Vector Spaces</span>"
    ]
  },
  {
    "objectID": "Ch5a.html#vector-space-definition",
    "href": "Ch5a.html#vector-space-definition",
    "title": "7  Vector Spaces",
    "section": "7.2 Vector Space Definition",
    "text": "7.2 Vector Space Definition\n\nA set of objects \\(\\cal W\\) is called a vector space if it satisfies the following conditions:\n\nSome form of addition between vectors \\(u\\) and \\(v\\), denoted \\(u+v\\), is defined in \\(\\cal W\\), with the result that u+v is also in \\(\\cal W\\). We describe that latter property by saying \\(\\cal W\\) is closed under addition.\nThere is a unique element called “0” such that \\(u +\n0 = 0 + u = u\\).\nSome form of scalar multiplication is defined, so that for any number \\(c\\) and any \\(u\\) in \\(\\cal W\\), \\(cw\\) exists and is in \\(\\cal W\\). We describe that latter property by saying \\(\\cal W\\) is closed under scalar multiplication\nThis being a practical book with just a dash of theory, we’ll skip the remaining conditions, involving algebraic properties such as commutativity of addition (\\(u+v = v+u\\)).\n\n\n\n7.2.1 Examples\n\n\n7.2.2 \\(\\cal R^{n}\\)\nIn the vast majority of examples in this book, our vector space will be \\(\\cal R^{n}\\).\nHere \\(\\cal R\\) represents the set of all real numbers, and \\(\\cal R^{n}\\) is simply the set of all vectors consisiting of \\(n\\) real numbers. In an \\(m\n\\times k\\) matrix the rows are members of \\(\\cal R^{m}\\) and the columns are in \\(\\cal R^{k}\\).\n\n\n7.2.3 The set C(0,1) of all continuous functions on the interval [0,1]\nVector addition and scalar multiplication are done as functions. If say \\(u\\) is the squaring function and \\(v\\) is the sine function, then for example \\(3u\\) is the function\n\\[\nf(x) = 3x^{0.5}\n\\]\nand \\(u+v\\) is defined to be\n\\[\nf(x) = x^{0.5} + \\sin(x)\n\\]\n\n\n7.2.4 A set \\(\\cal RV(\\Omega)\\) of random variables\nThis vector space will consist of all random variables \\(X\\) defined on some probability space \\(\\Omega\\), with the additional restriction that \\(E(X^2) &lt; \\infty\\), i.e. \\(X\\) has finite variance.\nConsider the example in {Section 5.3.5} on major league baseball players. We choose a player at random. Denote weight, height and age by \\(W\\), \\(H\\) and \\(A\\).\nVector addition and scalar multiplication are defined in a straightforward manner. For instance, the sum of \\(H\\) and \\(A\\) is simply height + age. This may seem like a rather nonsensical sum, but it fits the technical definition, and moreover, we have already been doing things like this! This after all is what is happening in our prediction expression from that section,\n\\[\n\\textrm{predicted weight} =\n-187.6382 + 4.9236 H+ 0.9115 A\n\\]\nIn fact, in this vector space, the above is a linear combination of the random variables \\(1\\), \\(H\\) and \\(A\\). Note that random variables such as \\(HW^{1.2}\\) and so on are also members of this vector space, essentially any function of \\(W\\), \\(H\\) and \\(A\\).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Vector Spaces</span>"
    ]
  },
  {
    "objectID": "Ch5a.html#subspaces",
    "href": "Ch5a.html#subspaces",
    "title": "7  Vector Spaces",
    "section": "7.3 Subspaces",
    "text": "7.3 Subspaces\nSay \\(\\cal W_{1}\\) a subset of a vector space \\(\\cal W\\), such that \\(\\cal\nW_{1}\\) is closed under addition and scalar multiplication. \\(\\cal W_{1}\\) is called a subspace of \\(\\cal W\\). Note that a subspace is also a vector space in its own right.\n\n7.3.1 Examples\n\\(R^3\\) and \\(R^n\\):\nFor instance, take \\(\\cal W_{1}\\) to be all vectors of the form (a,b,0). Clearly, \\(\\cal W_{1}\\) is closed under addition and scalar multiplication, so it is subspace of \\(R^3\\).\nAnother subspace of \\(R^3\\) is the set of vectors (a,a,b), i.e. those vectors whose first two element are equal. What about vectors of the form (a,b,a+b)? Yes.\nWe saw earlier that the row space of an \\(m \\times n\\) matrix \\(A\\), consisting of all linear combinations of the rows of \\(A\\), is a subspace of \\(R^m\\). Similarly, the column space, consisting of all linear combinations of columns of \\(A\\), is a subspace of \\(R^n\\).\nAnother important subspace is the null space, the set of all \\(x\\) such that \\(Ax = 0\\). The reader should verify that this is indeed a subspace of \\(R^n\\).\n\\(C(0,1)\\):\nOne subspace is the set of all polynomial functions. Again, the sum of two polynomials is a polynomial, and the same holds for scalar multiplication, so the set of polynomials is closed under those operations, and is a subspace.\n\\(\\cal RV(\\Omega)\\):\nThe set of all random variables that are functions of \\(H\\), say, is a subspace. Another subspace is the set of all random variable with mean 0.\n\n\n7.3.2 Span of a set of vectors\nAs noted earlier, the span of a set of vectors \\(G = {v_1,...,v_k}\\) is the set of all linear combinations of those vectors. It’s a subspace of the main space.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Vector Spaces</span>"
    ]
  },
  {
    "objectID": "Ch5a.html#basis",
    "href": "Ch5a.html#basis",
    "title": "7  Vector Spaces",
    "section": "7.4 Basis",
    "text": "7.4 Basis\nConsider a set of vectors \\(u_1,...u_r\\) in a vector space \\(\\cal W\\). Recall that the span of these vectors is defined to be the set of all linear combinations of them. In verb form, we say that \\(u_1,...u_r\\) spans \\(\\cal W\\) if we can generate the entire vector space from those vectors via linear combinations. It’s even nicer if the vectors are linearly independent:\n\nWe say the vectors \\(u_1,...u_r\\) in a vector space \\(\\cal W\\) form a basis for \\(\\cal W\\) if they are linearly independent and span \\(\\cal W\\).\n\nNote that subspaces are vector spaces in their own right, and thus also have bases.\n\n7.4.1 Examples\n\\(\\cal R^3\\):\nThe vectors (1,0,0), (0,1,0) and (0,0,1) are easily seen to be a basis here. They are linearly independent, and clearly span \\(\\cal R^3\\). For instance, to generate (3,1,-0.2), we use this linear combination:Our convention has been that vectors are considered in matrix terms as column vectors by default. However, in nonmatrix contexts, it will be convenient to write \\(\\cal R^n\\) vectors as rows.\n(3,1,-0.2) = 3 (1,0,0) + 1 (0,1,0) + (-0.2) (0,0,,1)\nBut bases are not unique; for instance, the set (1,0,0, (0,1,0), (0,1,1) works equally well as a basis for this space, as do (infinitely) many others..\nA basis for the subspace of vectors of the form (a,a,b) is (1,1,0) and (0,0,1).\n\\(C(0,1)\\):\nAlas, there is no finite basis here. Even infinite ones have issues in their mathematical formulation.For those with a background in mathematical analysis, here is how the problem is handled. One starts with a subspace that is dense in the full space, i.e. any vector in the full space can be approximated to any precision by some linear combination of vectors in the dense set. For instance, the famoua Stone-Weierstrauss Theorem says that the set of all polynomials is dense in \\(C(0,1)\\). One then defines the dense set to be a “basis.” The use of trig functions as the dense set is much more common than use of polynomials, in the familiar Fourier series.\n\\(\\cal RV(\\Omega)\\):\nThe situation here is the same as for C(0,1).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Vector Spaces</span>"
    ]
  },
  {
    "objectID": "Ch5a.html#dimension",
    "href": "Ch5a.html#dimension",
    "title": "7  Vector Spaces",
    "section": "7.5 Dimension",
    "text": "7.5 Dimension\nGeometrically, we often refer to what is called \\(\\cal R^3\\) here as ``3-dimensional.’’ We extend this to general vector spaces as follows:\n\nThe dimension of a vector space is the number of vectors in any of its bases.\n\nThere is a bit of a landmine in that definition, as it presumes that all the bases do consist of the same number of vectors. This is true, but must be proven. We will not do so here, and refer the interested reader to the elegant proof AF Beardon (Algebra and Geometry, 2005, Cambridge).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Vector Spaces</span>"
    ]
  },
  {
    "objectID": "Ch5a.html#linear-transformations",
    "href": "Ch5a.html#linear-transformations",
    "title": "7  Vector Spaces",
    "section": "7.6 Linear Transformations",
    "text": "7.6 Linear Transformations\nConsider vector spaces \\(\\cal{V}_1\\) and \\(\\cal{V}_2\\). A function \\(f\\) whose input is in \\(\\cal{V}_1\\) and output is in \\(\\cal{V}_2\\) is called a transformation (or map) \\(\\cal{V}_1\\) to \\(\\cal{V}_2\\).\nAn important special case is that in which \\(f\\) is linear, i.e.\n\\[\nf(av + bw) = a f(v) = b f(w)\n\\]\nfor all scalars \\(a\\) and \\(b\\), and all vectors \\(v\\) and \\(w\\) in $_1 $.\nA further important special case is that in which \\(\\cal{V}_1\\) and \\(\\cal{V}_2\\) are \\(R^n\\) and \\(R^m\\), respectively. Then it can be shown that there a some \\(m \\times n\\) matrix \\(A\\) that does its work, i.e.\n\\[\nf(x) = Ax\n\\]\nfor all \\(x\\) in \\(R^n\\).\nNote that it is not necessarily true that a linear transformation will be one-to-one (injective), meaning that \\(x \\neq y\\) implies \\(f(x) \\neq f(y)\\). Nor is it necessarily onto (surjective).\nTo see that the first is false, consider the matrix \\(A\\) above. If say \\(f(v) = w\\) then we will also have \\(f(v+s) = f(v) + f(s) = w\\) for any \\(s\\) in the null space of \\(A\\). The reader is encouraged to verify that the second property also does not necessarily hold either.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Vector Spaces</span>"
    ]
  },
  {
    "objectID": "Ch5a.html#your-turn",
    "href": "Ch5a.html#your-turn",
    "title": "7  Vector Spaces",
    "section": "7.7 Your Turn",
    "text": "7.7 Your Turn\n❄️ Your Turn: In the vector space \\(C(0,1)\\), consider the subset of all polynomial functions of degree \\(k\\) or less. Explain why this is a subspace of \\(C(0,1)\\). What is its dimension? Give an example of a basis for this subspace, for \\(k = 3\\).\n❄️ Your Turn: Say \\(U\\) and \\(V\\) are subspaces of \\(W\\). Explain why \\(U\n\\cap V\\) is also a subspace, and that its dimension is at most the minimum of the dimensions of \\(U\\) and \\(V\\). Show by counterexample that the result does not hold for union.\n❄️ Your Turn: Citing the properties of expected value, E(), show that the set of all random variable with mean 0 is a subspace of \\(\\cal RV(\\Omega)\\).\n❄️ Your Turn: Prove that the coefficients in a basis representation are unique. In other words, in a representation of the vector \\(x\\) in terms of a basis \\(u_1,...,u_n\\), there cannot be two different sets of coefficients \\(c_1,...,c_n\\) such that \\(c_1 u_1 + ... + c_n u_n = x\\).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Vector Spaces</span>"
    ]
  },
  {
    "objectID": "Ch5b.html",
    "href": "Ch5b.html",
    "title": "8  Inner Product Spaces",
    "section": "",
    "text": "8.1 Geometric Aspirations\nYou may recall from your high school geometry course the key concept of perpendicularity, represented by the ⊥ symbol. You may also recall that in 2-dimensional space, given a point P and a line L, the line drawn from point P to the closest point P’ within L is perpendicular to L. The same is true if L is a plane. The point P’ is called the projection of P onto L.\nThis was shown in this book’s cover, shown here:\nThe point at the end of the green vector is projected onto the mustard-colored plane, producing the red vector. It in turn is projected onto the blue line. There are right angles in each case.\nThe early developers of linear algebra wanted to extend such concepts to abstract vector spaces. This aids intuition, and has very powerful applications.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Inner Product Spaces</span>"
    ]
  },
  {
    "objectID": "Ch5b.html#geometric-aspirations",
    "href": "Ch5b.html#geometric-aspirations",
    "title": "8  Inner Product Spaces",
    "section": "",
    "text": "Projections",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Inner Product Spaces</span>"
    ]
  },
  {
    "objectID": "Ch5b.html#definition",
    "href": "Ch5b.html#definition",
    "title": "8  Inner Product Spaces",
    "section": "8.2 Definition",
    "text": "8.2 Definition\nYou may have seen dot products in a course on vector calculus or physics. For instance, the dot product of the vectors (3,1,1.5)’ and (0,5,6)’ is\n3x0 + 1x5 + 1.5x6 = 14\nThis in fact is a standard inner product on \\(\\cal R^3\\), but the general definition is as follows.\n\nAn inner product on a vector space \\(\\cal V\\), denoted by the “angle brackets” notation \\(&lt;u,v&gt;\\), is a function with two vectors as arguments and a numerical output, with the following properties:\n\n\\(&lt;u,v&gt; = &lt;v,u&gt;\\)\nThe function is bilinear:\n\\[\n&lt;u,av+bw&gt; = a &lt;u,v&gt; + b &lt;u,w&gt;\n\\]\n\\(&lt;u,u&gt; \\geq 0\\), with equality if and only if \\(u = 0\\).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Inner Product Spaces</span>"
    ]
  },
  {
    "objectID": "Ch5b.html#examples",
    "href": "Ch5b.html#examples",
    "title": "8  Inner Product Spaces",
    "section": "8.3 Examples",
    "text": "8.3 Examples\n\\(\\cal R^n\\):\nAs noted, ordinary dot product is the most common inner product on this space.\n\\(&lt;(a_1,...,a_n),(b_1,...,b_n&gt; =\na_1 b_1 + ... + a_n b_n\\)\nC(0,1):\nOne inner product on this space is\n\\[\n&lt;f,g&gt; = \\int_{0}^{1} f(t) g(t) ~ dt\n\\]\nFor instance, with \\(f(t) = t^2\\) and \\(g(t) = \\sin(t)\\), the inner product can be computed with R:\n\nf &lt;- function(t) t^2\ng &lt;- function(t) sin(t)\nfg &lt;- function(t) f(t) * g(t)\nintegrate(fg,0,1)\n\n0.2232443 with absolute error &lt; 2.5e-15\n\n\nThis clearly fits most requirements for inner products, but what about \\(&lt;f,f&gt; = 0\\) only if \\(f = 0\\)? A non-0 \\(f\\) will have \\(f^2(t) &gt; 0\\) for at least one \\(t\\), and by continuity, \\(f^2(t) &gt; 0\\) on an interval containing that \\(t\\), thus making a nonzero contribution to the integral and thus to the inner product.Note that the 0 vector in this space is the function that is identically 0, not just 0 at some points\n\\(\\cal RV(\\Omega)\\):\nWe will take covariance as our inner project:\n\\[\n&lt;U,V&gt; = cov(U,V) = E[(U - EU) (V - EV)]\n\\]\nThe properties of expected value, e.g. linearity, show that the requirements for an inner product hold.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Inner Product Spaces</span>"
    ]
  },
  {
    "objectID": "Ch5b.html#norm-of-a-vector",
    "href": "Ch5b.html#norm-of-a-vector",
    "title": "8  Inner Product Spaces",
    "section": "8.4 Norm of a Vector",
    "text": "8.4 Norm of a Vector\nThis concept extends the notion of a the length of a vector, as we know it in \\(\\cal R^2\\) and \\(\\cal R^3\\).\nDefinition:\n\nThe norm of a vector \\(x\\), denoted \\(||x||\\), is\n\\[\n(&lt;x,x&gt;)^{0.5}\n\\]\nThe distance from a vector \\(x\\) to a vector \\(y\\) is\n\\[\n||y - x||\n\\]",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Inner Product Spaces</span>"
    ]
  },
  {
    "objectID": "Ch5b.html#the-cauchy-schwarz-inequality",
    "href": "Ch5b.html#the-cauchy-schwarz-inequality",
    "title": "8  Inner Product Spaces",
    "section": "8.5 The Cauchy-Schwarz Inequality",
    "text": "8.5 The Cauchy-Schwarz Inequality\n\nTheorem 8.1 (Cauchy-Schwarz Inequality) Say \\(u\\) and \\(v\\) are vectors in an inner product space. Then\n\\[\n|&lt;u,v&gt;| \\leq ||u|| ~ ||v||\n\\]\n\n\nProof. See the Your Turn problem below.\n\n\n8.5.1 Application: Correlation\nCorrelation coefficients are ubiquitous in data science. It is well known that their values fall into the interval [-1,1]. Let’s prove that.\nRecall from Chapter 4 the notion of the covariance between two random variables (from which the covariance matrix of a random vector is formed). We remarked that covariance is intuitively like correlation,but that latter is a scaled form. Formally,\n\\[\n\\rho(X,Y) =\n\\frac{E[(X-EX)(Y-EY)]}{\\sqrt{Var(X)} \\sqrt{Var(Y)}}\n\\]\nBy dividing the covariance by the product of the standard deviations, we obtain a unitless quantity, i.e. free of units such as centimeters and degrees.\nNow, \\(X\\) and \\(Y\\) are in \\(\\cal RV(\\Omega)\\). To simplify the algebra, consider the case \\(EX = EY = 0\\).Actually, we should say, ``Make the transformation \\(X \\rightarrow X - EX\\), and note that it leaves both sides of the above correlation formula unchanged. We can thus assume \\(EX = EY = 0\\).’’ This is a common strategy, and should be kept in mind.\nRecalling our inner product for this space, we have\n\\[\n&lt;X,Y&gt; = E(XY)\n\\]\nand\n\\[\n||X||^2 = &lt;X,X&gt; = E(X^2) = Var(X)\n\\]\nwith the analogous relations for \\(Y\\).\nCauchy=Schwarz then says\n\\[\n|E(XY)| \\leq \\sqrt{Var(X)} \\sqrt{Var(Y)}\n\\]\nwhich says the correlation is between -1 and 1 inclusive.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Inner Product Spaces</span>"
    ]
  },
  {
    "objectID": "Ch5b.html#the-triangle-inequality",
    "href": "Ch5b.html#the-triangle-inequality",
    "title": "8  Inner Product Spaces",
    "section": "8.6 The Triangle Inequality",
    "text": "8.6 The Triangle Inequality\nIn the world of ordinary physical geometry, we know the following\nThe distance from A to B is less than or equal to the sum of the distances from A to C and C to B. This is true as well in general, abstract inner product spaces.\n\nTheorem 8.2 (Triangle Inequality) In a general inner product space,\n\\[\n||x - z|| \\leq ||x - y|| + ||y - z||\n\\]\n\n\nProof. See the Your Turn problem below.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Inner Product Spaces</span>"
    ]
  },
  {
    "objectID": "Ch5b.html#sec-projections",
    "href": "Ch5b.html#sec-projections",
    "title": "8  Inner Product Spaces",
    "section": "8.7 Projections",
    "text": "8.7 Projections\nAs mentioned, the extension of classical geometry to abstract vector spaces has powerful applications. There is no better example of this than the idea of projections.\n\n8.7.1 Theorem\nWe say that vectors \\(u\\) and \\(v\\) are orthogonal if \\(&lt;u,v&gt; = 0\\). This is the general extension of the notion of perpendicularity in high school geometry. Then we have the following:\n\nTheorem 8.3 (Projection Theorem) Consider an inner product space \\(\\cal V\\), with subspace \\(\\cal W\\). Then for any vector \\(x\\) in \\(\\cal V\\), there is a unique vector \\(z\\) in \\(\\cal\nW\\), such that \\(z\\) is the closest vector to \\(x\\) in \\(\\cal W\\).\nFurthermore, \\(x-z\\) is orthogonal to any vector \\(r\\) in \\(\\cal W\\).\n\nThe full proof is beyond the scope of this book, as it requires background in real analysis. Indeed, even the statement of the theorem is not mathematically tight.For readers who do have such background, this is the Hilbert Projection Theorem. “Closest” is defined in terms of infimum, and \\(\\cal W\\) needs to be a topologically closed set. See for example the Wikipedia entry.l\nFor example, in the case of \\(R^n\\), It will be seen shortly that for each \\(\\cal W\\) in the theorem, there is a matrix \\(P_{W}\\) that implements the projection, i.e.\n\\[\nz = P_W x\n\\]\nNote that projection operators are idempotent, meaning that if you apply a projection twice, the effect is the same as applying it once. In the matrix equation above, this means \\(P_W^2 = P_W\\). This makes sense; once you drop down to the subspace, there is no further dropping down to that same space.\n\n\n8.7.2 The Pythagorean Theorem\nThat this ancient theorem in geometry still holds in general inner product spaces is a tribute to the power of abstraction.\n\nTheorem 8.4 If vectors \\(X\\) and \\(Y\\) are orthogonal, then\n\\[\n||X+Y||^2 = ||X||^2 + ||Y||^2\n\\tag{8.1}\\]\n\n\nProof. Replace the norms by expression in inner products. Simplify using properties of inner product.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Inner Product Spaces</span>"
    ]
  },
  {
    "objectID": "Ch5b.html#projections-in-the-linear-model",
    "href": "Ch5b.html#projections-in-the-linear-model",
    "title": "8  Inner Product Spaces",
    "section": "8.8 Projections in the Linear Model",
    "text": "8.8 Projections in the Linear Model\nThe case of the linear model will deepen our understanding, and will lead to a method for outlier detection that is commonly used in practice.\n\n8.8.1 The least-squares solution is a projection\nArmed with our new expertise on inner product spaces, we see that Equation 5.2 is\n\\[\n&lt;S-Ab,S-Ab&gt;\n\\]\nin the vector space \\(R^n\\), where \\(n\\) is the number of our data points. Since we are minimizing that quantity with respect to \\(b\\), the solution, \\(A \\widehat{\\beta}\\), is the projection of \\(Y\\) onto the subspace.Of course, “data points” means rows in the data frame. In the statistics realm, people often speak of “observations.”\nBut wait – what subspace? Well, it is the subspace consisting of all vectors of the form \\(Ab\\):\n\nThe linear model projects the vector \\(Y\\) onto the column space of \\(A\\).\n\nAgain, this follows from fact that setting \\(\\widehat{\\beta}\\) to the least-squares estimate amounts to minimizing \\(||S - Ab||\\).\n\n\n8.8.2 The “hat” matrix\nRecall Equation 5.5, which showed that the general solution to our linear regression model:\n\\[\n\\widehat{\\beta} = (A'A)^{-1} A'S\n\\]\nThe projection itself, i.e. the matrix \\(P_W\\) in Section 8.7, is then\n\\[\nA \\widehat{\\beta} = A(A'A)^{-1} A'S = HS\n\\]\nwhere the matrix\n\\[\nH = A(A'A)^{-1} A'\n\\]\nwhich projects \\(S\\) onto the column space of \\(A\\), is called the hat matrix.\nAs a projection, \\(H\\) is idempotent, which one can easily verify by multiplication. \\(H\\) is also symmetric.\n\n\n8.8.3 Application: identifying outliers\nAn outlier is a data point that is rather far from the others. It could be an error, or simply an anomalous case. Even in the latter situation, such a data point could distort our results, so in both cases, identifying outliers, and possibly removing them, is important.\nLet \\(h_{ii}\\) denote element \\(i\\) of the diagonal of \\(H\\), with \\(x_i\\) denoting row \\(i\\) of A. One can show that\n\\[\nh_{ii} = x_{i} (A'A)^{-1} x_i'\n\\tag{8.2}\\]\nThe quantity \\(h_{ii}\\) is called the leverage for datapoint \\(i\\), with the metaphor alluding to the impact of datapoint \\(i\\) on \\(\\widehat{\\beta}\\)\nUsing the material on circular shifts in Section 2.11.3, we have\n\\[\ntr(H) =\ntr[\\underbrace{A(A'A)^{-1}} A'] =\ntr[A'\\underbrace{A(A'A)^{-1}}] = tr(I) = p\n\\]\nfor \\(A\\) of size \\(n \\times p\\).\nThus the average value of \\(h_{ii}\\) is \\(p/n\\). Accordingly, we might suspect an outlier if \\(h_{ii}\\) is considerably larger than \\(p/n\\).\nFor example, let’s look at the Major League Baseball player data we’ve seen earlier (Section 5.3.5):\n\nlibrary(qeML)\ndata(mlb1)\nourData &lt;- as.matrix(mlb1[,-1]) # must have matrix to enable %*%\nA &lt;- cbind(1,ourData[,c(1,3)])\ndim(A)\n\n[1] 1015    3\n\nS &lt;- as.vector(mlb1[,3])\nH &lt;- A %*% solve(t(A) %*% A) %*% t(A)\nhist(diag(H))\n\n\n\n\n\n\n\n\nThe ratio \\(p/n\\) here is 3/1015, about 0.003. We might take a look at the observations having \\(h_{ii}\\) above 0.01, say.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Inner Product Spaces</span>"
    ]
  },
  {
    "objectID": "Ch5b.html#orthogonal-bases",
    "href": "Ch5b.html#orthogonal-bases",
    "title": "8  Inner Product Spaces",
    "section": "8.9 Orthogonal Bases",
    "text": "8.9 Orthogonal Bases\nIt turns out that a basis for a vector space is especially useful if its members are orthogonal to each other. We’ll now see why, and see how to generate such a basis from a nonorthogonal one.\nAn orthogonal basis in which every vector has length 1 is called orthonormal. Recall that \\(x \\neq 0\\) then \\(x/||x||\\) has length 1, so that any orthogonal basis can easily be converted to orthonormal.\n\n8.9.1 Motivation\nSay we have a vector space \\(\\cal V\\), a subspace \\(\\cal W\\), and a vector \\(x\\) in \\(\\cal V\\). We know \\(x\\) has a projection in \\(\\cal W\\); call it \\(z\\). But how do we find \\(z\\)?\nLet \\(u_1,...,u_k\\) be a basis for \\(\\cal W\\). Then there exist \\(a_1,...,a_k\\) such thatSo we are assuming \\(\\cal W\\) (but not necessarily \\(\\cal V\\) is finite-dimensional. Using proper math, this could be extended.\n\\[\nz = a_1 u_1 + ..., + a_k u_k\n\\tag{8.3}\\]\nSo we can find \\(z\\) by finding the \\(a_i\\), as follows.\n\n\n8.9.2 The virtues of orthogonality\nWe do have a hint to work from: We know that \\(x-z\\) is orthogonal to every vector in \\(\\cal W\\) – including the \\(u_i\\). So\n\\[\n0 = &lt;x-z,u_i&gt; = &lt;x,u_i&gt; - &lt;z,u_i&gt;\n\\]\nThus\n\\[\n&lt;x,u_i&gt; = &lt;z,u_i&gt;\n\\]\nNow, say the \\(u_i\\) are orthogonal to each other, and let’s also say they are length 1. Then \\(&lt;z,u_i&gt; = a_i\\),\nso\n\\[\n&lt;x,u_i&gt; = a_i\n\\]\nSo, we’re done! We want to determine the \\(a_i\\), and now we see that we can easily obtain it by calculating \\(&lt;x,u_i&gt;\\). So, we have:\n\nGiven: a vector space \\(\\cal V\\); a subspace \\(\\cal W\\) with orthonormal basis \\(u_1,...,u_k\\); and a vector \\(x\\) in \\(\\cal V\\). Then the projection of \\(x\\) onto \\(\\cal V\\) is equal to\n\\[\np = &lt;x,u_1&gt; u_1+...+&lt;x,u_k&gt; u_k.\n\\tag{8.4}\\]\nAnd, as a projection, we have that \\(x-p\\) is orthogonal to all the \\(u_i\\).\n\nBut how do we obtain an orthogonal basis, if we only have a nonorthogonal one? That’s next…",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Inner Product Spaces</span>"
    ]
  },
  {
    "objectID": "Ch5b.html#sec-gramschmidt",
    "href": "Ch5b.html#sec-gramschmidt",
    "title": "8  Inner Product Spaces",
    "section": "8.10 The Gram-Schmidt Method",
    "text": "8.10 The Gram-Schmidt Method\nAs seen in the last section, it is desirable to have an orthogonal basis, and it’s even more convenient if its vectors have length 1 (an orthonormal basis). Converting to length 1 is trivial – just divide the vector by its length.\nBut if we start with a basis \\(b_1,b_2,...,b_m\\), how can we generate an orthonormal basis from this?\n\nThe Gram-Schmidt Method\nSay we have a basis \\(b_1,...,b_k\\) for some vector space. Convert it to an orthonormal basis as follows.\n\nSet \\(u_1 = b_1/||b_1||\\).\nFor each \\(i = 2,...,k\\), find the projection \\(q\\) of \\(b_{i}\\) onto the subspace generated by \\(u_1,...,u_{i-1}\\). Set \\(u_i\\) to \\(b_i-q\\), and normalize it.\n\n\nWny does this work? Let \\(\\cal{W}\\) denote the subspace generated by \\(u_1,...,u_{i-1}\\). Since \\(q\\) is the projection of \\(b_i\\) onto \\(\\cal{W}\\), \\(b_i - q\\) will be orthogonal to \\(\\cal{W}\\), thus to \\(u_1,...,u_{i-1}\\) – exactly what we need.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Inner Product Spaces</span>"
    ]
  },
  {
    "objectID": "Ch5b.html#orthogonal-complements-and-direct-sums",
    "href": "Ch5b.html#orthogonal-complements-and-direct-sums",
    "title": "8  Inner Product Spaces",
    "section": "8.11 Orthogonal Complements and Direct Sums",
    "text": "8.11 Orthogonal Complements and Direct Sums\nConsider a subspace \\(W\\) of an inner product space \\(V\\). The set of vectors having inner product 0 with vectors in \\(W\\) is denoted \\(W^{\\perp}\\), known as the orthogonal complement of \\(W\\). It too is a subpace, and jointly \\(W\\) and \\(W^{\\perp}\\) span all of \\(V\\).\nFrom Section 8.7, we know that for any \\(x\\) in \\(V\\), one can uniquely write\n\\[\nx = x_1 + x_2\n\\]\nwhere \\(x_1\\) and \\(x_2\\) are in \\(W\\) and \\(W^{\\perp}\\), respectively.\nSay \\(u_1,...,u_r\\) and \\(v_1,...,v_s\\) are bases for \\(W\\) and \\(W^{\\perp}\\). Then together they form a basis for all of \\(V\\). Typically they are chosen to be orthonormal.\nFinally, we say that \\(V\\) is the direct sum of \\(W\\) and \\(W^{\\perp}\\), denoted \\(V = W \\oplus W^{\\perp}\\).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Inner Product Spaces</span>"
    ]
  },
  {
    "objectID": "Ch5b.html#projections-in-cal-rvomega",
    "href": "Ch5b.html#projections-in-cal-rvomega",
    "title": "8  Inner Product Spaces",
    "section": "8.12 Projections in \\(\\cal RV(\\Omega)\\)",
    "text": "8.12 Projections in \\(\\cal RV(\\Omega)\\)\nHere is a good example of how a very abstract vector space becomes useful in practical applications, such as will be presented in Section 8.13. The material is rather involved, consisting of computation of various probabilistic quantities. Since \\(\\cal RV(\\Omega)\\) is a vector space of random variables, each entity is both a random variable and a vector. Sometimes the latter will be the focus, sometimes the former. We request the reader’s patience in following this duality.\n\n8.12.1 Conditional expectation\nIt will turn out that in \\(\\cal RV(\\Omega)\\), projections take the form of conditional means. Let’s see how that arises.\nOne of the Your Turn problems at the end of this chapter covers this setting:\n\nSay we roll a die once, producing \\(X\\) dots. If \\(X = 6\\), we get a bonus roll, yielding \\(B\\) additional dots; otherwise, \\(B = 0\\). Let \\(Y = X+B\\).\n\nThe basic form:\nNow, what is \\(E(Y | B = 2)\\)? If \\(B = 2\\), then we got the bonus roll, so \\(X = 6\\) and \\(Y = X + B = 8\\):\n\\[\nE(Y | B = 2) = 8\n\\]\nSimilarly,\n\\[\nE(Y | B = 3) = 9\n\\]\nand so on.\nBut what about \\(E(Y | B=0)\\)? In that case, \\(Y = X\\), so\n\\[\nP(Y = i | B = 0) = P(X = i | X \\neq 6) = \\frac{1}{5}, ~ i=1,2,3,4,5\n\\]\nMore generally,\n\\[\nE(Y | B = i) =\n\\begin{cases}\n3 & i = 0 \\\\\n6 + i & i = 1,2,3,4,5\n\\end{cases}\n\\]\nThe random variable form:\nThe quantity \\(E(Y | B = i)\\), a number, can be converted to a random variable, in the form of a function of \\(B\\), which we will call \\(q(B)\\), and denoted \\(E(Y | B)\\) – without “= i” – where\n\\[\nq(B) =\n\\begin{cases}\n3 & B = 0 \\\\\n6 + B & B = 1,2,3,4,5\n\\end{cases}\n\\]\n\\(B\\) is random, so \\(q(B)\\) is also random.\nWe need one more thing:\nThe Law of Iterated Expectation:\nFor random variables \\(U\\) and \\(V\\), set\n\\[\nR = E[V |U]\n\\]\nThen\n\\[\nE(R) = E(V)\n\\]\nMore concisely:\n\\[\nE[E(V|U)] = E(V)\n\\tag{8.5}\\]\nIntuitive explanation: Say we wish to compute the mean height \\(E(H)\\) of all students at a university. We might ask each department \\(D\\) to measure their own students, and report to us the resulting mean \\(E(H|D)\\). We could then average all those departmental means to get the overall mean for the university:\n\\[\nE[E(H|D)] = E(H)\n\\]\nNote, though that that outer \\(E()\\) (the first ‘E’) is a weighted average, since some departments are larger than others. The weights are the distribution of \\(D\\).\n\n\n8.12.2 Projections in \\(\\cal RV(\\Omega)\\): how they work\nConsider random variables \\(X\\) and \\(Y\\). Let \\(W\\) be the set of all functions of \\(X\\) with finite variance, which is a subspace of the vector space \\(\\cal RV(\\Omega)\\). Theorem 8.3 talks of a closest vector \\(C\\) in \\(W\\) to \\(Y\\). Let’s see what form \\(C\\) might take in this vector space. For convenience, let’s assume that our variables have mean 0.Recall that we can convert a random variable to mean-0 by subtracting its mean.\nRemember, the (squared) distance from \\(Y\\) to \\(C\\) is\n\\[\n||Y - C||^2 = &lt;Y-C,Y-C&gt; = E[(Y-C)^2]\n\\]\nThat last term is\n\\[\nE[ E((Y-C)^2 | X) ]\n\\]\nFor any random variable \\(Q\\) of finite variance, the minimum value of \\(E[(Q-d)^2]\\) over all constants \\(d\\) is attained by taking \\(d\\) to be the mean of \\(Q\\), i.e. \\(d = E(Q)\\). (See Your Turn problem below.) So, the minimum of \\(E((Y-C)^2 | C)\\), for all random variables \\(C\\), is attained by the conditional mean,Note that since we are conditioning on the random variable \\(C\\), it becomes a constant, like \\(d\\) above.\n\\[\nC = E(Y | X)\n\\]\nIn other words:\n\nProjections in \\(\\cal RV(\\Omega)\\) take the form of conditional means.\nMoreover:\nSince the difference between a vector and its projection onto a subspace is orthogonal to that subspace we have:\n\n\nThe vector \\(Y - E(Y|X)\\) is uncorrelated with \\(E(Y|X)\\). In other words, the prediction error (also called the residual) has 0 correlation with the prediction itself.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Inner Product Spaces</span>"
    ]
  },
  {
    "objectID": "Ch5b.html#sec-fairness",
    "href": "Ch5b.html#sec-fairness",
    "title": "8  Inner Product Spaces",
    "section": "8.13 Application: Fairness in Algorithms",
    "text": "8.13 Application: Fairness in Algorithms\nCOMPAS is a software tool designed to aid judges in determining sentences in criminal trials, by assessing the probability that the defendant would recidivate. It is a commercial product by Northpointe.\nCOMPAS came under intense scrutiny after an investigation by ProPublica, which asserted evidence of racial bias against black defendants compared to white defendants with similar profiles. Northpointe contested these findings, asserting that their software treated black and white defendants equally.\nIt should be noted the ProPublica did not accuse Northpointe of intentional bias. Instead, the issue largely concerns proxies, variables that are related to race, rather than race (or gender etc.) itself. If for example COMPAS were to use a person’s home location as a predictor, that would be correlated to race, and thus would be unfair to use in prediction. The point here is that, due to proxies, we cannot solve the problem by simply removing \\(S\\) from our analysis; we would still be using correlates of \\(S\\).\nWhile this book does not take a position on the specific dispute, this case highlights the critical importance of addressing fairness in machine learning.\n\n8.13.1 Setting\nWe consider prediction of a variable \\(Y\\) from a feature vector \\(X\\) and a vector of sensitive variables \\(S\\). The target \\(Y\\) may be either numeric (in a regression setting) or dichotomous (in a two-class classification setting where \\(Y\\) = 1 or \\(Y\\) = 0). The \\(m\\)-class case can be handled using \\(m\\) dichotomous variables. We will consider only the numeric case here. Our goal is to eliminate the influence of \\(S\\).\n\n\n8.13.2 The method of Scutari et al\nThe basic assumption (BA) amounts to \\((Y,X,S)\\) having a multivariate Gaussian distribution, with \\(Y\\) scalar and \\(X\\) being a vector of length \\(p\\). For convenience, assume here that \\(S\\) is scalar. As in Section 8.5.1, all variables are assumed centered, i.e. mean 0.\nLet’s review the material in Section 4.3: Say we have \\(W\\) with a multivariate normal distribution, and wish to predict one of its components, \\(Y\\), from a vector \\(X\\) consisting of one or more of the other components, or linear combinations of them. Then\n\nthe distribution of \\(Y|X\\) is univariate normal\nE(Y|X=t) is a linear function of \\(t\\)\nVar(Y|X=t) is independent of \\(t\\)\n\nAnd most importantly:\n\nthough having 0 correlation does not in general imply independence, it does so in the multivariate normal case\n\nOne first applies a linear model in regressing \\(X\\) on \\(S\\),\n\\[\nE(X | S) = S \\gamma\n\\]\nwhere \\(\\gamma\\) is a length-\\(p\\) coefficient vector. Here we are predicting the predictors (of \\(Y\\)), seemingly odd, but a first step in ridding ourselves from the influence of \\(S\\).\nNow consider the prediction errors (residuals),\n\\[\nU = X - S \\gamma\n\\]\n\\(U\\) can be viewed as the part of \\(X\\) that is unrelated to \\(S\\); think of \\(U\\) as “having no \\(S\\) content.” Note that \\(U\\) is a vector of length \\(p\\).\nNote the following:\n\n\\(E(X|S)\\) is the projection of \\(X\\) onto the subspace of all functions of \\(S\\).\n\\(X - E(X|S)\\) (original vector minus the projection) is orthogonal to \\(S\\).\nThat is,\n\\[\n0 = &lt;S,X - E(X|S)&gt; = E[S (X - E(X|S))] = E(SU) = Cov(S,U)\n\\]\nThus \\(S\\) and \\(U\\) are uncorrelated.\nDue to the BA, that means \\(S\\) and \\(U\\) are independent.\nIn other words, our intution above that \\(U\\) “has no \\(S\\) content” was mathematically correct.\nBottom line: Instead of predicting \\(Y\\) from \\(X\\), use \\(U\\) as the predictor vector. This will enable truly \\(S\\)-free prediction.\n\nGoal achieved.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Inner Product Spaces</span>"
    ]
  },
  {
    "objectID": "Ch5b.html#your-turn",
    "href": "Ch5b.html#your-turn",
    "title": "8  Inner Product Spaces",
    "section": "8.14 Your Turn",
    "text": "8.14 Your Turn\n❄️ Your Turn: Consider the space \\(C(0,1)\\). For function \\(f\\) and \\(g\\) of your own choosing, verify that the Cauchy-Schwarz and Triangle Inequalities hold in that case.\n❄️ Your Turn: Show that for any random variable \\(Q\\) of finite variance, the minimum value of \\(E[(Q-d)^2]\\) over all constants \\(d\\) is attained by taking \\(d\\) to be the mean of \\(Q\\), i.e. \\(d = E(Q)\\). Hint: First expand \\((Q-d)^2\\) as \\(Q^2 - dQ + d^2\\).\n❄️ Your Turn: In the die rolling example, verify that\n\\[\nE[E(Y|B)] = E(Y)\n\\]\nby calculating both sides.\n❄️ Your Turn: Say we roll a die once, producing \\(X\\) dots. If \\(X = 6\\), we get a bonus roll, yielding \\(B\\) additional dots; otherwise, \\(B = 0\\). Let \\(Y = X+B\\). Verify that \\(X\\) and \\(B\\) satisfy the Cauchy-Schwarz and Triangle Inequalities, and also find \\(\\rho(X,B)\\).\n❄️ Your Turn: Derive the Cauchy-Schwarz Inequality, using the following algebraic outline:\n\nThe inequality\n\\[\n0 ~ \\leq ~ &lt;(au+v),(au+v)&gt; ~\n\\]\nholds for any scalar \\(a\\).\nExpand the right-hand side (RHS), using the bilinear property of inner products.\nMinimize the resulting RHS with respect to \\(a\\).\nCollect terms to yield\n\\[\n&lt;u,v&gt;^2 \\leq ||u||^2 ||v||^2\n\\]\n\n❄️ Your Turn: Use the Cauchy-Schwarz Inequality to prove the Triangle Inequality, using the following algebraic outline.\n\nStart with\n\\[\n||u+v||^2 = &lt;(u+v,u+v&gt;\n\\]\nExpand the RHS algebraically.\nUsing Cauchy-Schwarz to make the equation an inequality.\nCollect terms to yield the Triangle Inequality.\n\n❄️ Your Turn: Consider a set of vectors \\(W = {v_1,...,v_k}\\) in an inner product space \\(V\\). Let \\(u\\) be another vector in \\(V\\). Show that there exist scalars \\(a_1,...,a_k\\) and a vector \\(v\\) such that\n\\[\nu = a_1 v_1 + ... + a_k v_k +v\n\\]\nwith\n\\[\n&lt;v,v_i&gt; = 0 \\textrm{ for all i}\n\\]\n❄️ Your Turn: Consider the quadratic form \\(x'Px\\). Explain why, if \\(P\\) is a projection matrix, the form equals \\(||Px||^2\\).\n❄️ Your Turn: Explain why any set of orthogonal vectors is linearly independent.\n❄️ Your Turn: Prove that the vector \\(Y - E(Y|X)\\) is uncorrelated with \\(E(Y|X)\\)\n❄️ Your Turn: For \\(X\\) and \\(Y\\) in \\(\\cal RV(\\Omega\\), prove that\n\\[\nVar(Y) = E[Var(Y|X)] + Var[E(Y|X)],\n\\]\nfirst algebraically using Equation 8.5 and the relation \\(Var(R) =\nE(R^2) - (E(R))^2\\), and then using the Pythagorean Theorem for a much quicker proof. As before, assume \\(X\\) and \\(Y\\) are centered.\n❄️ Your turn: Show that Equation 8.2 holds.\n❄️ Your Turn: Consider the space \\(\\cal RV(\\Omega)\\). In order for the claimed inner product to be valid, we must have that if \\(&lt;X,X&gt; = 0\\), then \\(X\\) must be the 0 vector. Prove this.\n❄️ Your Turn: Say \\(\\cal V\\) is \\(R^n\\). Form the matrix \\(A\\) whose columns are the \\(u_i\\), and let \\(P = AA'\\). Show that\n\\[\nPx = &lt;x,u_1&gt; u_1+...+&lt;x,u_k&gt; u_k\n\\]\nso that \\(P\\) thereby implements the projection.\n❄️ Your Turn: Let \\(A\\) be an \\(m \\textrm{ x } n\\) matrix. Consider the possible inner product on \\(\\cal{R}^n\\) defined by \\(&lt;x,y&gt; = x'A'A y\\). State a condition on \\(A\\) that is necessary and sufficient for the claimed inner product to be value, and prove this.\n❄️ Your Turn: Show that for any vector \\(w\\) and symmetric matrix \\(M\\), the quadratic form \\(w'Mw \\geq 0\\).\n❄️ Your Turn: Say in \\(C(0,1)\\) we want to approximate functions by polynomials. Specifically, for any \\(f\\) in \\(C(0,1)\\), we want to find the closest polynomial of degree \\(m\\) or less. Write functions to do this, with the following call forms:\ngsc01(f,m)  # performs Gram-Schmidt and returns the result\nbestpoly(f,gsout)  # approx. f by output from gsc01\nHint: Note that the set of polynomials of a given degree or less is a subspace of \\(C(0,1)\\). Also since the vectors here are functions, you’ll need a data structure capable of storing functions. An R list will work well here.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Inner Product Spaces</span>"
    ]
  },
  {
    "objectID": "Ch5bb.html",
    "href": "Ch5bb.html",
    "title": "9  Four Fundamental Spaces",
    "section": "",
    "text": "9.1 The Four Fundamental Subspaces of a Matrix\nBefore going to SVD, let us first define four fundamental subspaces for any \\(m \\textrm{ x } n\\) matrix \\(A\\):\nThe null space is also called the kernel of \\(A\\), viewing the matrix as a function from \\(\\mathcal{R}^n\\) to \\(\\mathcal{R}^m\\); what vectors are mapped to 0?\nUse ‘dim()’ to indicate vector space dimension, and ‘rank()’ for matrix rank. Following are a few important facts about these spaces.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Four Fundamental Spaces</span>"
    ]
  },
  {
    "objectID": "Ch5bb.html#the-four-fundamental-subspaces-of-a-matrix",
    "href": "Ch5bb.html#the-four-fundamental-subspaces-of-a-matrix",
    "title": "9  Four Fundamental Spaces",
    "section": "",
    "text": "\\(\\textrm{row space}(A), \\mathcal{R}(A)\\): \\(\\{x'A\\}~~\\) (all linear combinations of rows of \\(A\\))\n\\(\\textrm{column space}(A), \\mathcal{C}(A)\\): \\(\\{Ax\\}~~\\) (all linear combinations of columns of \\(A\\))\n\\(\\textrm{null space}(A), \\mathcal{N}(A)\\): \\(\\{x: Ax = 0\\}\\)\n\\(\\textrm{left null space}(A) = \\mathcal{N}(A')\\): \\(\\{x: x'A = 0\\}\\)\n\n\n\n\nTheorem 9.1 \\[\nrank(A) = dim(\\mathcal{R}(A))\n\\]\n\n\nProof. \n\n\nTheorem 9.2 \\[\nrank(A) = dim(\\mathcal{C}(A))\n\\]\n\n\nProof. Let \\(z_1,...,z_r\\) be a maximal linearly independent set of columns of \\(A\\), where \\(r\\) is the rank of that matrix. Then this set is in \\(\\mathcal{C}(A)\\), and in fact must span that subspace. If not, there would be some vector \\(x\\) outside the span of \\(z_1,...,z_r\\), i.e. no linear combination of those vectors would equal \\(x\\). Then \\(z_1,...,z_r,x\\) would be a linearly independent set, contradicting the maximal nature of the \\(z_i\\).\nIn other words, the \\(z_i\\) form a basis for \\(\\mathcal{C}(A)\\), and thus the dimension of that subspace is \\(r\\).\n\\(\\square\\)\n\n\nTheorem 9.3 The \\(\\mathcal{C}(A) \\textrm{ and } \\mathcal{N}(A)\\) subspaces are closely related:\n\\[\n\\mathcal{C}(A')^{\\perp} = \\mathcal{N}(A)\n\\]\nand\n\\[\ndim(\\mathcal{C}(A')) + dim(\\mathcal{N}(A)) = n\n\\]\n\n\nProof. Say \\(w\\) is in \\(\\mathcal{N}(A)\\). Using partitioning, we have\n\\[\n\\left (\n\\begin{array}{r}\n0 \\\\\n... \\\\\n0 \\\\\n\\end{array}\n\\right )\n=\nAw\n=\n\\left (\n\\begin{array}{r}\na_1 w \\\\\n... \\\\\na_m w \\\\\n\\end{array}\n\\right )\n\\]\nwhere \\(a_i\\) is row \\(i\\) of \\(A\\). Thus \\(w\\) is orthogonal to the rows of \\(A\\), thus to the row space of \\(A\\). The latter is the column space of \\(A'\\). Thus \\(\\mathcal{N}(A)\\) is a subset of \\(\\mathcal{C}(A')^{\\perp}\\). This argument works exactly in reverse, so the first claim is established.\nThe second claim follows from first forming an orthonormal basis \\(q_1,...,q_r\\) for \\(\\mathcal{C}(A')\\), then extending it to one for all of \\(\\mathcal{R}^m\\), \\(q_1,...,q_r,q_{r+1},...,m\\).\n\\(\\square\\)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Four Fundamental Spaces</span>"
    ]
  },
  {
    "objectID": "Ch5c.html",
    "href": "Ch5c.html",
    "title": "10  Shrinkage Estimators",
    "section": "",
    "text": "10.1 Multicollinearity\nThe notion of multicollinearity was the original motivation for shrinkage estimators. It refers to settings in which the following concerns arise:\nThat latter point is often quantified by the Variance Inflation Factor. To motivate it, consider the “R-squared” value from linear regression analyis, which is the squared correlation between true “Y” and predicted “Y”. Let \\(R^2_j\\) denote that measure in the case of predicting column \\(j\\) of \\(A\\) from the other columns. The quantity\n\\[\nVIF_j = \\frac{1}{1-R^2_j}\n\\]\nthen measures the negative impact due to multicollinearity on estimating \\(\\widehat{\\beta}_j\\). The intuition is that, say, column 3 of \\(A\\) can be predicted well using a linear model, then that column is approximately equal to a linear combination of the other “X” columns. This is worrisome in light of the problems described above.\nNeedless to say, the word “nearly” above, e.g. in “nearly not of full rank,” is vague, and leaves open the question of “What can we do about it?” We will present several answers to these questions in this and the succeeding chapters.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Shrinkage Estimators</span>"
    ]
  },
  {
    "objectID": "Ch5c.html#sec-nearly",
    "href": "Ch5c.html#sec-nearly",
    "title": "10  Shrinkage Estimators",
    "section": "",
    "text": "One column of the matrix \\(A\\) in Equation 5.5 is nearly equal to some linear combination of the others.\nThus \\(A\\) is nearly not of full rank.\nThus \\(A'A\\) is nearly not of full rank.\nThus \\(\\widehat{\\beta}\\) is unstable, in the form of high variance.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Shrinkage Estimators</span>"
    ]
  },
  {
    "objectID": "Ch5c.html#sec-millionsong",
    "href": "Ch5c.html#sec-millionsong",
    "title": "10  Shrinkage Estimators",
    "section": "10.2 Example: Million Song Dataset",
    "text": "10.2 Example: Million Song Dataset\nLet’s consider the Million Song Dataset, varous versions of which are on the Web.\nOurs is a 50,000-line subset of the one with 515345 rows and 91 columns. The first column is the year of release, followed by 90 columns of various audio measurements. The goal is to predict the year, V1, from the audio variables V2 through V91.\nThe function regclass::VIF will compute the VIF values for us.\n\nlibrary(WackyData)\ndata(MillSong50K) # loads s50\nlmout &lt;- lm(V1 ~ .,data=s50)\nlibrary(regclass)\nVIF(lmout)\n\n      V2       V3       V4       V5       V6       V7       V8       V9 \n3.215081 2.596474 4.236853 7.414375 1.534492 5.844729 2.794018 3.192805 \n     V10      V11      V12      V13      V14      V15      V16      V17 \n2.072851 3.673590 4.705886 1.708520 2.446279 2.827296 3.672250 7.409782 \n     V18      V19      V20      V21      V22      V23      V24      V25 \n2.634033 9.472261 4.217311 7.147952 5.122114 7.984860 9.675134 3.591586 \n     V26      V27      V28      V29      V30      V31      V32      V33 \n1.818596 1.758043 3.879968 1.663670 2.108174 2.321385 2.056017 1.854913 \n     V34      V35      V36      V37      V38      V39      V40      V41 \n3.011534 2.040587 2.760111 2.879667 1.918229 2.176048 2.074103 1.946859 \n     V42      V43      V44      V45      V46      V47      V48      V49 \n1.704259 2.138794 1.690651 1.556782 1.817380 3.000759 1.547592 2.140282 \n     V50      V51      V52      V53      V54      V55      V56      V57 \n2.496121 1.612253 2.042571 2.208492 1.723669 2.024290 2.016403 2.033654 \n     V58      V59      V60      V61      V62      V63      V64      V65 \n2.004260 2.998469 2.074152 3.410124 2.153116 1.378160 3.270617 1.502543 \n     V66      V67      V68      V69      V70      V71      V72      V73 \n2.581171 1.725809 2.167673 2.379354 2.062862 1.703360 2.036596 1.984427 \n     V74      V75      V76      V77      V78      V79      V80      V81 \n2.557163 1.465020 1.515436 2.260728 1.840509 2.078497 3.604771 1.595064 \n     V82      V83      V84      V85      V86      V87      V88      V89 \n2.528307 2.005876 2.283956 1.448379 1.895053 1.601004 1.581099 2.252362 \n     V90      V91 \n1.332590 1.570891 \n\n\nAs a rough guide, values of VIF about 5.0 are considered concerning by many analysts. Under that criterion, variables V5, V7, V17 and so on look troublesome.\nWhat can be done? One simple approach would be to delete those columns from the dataset. This is indeed is a common solution, but another is ridge regression, which we present next.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Shrinkage Estimators</span>"
    ]
  },
  {
    "objectID": "Ch5c.html#ridge-regression",
    "href": "Ch5c.html#ridge-regression",
    "title": "10  Shrinkage Estimators",
    "section": "10.3 Ridge Regression",
    "text": "10.3 Ridge Regression\nIn seminal paper, Hoerl and Kennard presented a new approach to the problem of multicollinearity of predictor variables in a linear model.Technometrids, Februaru1970\n\n10.3.1 The ridge solution\nTheir solution is simple; Add some quantity to the diagonal of \\(A'A\\). Specifically, Equation 5.5 now becomes\n\\[\n\\widehat{\\beta} = (A'A + \\lambda I)^{-1} A'S\n\\tag{10.1}\\]\nwhere \\(\\lambda\\) is a positive number chosen by the analyst.\nHere \\(A\\) has dimensions \\(n \\textrm{ x } p\\), and \\(I\\) is the \\(p \\textrm{ x } p\\) identity matrix.\n\n\n10.3.2 Matrix formulation\nUsing partitioned matrices helps understand ridge. Replace \\(A\\) and \\(S\\) by\n\\[\nA_{new} =\n\\left (\n\\begin{array}{r}\nA \\\\\n\\lambda I \\\\\n\\end{array}\n\\right )\n\\]\nand\n\\[\nS_{new} =\n\\left (\n\\begin{array}{r}\nS \\\\\n0 \\\\\n\\end{array}\n\\right )\n\\]\nwhere 0 means \\(p\\) 0s. In essence, we are adding artificial data here, consisting of \\(p\\) new rows to \\(A\\), and \\(p\\) new elements to \\(S\\). So Equation 10.1 is just the result of applying Equation 5.5 to \\(A_{new}\\) and \\(S_{new}\\).\nLoosely speaking, we can think of the addition of \\(\\lambda I\\) to \\(A'A\\) makes the latter “larger”, and thus its inverse smaller. In other words, we are “shrinking” \\(\\widehat{\\beta}\\) towards 0. This effect is made even stronger by the fact that we added 0s data to \\(S\\). This will be made more precise below.\n\n\n10.3.3 Example: Million Song dataset\nWe will use glmnet, one of the most widely-used R packages.\n\nlibrary(glmnet)\nx &lt;- s50[,-1]\ny &lt;- s50[,1]\nglmOut &lt;- glmnet(x,y,\n             alpha=0,  # ridge\n             lambda=0.1)\ncoef(glmOut)\n\n91 x 1 sparse Matrix of class \"dgCMatrix\"\n                       s0\n(Intercept)  1.952611e+03\nV2           8.573258e-01\nV3          -5.589618e-02\nV4          -4.521365e-02\nV5           8.939743e-04\nV6          -9.630525e-03\nV7          -2.075296e-01\nV8          -4.753624e-03\nV9          -9.733144e-02\nV10         -6.383153e-02\nV11          2.778348e-02\nV12         -1.486791e-01\nV13         -1.326579e-02\nV14          4.756877e-02\nV15          3.197157e-04\nV16         -4.765018e-04\nV17          4.951670e-04\nV18          5.276124e-04\nV19          1.254372e-03\nV20          1.518736e-03\nV21          2.206416e-03\nV22         -4.304329e-04\nV23          5.792964e-04\nV24          7.717453e-03\nV25          3.205090e-03\nV26         -3.527333e-03\nV27          4.785205e-05\nV28          1.510325e-03\nV29          2.996810e-04\nV30          6.384049e-04\nV31         -2.560997e-04\nV32         -4.861561e-04\nV33         -6.250266e-04\nV34         -3.757523e-03\nV35          3.563733e-04\nV36          1.288622e-03\nV37         -4.417041e-03\nV38         -2.502325e-04\nV39          9.405005e-04\nV40          1.490186e-03\nV41         -1.534097e-03\nV42         -1.510983e-03\nV43         -1.777780e-03\nV44         -1.814201e-03\nV45         -1.865966e-03\nV46         -1.107727e-03\nV47          5.906898e-03\nV48          6.578124e-04\nV49         -2.040170e-03\nV50          4.795372e-04\nV51          1.162657e-03\nV52          6.164815e-04\nV53         -9.613775e-04\nV54          1.571230e-03\nV55         -1.230929e-03\nV56         -1.395143e-03\nV57          2.158453e-04\nV58         -1.975641e-03\nV59          2.039760e-03\nV60         -1.302405e-03\nV61          6.875221e-04\nV62         -3.480975e-03\nV63         -3.285816e-03\nV64         -9.199332e-03\nV65          1.283424e-03\nV66         -1.444237e-03\nV67         -5.957302e-05\nV68          1.139299e-03\nV69         -9.805816e-04\nV70         -3.734949e-03\nV71         -5.127353e-03\nV72         -1.071399e-03\nV73          1.808175e-04\nV74         -1.034781e-05\nV75          4.315147e-03\nV76          3.382526e-03\nV77          1.111779e-02\nV78          3.162072e-04\nV79         -4.565407e-03\nV80          3.729735e-05\nV81          2.362107e-04\nV82         -9.349464e-04\nV83         -2.846584e-04\nV84          1.439897e-03\nV85          1.360484e-03\nV86          2.442744e-02\nV87         -6.838186e-04\nV88          8.555403e-04\nV89         -3.329155e-02\nV90         -1.793489e-03\nV91         -3.614719e-05\n\n\nWe had earlier flagged variable \\(V5\\) as causing multicollinearity. As noted then, we could simply exclude it, but here under ridge, we see that it has been assigned a very small regression coefficient compared to most others.\nSo, did the estimated coefficient vector shrink?\n\nl2norm &lt;- function(x) sqrt(sum(x^2))\nbh01&lt;- coef(glmnet(x,y,alpha=0,lambda=0.1))\nl2norm(bh01)\n\n[1] 1952.612\n\nbhOLS &lt;- coef(lm(V1 ~ .,s50))\nl2norm(bhOLS)\n\n[1] 1951.028\n\n\nNo, the vector got larger!\nThe culprit is the intercept term, \\(\\widehat{\\beta}_0\\):\n\nbh01[1]\n\n[1] 1952.611\n\nbhOLS[1]\n\n(Intercept) \n   1951.028 \n\nl2norm(bh01[-1])\n\n[1] 0.9080075\n\nl2norm(bhOLS[-1])\n\n[1] 0.9451025\n\n\nThe rest of the vector did shrink (though not necessily element-by-element).\nActually, we should have centered and scaled “X” before applying ridge, since the predictors are of such different magnitudes. This also makes the intercept 0.\n\ns50a &lt;- s50\ns50a[,-1] &lt;- scale(s50[,-1])\ns50a[,1] &lt;- s50a[,1] - mean(s50a[,1])\nbh01&lt;- coef(glmnet(s50a[,-1],s50a[,1],alpha=0,lambda=0.1))\nl2norm(bh01)\n\n[1] 7.570367\n\nbhOLS &lt;- coef(lm(V1 ~ .,s50a))\nl2norm(bhOLS)\n\n[1] 7.886902",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Shrinkage Estimators</span>"
    ]
  },
  {
    "objectID": "Ch5c.html#choosing-the-value-of-lambda",
    "href": "Ch5c.html#choosing-the-value-of-lambda",
    "title": "10  Shrinkage Estimators",
    "section": "10.4 Choosing the Value of \\(\\lambda\\)",
    "text": "10.4 Choosing the Value of \\(\\lambda\\)\nSo, how do we choose \\(\\lambda\\)? We need to note a couple of very important principles first.\n\n10.4.1 Two key general statistical properties\n\n\n\n\n\n\nNote\n\n\n\nSubtracting the mean of a variable from that variable results in a mean-0 entity. In the linear regression context, centering both “X” and “Y” will result in the model having 0 as its intercept term.\n\n\n\n\n\n\n\n\nNote 10.1\n\n\n\nIn a set of 10,000 randomly typing monkeys, one of them will accidentally type a Shakespearian sonnet.\nSome readers have heard this before in the context of p-hacking, in which an analyst poses a large collection of research questions, and runs a statistical hypothesis on each of them. Even if the null hypothesis is true for all of them, each will have a 5% chance of being  rejected, and since there are many, the chance that at least one will be rejected and declared “significant.”\nThis problem arises in many, many Data Science contexts, and a good analyst must be vigilant to recognize the potential to mislead.\n\n\nOne should not be doing hypothesis tests in the first place. See these notes.\n\n10.4.2 Cross-validation\nA common way to choose among models is cross-validation: We set aside  a subset of the data, known as the holdout or test set, for use in testing predictive accuracy. The remaining data is the training set. For each of our competing models – in this case, competing values of \\(\\lambda\\) – we fit the model on the training set, then use the result to predict the test set. We then use whichever model does best in the test set.Some analysts break the data into three sets, including a validation set, considered even “fresher.”\nThe test set serves as “fresh data,” simulating how our fitted model might do in the real world (assuming our data is representative of the real world). Predicting on the training set is not as good, since our fit was by design tailored to that data.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Shrinkage Estimators</span>"
    ]
  },
  {
    "objectID": "Ch5c.html#example-million-song-data",
    "href": "Ch5c.html#example-million-song-data",
    "title": "10  Shrinkage Estimators",
    "section": "10.5 Example: Million Song Data",
    "text": "10.5 Example: Million Song Data\n&gt; library(glmnet)\n&gt; glmOut &lt;- cv.glmnet(x=as.matrix(s50[,-1]),y=s50$V1,alpha=0)\n&gt; glmOut\n    Lambda Index Measure     SE Nonzero\nmin 0.2474   100   89.44 0.9201      90\n1se 0.9987    85   90.28 0.9823      90\nThe \\(\\lambda\\) value that gave the smallest Mean Squared Prediction Error (MSE) was 0.2474. (Coincidentally, it was also the smallest value that the function tried; see **glmOut\\(lambda**.) A more conservative value of\\)$ was 0.9987, the largest \\(\\lambda\\) giving MSE within one standard error of the minimum; it’s conservative in the sense of being less likely to overfit; its MSE value, 90.28, was only slightly larger than the best one. In each case, all 90 predictors had nonzero coefficient estimates.\nWe can then predict as usual. Say we have a song similar to that in s50[1,], but with V2 equal to 25.0.\n\nz &lt;- s50[1,-1]  # exclude Y\nz[1,1] &lt;- 25.0\npredict(glmOut,z)\n\n           s0\n[1,] 2003.235\n\n\nThe year of release is predicted to be 2003.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Shrinkage Estimators</span>"
    ]
  },
  {
    "objectID": "Ch5c.html#sec-pgtn",
    "href": "Ch5c.html#sec-pgtn",
    "title": "10  Shrinkage Estimators",
    "section": "10.6 Modern View",
    "text": "10.6 Modern View\nThere are many ways to deal with multicollinearity other than shrinkage, and indeed, these days one seldom hears mention of multicollinearity in discussions of shrinkage. Instead, the goal is dimension reduction, meaning to reduce the complexity of a model in order to avoid overfitting; the LASSO, introduced below, does this explicitly, while ridge accomplishes it via pure size reduction.\nWe will discuss this further in Section 10.9, but one more point about the discussion no longer being motivated explicitly by multicollinearity: One can apply ridge to situations of exact dependence among the columns of X, as opposed to the original motivation of dealing with approximate linear dependence. in which there is exact linear dependence.\nWe saw such a setting in Section 6.1. There we deliberately induced exact linear dependence by inclusion of both male and female dummy variables. Let’s apply ridge:\n\nlibrary(qeML) \ndata(svcensus) \nsvc &lt;- svcensus[,c(1,4:6)]  \nsvc$man &lt;- as.numeric(svc$gender == 'male') \nsvc$woman &lt;- as.numeric(svc$gender == 'female') \nsvc$gender &lt;- NULL \na &lt;- svc[,-2] \nlambda &lt;- 0.1 \na &lt;- as.matrix(a) \ntmp1 &lt;- solve(t(a) %*% a + lambda * diag(4)) \ntmp1 %*% t(a) %*% as.matrix(svc$wageinc) \n\n               [,1]\nage        496.6716\nwkswrkd   1372.7052\nman     -18677.8751\nwoman   -29378.3086\n\n\nThe results essentially are the same as what we obtained by having only one dummy, thus no linear dependence: Men still enjoy about an $11,000 advantage. But ridge allowed us to avoid deleting one of our dummies. Such deletion is easy in this case, but for large \\(p\\), say in the hundreds or even more, some analysts prefer the convenience of ridge.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Shrinkage Estimators</span>"
    ]
  },
  {
    "objectID": "Ch5c.html#formalizing-the-notion-of-shrinkage",
    "href": "Ch5c.html#formalizing-the-notion-of-shrinkage",
    "title": "10  Shrinkage Estimators",
    "section": "10.7 Formalizing the Notion of Shrinkage",
    "text": "10.7 Formalizing the Notion of Shrinkage\nHow in the world did statisticians develop an interest in shrinking estimators? A watershed event occurred in the early 1980s, when the statistical world was shocked by research by James and Stein that found, in short that:\n\nSay \\(W\\) has \\(q\\)-dimensional normal distribution with mean vector \\(\\mu\\) and independent components having variance \\(\\sigma^2\\), each. We have a random sample of size \\(n\\), i.e. \\(n\\) independent observations on \\(W\\). Then if \\(q \\geq 3\\), in terms of Mean Squared Estimation Error, the best estimator of \\(\\mu\\) is NOT the sample mean \\(\\bar{W}\\). Instead, it’s\n\\[\n\\left (\n1 - \\frac{(q-2) \\sigma^2/n}{||\\bar{W}||^2}\n\\right )\n\\bar{W}\n\\]\n\nThe quantity within the parentheses is typically smaller than 1, giving us the shrinkage property. Note, though, that with larger \\(n\\), the amount of shrinkage is minor.\nIn the case of linear regression, shrinkage works there too, with \\(q\\) being the number of columns in the \\(A\\) matrix.\nSo, let’s view the issue of shrinkage more formally, first for ridge and later for the LASSO.\n\n10.7.1 Shrinkage through length penalization\nSay instead of minimizing Equation 5.2, we minimize\n\\[\n(S - Ab)'(S - Ab) + \\lambda ||b||^2\n\\tag{10.2}\\]\nThe larger \\(b\\) is, the harder it is to minimize the overall quantity Equation 10.2. We say that we penalize large values of \\(b\\), an indirect way of pursuing shrinkage. Now take the derivative and set to 0:\n\\[\n0 = A'(S-Ab) + \\lambda b\n\\tag{10.3}\\]\ni.e.\n\\[\n(A'A + \\lambda I) b = A'S\n\\]\nand thus\n\\[\n\\widehat{\\beta} = (A'A+\\lambda I)^{-1} A'S\n\\]\nIt’s ridge! So ridge, originally motivated by “almost singular” settings, actually turns out to justify ridge as a shrinkage estimator..\n\n\n10.7.2 Shrinkage through length limitation\nInstead of penalizing \\(||b||\\), we could simply constrain it, i.e. we could set our optimization problem to:\n\nminimize \\((S-Ab)'(S-Ab)\\), subject to the constraint \\(||b||^2 \\leq \\gamma\\)\n\nWe say that this new formulation is the dual of the first one. One can show that they are typically equivalent.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Shrinkage Estimators</span>"
    ]
  },
  {
    "objectID": "Ch5c.html#the-lasso",
    "href": "Ch5c.html#the-lasso",
    "title": "10  Shrinkage Estimators",
    "section": "10.8 The LASSO",
    "text": "10.8 The LASSO\nThe LASSO (Least Absolute Shrinkage and Selection Operator) was developed by Robert Tibshirani in 1996, following earlier work by Leo Breiman. It takes \\(\\widehat{\\beta}\\) to be the value of \\(b\\) that minimizes\n\\[\n(S - Ab)'(S - Ab) + \\lambda ||b||_1\n\\tag{10.4}\\]\nwhere the “l1 norm” is\n\\[\n||b|| = \\sum_{i=1}^p |b_i|\n\\]\nWe will write our original norm as \\(||b||_2\\).\nThis is a seemingly minor change, but with important implications. What Breiman and Tibshirani were trying to do was to obtain a sparse \\(\\widehat{\\beta}\\) , i.e. a solution with lots of 0s, thereby providing a method for predictor variable selection. This is important because so-called “parsimonious” prediction models are desirable.\n\n10.8.1 Properties\nTo that end, first note that it can be shown that, under some technical conditions, that the ridge solution minimizes\n\\[\n(S - Ab)'(S - Ab)\n\\]\nsubject to the constraint\n\\[\n||b||_2 \\leq \\gamma\n\\]\nwhile in the LASSO case the constraint is\n\\[\n||b||_1 \\leq \\gamma\n\\]\nAs with \\(\\lambda\\) in the original formulation, \\(\\gamma\\) is a positive number chosen by the analyst.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Shrinkage Estimators</span>"
    ]
  },
  {
    "objectID": "Ch5c.html#sec-dimred",
    "href": "Ch5c.html#sec-dimred",
    "title": "10  Shrinkage Estimators",
    "section": "10.9 Ridge vs. LASSO for Dimension Reduction",
    "text": "10.9 Ridge vs. LASSO for Dimension Reduction\nToday’s large datasets being so common, we need a way to “cut things down to size,” i.e. dimension reduction, aimed at reducing the number of predictor variables. This is done both for the sake of simplicity and to over overfitting, in which fitting an overly complex model can reduce predictive power.\n\n10.9.1 Geometric view\nComparison between the ridge and LASSO concepts is often done via this graph depicting the LASSO setting:\n\n\n\nLASSO sparsity\n\n\nHere \\(p = 2\\), with \\(b = (b_1,b_2)'\\). The horizontal and vertical axes represent \\(b_1\\) and \\(b_2\\).\n\nThe constraint \\(||b||_1 \\leq \\gamma\\) then takes the form of a diamond, with corners at \\((\\gamma,0)\\), \\((0,\\gamma)\\), \\((-\\gamma,0)\\) and \\((0,-\\gamma)\\). The constraint \\(||b||_1 \\leq \\gamma\\) requires us to choose a point \\(b\\) somewhere in the diamond, including the boundary.\nNote that each of the four corners of the diamond represents a sparse solution. For instance, the point \\((0,\\gamma)\\) has \\(b_2 = 0\\)\nThe concentric ellipses depict the values of \\(c(b) = (S - Ab)'(S -\nAb)\\), as follows.\nConsider one particular value of \\(c(b)\\), say 1.68.\nMany different points \\(b\\) in the graph will have \\(c(b) = 1.68\\); in fact, the locus of all such points is an ellipse.\nThere is one ellipse for each possible value of \\(c(b)\\). So, there are infinitely many ellipses, though only two are shown here.\nLarger values of \\(c(b)\\) yield larger ellipses.\nOn the one hand, we want to choose a \\(b\\) for which \\(c(b)\\) – our total squared prediction error – is small, thus a smaller ellipse.\nBut on the other hand, we need at least one point on the ellipse to be in common with the diamond.\nThe solution is then a point \\(b\\) in which the ellipse just barely touches the diamond.\nPicture in your mind an ellipse, say the inner one in the graph, growing larger and larger, while retaining the same center and orientation, until it hits the diamond. That is the outer ellipse, which indeed hits the diamond at one of the corners.\nThen picture other ellipses, at other centers with other orientations, and go through the same process in your mind’s eye. You will see that  typically the solution turns out to be one of the corners. Again, this is important because it gives us a sparse solution.\n\nOf course, we are in just two dimensions here. With \\(p = 10\\) predictor variables, the graph would be in 10 dimensions, beyond our human ability to picture. But we still would have a diamong in that space, with \\(2p\\) corners etc.\n\n10.9.2 Implication for dimension reduction.\nThe key point is that that “barely touching” point will be one of the four corners of the diamond, points at which either \\(b_1 = 0\\) or \\(b_2 = 0\\) – hence a sparse solution, meaning one in which many/most of the coefficients in the fitted model will be 0. This achieves the goal of dimension reduction.\nRidge will not produce a sparse solution. The diamond would now be a circle (not shown). The “barely touching point” will almost certainly will be at a place in which both \\(b_1\\) and \\(b_2\\) are nonzero. Hence no sparsity.\n\n\n10.9.3 Avoidance of overfitting without dimension reduction\nAs we’ve seen, both ridge and LASSO reduce the size of the \\(\\widehat{\\beta}\\) vector of estimated coefficients. Smaller quantities have smaller statistical variances, hence a guard against overfitting. So, even ridge can be employed as an approach to the overfitting problem, even though it does not provide a sparse solution.\nOn the other hand, in some settings, it may be desirable to keep all predictors, as seen in the next section.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Shrinkage Estimators</span>"
    ]
  },
  {
    "objectID": "Ch5c.html#sec-useall",
    "href": "Ch5c.html#sec-useall",
    "title": "10  Shrinkage Estimators",
    "section": "10.10 Example: NYC Taxi Data",
    "text": "10.10 Example: NYC Taxi Data\nThe purpose of this data is to predict trip time in the New York City taxi system. The \\(qeML\\) package includes a 10,000-row subset.\n\nlibrary(qeML)\ndata(nyctaxi)\nhead(nyctaxi)\n\n        trip_distance PULocationID DOLocationID tripTime DayOfWeek\n2969561          1.37          236           43      598         1\n7301968          0.71          238          238      224         4\n3556729          2.80          100          263      761         3\n7309631          2.62          161          249      888         4\n3893911          1.20          236          163      648         5\n4108506          2.40          161          164      977         5\n\ndim(nyctaxi)\n\n[1] 10000     5\n\nlength(unique(nyctaxi$PULocationID))\n\n[1] 143\n\nlength(unique(nyctaxi$DOLocationID))\n\n[1] 205\n\n\nIf we fit, say, a linear model, \\(lm\\) will form a dummy variable for each of the pickup and dropoff locations. Thus we will have \\(p = 1+143+205+1 = 350\\). An old rule of thumb says that if we have \\(p\\) predictors and \\(n\\) data points, we should keep \\(p &lt; \\sqrt{n}\\) to avoid overfitting. As we will see in a later chapter, these days that rule is being questioned, so  since here we have \\(\\sqrt{n} = 100\\), there is a strong suggestion that we do some dimension reduction.The problem would be even worse if we add pickup/dropoff location interaction variables, basically products of the pickup and dropoff dummy variables.\nThus we either should delete some of the pickup and dropoff variables, or use all of them but temper the fit using ridge. The latter may be more attractive, as riders would like a time estimate for their particular pickup and dropoff locations.\n\nlibrary(glmnet)\nnycwide &lt;- factorsToDummies(nyctaxi[,-1])\nglmOut &lt;- cv.glmnet(x=nycwide,y=nyctaxi[,1],alpha=0)\nglmOut\n\n\nCall:  cv.glmnet(x = nycwide, y = nyctaxi[, 1], alpha = 0) \n\nMeasure: Mean-Squared Error \n\n    Lambda Index Measure     SE Nonzero\nmin 0.3002   100   2.962 0.1804     356\n1se 1.0063    87   3.126 0.1644     356\n\n\nThe best \\(\\lambda\\) value was found to be 0.3002.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Shrinkage Estimators</span>"
    ]
  },
  {
    "objectID": "Ch5c.html#other-shrinkage-estimators",
    "href": "Ch5c.html#other-shrinkage-estimators",
    "title": "10  Shrinkage Estimators",
    "section": "10.11 Other Shrinkage Estimators",
    "text": "10.11 Other Shrinkage Estimators\nThe huge popular success of the LASSO inspired applying shrinkage to lots of other estimators. We will give an example in Chapter 13.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Shrinkage Estimators</span>"
    ]
  },
  {
    "objectID": "Ch5c.html#sec-iter",
    "href": "Ch5c.html#sec-iter",
    "title": "10  Shrinkage Estimators",
    "section": "10.12 Iterative Calculation",
    "text": "10.12 Iterative Calculation\nAn advantage of ridge over LASSO and other l1 shrinkage estimators is that the former has an explicit (we say closed-form) solution, which is not the case for the LASSO. In fact, as will be seen often in this book, many algorithms in statistics/machine learning lack closed-form solutions, in which case one must resort to iterative computation.\nThese means we make a series of guesses as the to value of the desired quantity, hopefully each more accurate than the last, eventually settling on a final guess.\nThis may or may not work well. Here are some of the major issues/perils:\n\ninitial guess\nupdating method\nlearning rate\nconvergence\npresence or lack of (calculus) derivatives\n\nThe basic idea is to first (somehow) make some guess as to the value of the desired quantity, say \\(\\widehat{\\beta}\\) in the LASSO. The algorithm crunches this to make a new, updated guess, hopefully one that is more accurate than the first. One then updates the new guess, continuing this process until, hopefully, it converges, meaning that it no longer changes much from one iteration to the next. The current guess is then deemed to be the correct value.\nThe case of computation of the LASSO is further complicated by its being based on the \\(l_1\\) norm, which in turn uses absolute values, i.e. \\(|x|\\). These have no derivative in the calculus sense, say as used in Equation 10.3 for ridge. This is a problem because many iterative methods are based on derivatives, as follows.\nSay we have a function \\(f\\) whose root \\(r\\) is of interest to us. We might make a series of guesses for \\(r\\) by considering the derivative \\(f'\\). This is illustrated in the figure. Unknown to us, \\(r = 2\\). Our current guess is \\(x = 3\\). We draw \\(f'\\), i.e. the tangent line to the curve at our current guess, and temporarily pretend that the line is the curve. We thus compute the root for the line, which is seen here to be near 2.0, and then take this tangent root as our updated guess for the root of the curve.Our quest for a root may arise for instance in a minimization problem, where we set a derivative to 0 and solve for that root.\n\n\n\nroot hunting\n\n\nSome machine learning algorithm have a parameter called the learning rate, which is motivated by a concern that the process may overshoot the root. A smaller learning rate value directs the algorithm to take smaller steps in generating new guesses. In this case, we might go only partway to the tangent root. On the one hand, this can slow the computation but on the other, we may be less likely to overshoot the true value.\nAt any rate, if the quantity \\(f\\) that we are working with does not have a derivative, our work is extra difficult.\nAt first one may think that such internal details of computation need not concern the end user of the software. But the fact is that often an algorithm will fail to converge, and the user may need to get more directly involved, say in suggesting the value of the initial guess.\nSo, if say glmnet fails to converge, what can be done? For example, in glmnet, the argument thresh defines what we mean by “no longer changes much”; we can decrease or even increase that value. One can make sure to center and scale the X data. Tweaking other parameters may help as well, but in the end, there are no magic solutions. It may well be that one’s basic model is flawed.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Shrinkage Estimators</span>"
    ]
  },
  {
    "objectID": "Ch5c.html#a-warning",
    "href": "Ch5c.html#a-warning",
    "title": "10  Shrinkage Estimators",
    "section": "10.13 A Warning",
    "text": "10.13 A Warning\nMany statistical quantities now have regularized, i.e. shrunken versions. It is also standard practice in neural networks. This may be quite helpful in prediction contexts. However, note the following:\n\n\n\n\n\n\nNo Statistical Inference on Shrinkage Estimators\n\n\n\nShrinkage produces a bias, of unknown size. Thus classical statistical inference (confidence intervals, hypothesis tests), e.g. those based on Equation 5.8 for linear models, is not possible.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Shrinkage Estimators</span>"
    ]
  },
  {
    "objectID": "Ch5c.html#your-turn",
    "href": "Ch5c.html#your-turn",
    "title": "10  Shrinkage Estimators",
    "section": "10.14 Your Turn",
    "text": "10.14 Your Turn\n❄️ Your Turn: Show that \\(A_{new}\\) in Section 10.3.2 is of full rank, \\(p\\).\n❄️ Your Turn: Consider a generalization of ridge regression, in which we find\n\\[\n\\textrm{argmin}_b ~\n||{S} - {A} b||^2 + ||{D} b||^2\n\\]\nfor a diagonal matrix \\(D\\). The idea is to allow different shrinkage parameters for different predictor variables. Show that\n\\[\nb =\n{[{A}' {A} + {D}^2]}^{-1}\n{A}' {S}\n\\]\n❄️ Your Turn: In Section 10.10, it was pointed out that in some settings we may prefer to retain all of our predictor variables, rather than do dimension reduction, thus preferring ridge to LASSO. But we might pay a price for that preference, in that the LASSO may actually give us better predictive power. Write an R function to investigate this, with call form\ncompareRidgeLASSO(data,yName)\nwhere data and yName are in the format of the predictive \\(qeML\\) functions, and the minimum Mean Squared Prediction Error is returned for both algorithms. Try your function on some of our datasets, or others.\n❄️ Your Turn: The LASSO will tend to produce solutions with lesser sparsity if the dataset is large. Write an R function to illustrate this, with call form\ndependN(data,yName,n=seq(1,nrow(data),100,nReps=1)\nwhere: where data and yName are in the format of the predictive \\(qeML\\) functions; the LASSO is applied to n randomly chosen rows of data; and nReps is the number of replicates to run at each value of n. The function will compute the number of nonzero elements in the \\(\\widehat{\\beta}\\) chosen by cross-validation in cv.glmnet. Try your code on a few datasets.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Shrinkage Estimators</span>"
    ]
  },
  {
    "objectID": "Ch6a.html",
    "href": "Ch6a.html",
    "title": "11  Eigenanalysis",
    "section": "",
    "text": "11.1 Example: African Soils Data\nTo get things started, let’s consider the African Soils dataset.\nLet’s take a look around:\nlibrary(WackyData) \ndata(AfricanSoil)\ndim(AfricanSoil)\n\n[1] 1157 3600\n\nnames(AfricanSoil)[1:25]\n\n [1] \"PIDN\"     \"m7497.96\" \"m7496.04\" \"m7494.11\" \"m7492.18\" \"m7490.25\"\n [7] \"m7488.32\" \"m7486.39\" \"m7484.46\" \"m7482.54\" \"m7480.61\" \"m7478.68\"\n[13] \"m7476.75\" \"m7474.82\" \"m7472.89\" \"m7470.97\" \"m7469.04\" \"m7467.11\"\n[19] \"m7465.18\" \"m7463.25\" \"m7461.32\" \"m7459.39\" \"m7457.47\" \"m7455.54\"\n[25] \"m7453.61\"\n\nnames(AfricanSoil)[3576:3600]\n\n [1] \"m605.545\" \"m603.617\" \"m601.688\" \"m599.76\"  \"BSAN\"     \"BSAS\"    \n [7] \"BSAV\"     \"CTI\"      \"ELEV\"     \"EVI\"      \"LSTD\"     \"LSTN\"    \n[13] \"REF1\"     \"REF2\"     \"REF3\"     \"REF7\"     \"RELI\"     \"TMAP\"    \n[19] \"TMFI\"     \"Depth\"    \"Ca\"       \"P\"        \"pH\"       \"SOC\"     \n[25] \"Sand\"\nLet’s try predicting pH, the acidity. But that leaves 3599 possible predictors. There is an old rule of thumb that one should have \\(p &lt; \\sqrt{n}\\), for \\(p\\) predictors and \\(n\\) data points, to avoid overfitting, a rule which in our setting of \\(n = 1157\\) is grossly violated. We need to do dimension reduction. One way to accomplish this is to use Principal Components Analysis (PCA).",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Eigenanalysis</span>"
    ]
  },
  {
    "objectID": "Ch6a.html#overall-idea",
    "href": "Ch6a.html#overall-idea",
    "title": "11  Eigenanalysis",
    "section": "11.2 Overall Idea",
    "text": "11.2 Overall Idea\nThe goal is to find a few important linear combinations of our original predictor variables–important in the sense that they roughly summarize our data. These new variables are called the principal components (PCs) of the data. PCA will be covered in detail in the next chapter, but our preview here will set the stage for important general concepts that we will develop in the current chapter.\n\n11.2.1 The first PC\nLet X denote a set of variables of interest (not necessarily in a prediction context). In searching for good linear combinations, we want to aim for ones with high variance. We certainly don’t want ones with low variance; after all, a random variable with 0 variance is a constant. So we wish to find linear combinations\n\\[\nXu\n\\]\nwhich maximize\n\\[\nVar(Xu) = u' Cov(X) u\n\\]\nwhere we have invoked Equation 4.4.\nBut that goal is ill-defined, since we could take larger and larger vectors \\(u\\), thus larger and larger vectors \\(Xu\\), no maximum. So, let’s constrain it to vectors \\(u\\) of length 1:\n\\[\nu'u = 1\n\\]\nThe method of Lagrange multipliers is used to solve maximum/minimum problems that have constraints, by adding a new variable corresponding to the constraint. In our case, we maximize\n\\[\nu'Cov(X)u + \\gamma (u'u - 1)\n\\]\nwith respect to \\(u\\) and \\(\\gamma\\).\nSetting derivatives to 0, we have\n\\[\n0 =\n2 Cov(X) u + 2 \\gamma u\n\\]\nIn other words,\n\\[\nCov(X) ~ u = -\\gamma u\n\\tag{11.1}\\]\nWe see a situation in which a matrix (\\(Cov(X)\\)) times a vector (\\(u\\)) equals a constant (\\(-\\gamma\\)) times that same vector. We say that \\(-\\gamma\\) is an eigenvalue of the matrix \\(Cov(X)\\), and that \\(u\\) is a corresponding eigenvector. Seems innocuous, but it opens a huge new world!\nNote too that from Equation 4.5,\n\\[\n\\textrm{maximal variance} = u' Cov(X) u = u' (-\\gamma u = -\\gamma\n\\tag{11.2}\\]\nWe will return to the notion of principal components in the next chapter, after laying the groundwork in the current chapter.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Eigenanalysis</span>"
    ]
  },
  {
    "objectID": "Ch6a.html#definition",
    "href": "Ch6a.html#definition",
    "title": "11  Eigenanalysis",
    "section": "11.3 Definition",
    "text": "11.3 Definition\nThe concept itself is simple:\n\nConsider an \\(m \\times  m\\) matrix \\(M\\). If there is a nonzero vector \\(x\\) and a number \\(\\lambda\\) such that\n\\[\nMx = \\lambda x\n\\tag{11.3}\\]\nwe say that \\(\\lambda\\) is an eigenvalue of \\(M\\), with eigenvector \\(x\\).",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Eigenanalysis</span>"
    ]
  },
  {
    "objectID": "Ch6a.html#a-first-look",
    "href": "Ch6a.html#a-first-look",
    "title": "11  Eigenanalysis",
    "section": "11.4 A First Look",
    "text": "11.4 A First Look\nHere are a few properties to start with:\n\nThe definition is equivalent to\n\\[\n(M - \\lambda I) x = 0\n\\]\nwhich in turn implies that \\(M - \\lambda I\\) is noninvertible. That then implies that\n\\[\ndet(M - \\lambda I) = 0\n\\]\nThe left-hand side of this equation is a polynomial in \\(\\lambda\\). So for an \\(n \\times  n\\) matrix \\(M\\), there are \\(n\\) roots of the equation and thus \\(n\\) eigenvalues. Note that some roots may be repeated; if \\(M\\) is the zero matrix, it will have an eigenvalue \\(0\\) with multiplicity \\(n\\).\nIn principle, that means we can solve the above determinant equation to find the eigenvalues of the matrix, though There are much better ways to calculate the eigenvalues than this, but a useful piece information arising from that analysis is that \\(M\\) as \\(m\\) eigenvalues (not necessarily distinct).",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Eigenanalysis</span>"
    ]
  },
  {
    "objectID": "Ch6a.html#the-special-case-of-symmetric-matrices",
    "href": "Ch6a.html#the-special-case-of-symmetric-matrices",
    "title": "11  Eigenanalysis",
    "section": "11.5 The Special Case of Symmetric Matrices",
    "text": "11.5 The Special Case of Symmetric Matrices\nMany matrices in data science are symmetric, such as covariance matrices and \\(A'A\\) in Equation 5.5.\nSome eigenvalues may be complex numbers, i.e. of the form \\(a+bi\\), but it can be shown that if \\(M\\) is symmetric, its eigenvalues are guaranteed to be real. This is good news for data science, as many matrices in that field are symmetric, such covariance matrices.\n\nTheorem 11.1 Eigenvalues of a symmetric matrix are real, not complex numbers, i.e. not of the form \\(a+bi\\).\n\n\nTheorem 11.2 Eigenvectors corresponding to distinct eigenvalues of a symmetric matrix \\(M\\) are orthogonal.\n\n\nProof. Let \\(u\\) and \\(v\\) be such eigenvectors, corresponding to eigenvalues \\(\\mu\\) and \\(\\nu\\).\n\\[\nu' M u = (u' M u)' = u' M' u = u' M u\n\\]\n\\[\nu'v = (u'v)' = [u'(\\frac{-1}{\\nu} M v)]' = \\frac{-1}{\\nu}v'M'u =\n\\frac{-1}{\\nu}v'Mu =\n\\frac{\\mu}{\\nu}v'u\n\\]\nWe see that \\(u'v\\), a number, is equal to \\(\\mu/\\nu\\) times itself, and since that fraction is not equal to 1 by assumption, \\(u'v\\) must be 0\n\\(\\square\\)\n\nA square matrix \\(R\\) is said to be diagonalizable if there exists an invertible matrix \\(P\\) such that \\(P^{-1}RP\\) is equal to some diagonal matrix \\(D\\). We will see that symmetric matrices fall into this category.\nOne application of this is the computation of powers of a diagonal matrix:\n\\[\nR^k = (P^{-1}RP) ~ (P^{-1}RP) ~ ... ~ (P^{-1}RP) = P^{-1}D^kP\n\\]\n\\(D^k\\) is equal to the diagonal matrix with elements \\(d_k^k\\), so the computation of \\(M^k\\) is easy.\n\nTheorem 11.3 Any symmetric matrix \\(M\\) has the following properties\n\n\\(M\\) is diagonalizable.\nIn fact, the matrix \\(P\\) is equal to the matrix whose columns are the eigenvectors \\(u_i\\) of \\(M\\), chosen to have norm 1. Thus the same holds for the rows of \\(P'\\), so that\n\\[\nP'u_i = \\lambda_i u_i\n\\tag{11.4}\\]\nThe associated diagonal matrix has as its diagonal elements the eigenvalues of \\(M\\).\nThe matrix \\(P\\) has the property that \\(P^{-1} = P'\\).\nMoreover, the rank of \\(M\\) is equal to the number of nonzero eigenvalues of \\(M\\).\n\n\n\nProof. Use partitioned matrices.\nLet \\(D = diag(d_1,...,d_m)\\), where the \\(d_i\\) are the eigenvalues of \\(M\\), corresponding to eigenvectors \\(P_i\\). Set the latter to have length 1, by dividing by their lengths.\nRecall that the eigenvectors of \\(M\\), i.e. the columns of \\(P\\), are orthogonal. Again using partitioning, we see that\n\\[\nP'P = I\n\\]\nso that \\(P^{-1} = P'\\).\nBy the way, any square matrix \\(Q\\) such that \\(Q'Q = I\\) is said to be orthogonal. And, its inverse is its transpose.\nNow use partitioning again:\n\\[\nMP = M(P_1 | ... | P_m) = (MP_1 | ... | MP_n) =\n(d_1 P_1 | ... | (d_m P_m) = PD\n\\]\nMultiply on the left by \\(P'\\), and we are done:\n\\[\nP'MP = P'P D = D\n\\]\nRegarding rank, Theorem 6.1 tells us that pre- or postmultiplying by an invertible matrix does not change rank, and clearly the rank of a diagonal matrix is the number of nonzero elements.\n\\(\\square\\)",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Eigenanalysis</span>"
    ]
  },
  {
    "objectID": "Ch6a.html#example-census-dataset",
    "href": "Ch6a.html#example-census-dataset",
    "title": "11  Eigenanalysis",
    "section": "11.6 Example: Census Dataset",
    "text": "11.6 Example: Census Dataset\nLet’s illustrate all this with the data in Section 10.6. We will form the matrix \\(A'A\\) in Section 5.3.3, which as mentioned, is symmetric.\nRecall that in that example, \\(A\\) is not of full rank. Thus we should expect to see a 0 eigenvalue.\n\nlibrary(qeML)\ndata(svcensus)\nsvc &lt;- svcensus[,c(1,4:6)]\nsvc$man &lt;- as.numeric(svc$gender == 'male')\nsvc$woman &lt;- as.numeric(svc$gender == 'female')\nsvc$gender &lt;- NULL\na &lt;- as.matrix(svc[,-2])\na &lt;- cbind(1,a)  # add the column of 1s\nm &lt;- t(a) %*% a\neigs &lt;- eigen(m)\neigs\n\neigen() decomposition\n$values\n[1] 7.594762e+07 3.292955e+06 7.466971e+03 1.293594e+03 2.801489e-12\n\n$vectors\n             [,1]         [,2]         [,3]        [,4]          [,5]\n[1,] -0.015881850  0.004389585 -0.035995373  0.81553633  5.773503e-01\n[2,] -0.649660696  0.760031134  0.004461047 -0.01654550  1.561251e-17\n[3,] -0.759952974 -0.649864420  0.004991922 -0.01108122 -4.061024e-17\n[4,] -0.012070494  0.002130704 -0.724401141  0.37650952 -5.773503e-01\n[5,] -0.003811356  0.002258881  0.688405767  0.43902681 -5.773503e-01\n\nm %*% eigs$vectors[,1]\n\n               [,1]\n         -1206188.6\nage     -49340181.0\nwkswrkd -57716616.5\nman       -916725.2\nwoman     -289463.4\n\neigs$values[1] %*% eigs$vectors[,1]\n\n         [,1]      [,2]      [,3]      [,4]      [,5]\n[1,] -1206189 -49340181 -57716617 -916725.2 -289463.4\n\n\nYes, that first column is indeed an eigenvector, with the claimed eigenvalue.\nNote that the expected 0 eigenvalue shows up as 2.801489e-12, quite small but nonzero, due to roundoff error.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Eigenanalysis</span>"
    ]
  },
  {
    "objectID": "Ch6a.html#application-detecting-multicollinearity",
    "href": "Ch6a.html#application-detecting-multicollinearity",
    "title": "11  Eigenanalysis",
    "section": "11.7 Application: Detecting Multicollinearity",
    "text": "11.7 Application: Detecting Multicollinearity\nConsider the basic eigenanalysis equation,\n\\[\nAx = \\lambda x\n\\]\nfor a square matrix \\(A\\), a conformable vector \\(x\\) and a scalar \\(\\lambda\\). Suppose that, roughly speaking, \\(\\lambda x\\) is small relative to \\(A\\). Then\n\\[\nAx \\approx 0\n\\]\nand since \\(Ax\\) is a linear combination of the columns of \\(A\\), we thus we have found multicollinearity in \\(A\\).\nOne often sees use of the condition number of a matrix, which is the ratio of the largest eigenvalue to the smallest one. This too might be used as a suggestion of multicollinearity, though the main usage is as a signal that matrix operations such finding inverses may have significant problems with roundoff error.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Eigenanalysis</span>"
    ]
  },
  {
    "objectID": "Ch6a.html#example-currency-data",
    "href": "Ch6a.html#example-currency-data",
    "title": "11  Eigenanalysis",
    "section": "11.8 Example: Currency Data",
    "text": "11.8 Example: Currency Data\nThis dataset tracks five pre-euro European currencies.\n\nlibrary(qeML)\ndata(currency)\nhead(currency)\n\n  Can..dollar Ger..mark Fr..franc UK.pound J..yen\n1          19       580     4.763       29    602\n2          18       609     4.818       44    609\n3          20       618     4.806       66    613\n4          46       635     4.825       79    607\n5          42       631     4.796       77    611\n6          45       635     4.818       74    610\n\ndim(currency)\n\n[1] 762   5\n\ncrc &lt;- currency\ncrc &lt;- as.matrix(crc)\ncrcapa &lt;- t(crc) %*% crc\neigs &lt;- eigen(crcapa)\neigs\n\neigen() decomposition\n$values\n[1] 4.180990e+08 5.356321e+07 8.199388e+06 4.686379e+06 4.764149e+02\n\n$vectors\n             [,1]         [,2]         [,3]          [,4]          [,5]\n[1,] -0.464701846 -0.589664107  0.573442227  0.3277874495 -0.0082362154\n[2,] -0.548123722  0.364936898 -0.424252867  0.6216029591 -0.0008432535\n[3,] -0.008118519 -0.002208129  0.005818183 -0.0005349635  0.9999475368\n[4,] -0.541352395 -0.364398075 -0.476833987 -0.5888747450 -0.0027404807\n[5,] -0.436445017  0.621551662  0.513584476 -0.3992385223 -0.0053728096\n\n# illustrate Ax = lambda x\ncrcapa %*% eigs$vectors[,5]\n\n                   [,1]\nCan..dollar  -3.9238561\nGer..mark    -0.4017386\nFr..franc   476.3899505\nUK.pound     -1.3056059\nJ..yen       -2.5596868\n\neigs$values[5] *  eigs$vectors[,5] \n\n[1]  -3.9238561  -0.4017386 476.3899505  -1.3056060  -2.5596868\n\n# is Ax small?\nhead(crcapa)\n\n            Can..dollar Ger..mark  Fr..franc  UK.pound    J..yen\nCan..dollar   112111475  93929522 1673631.29 113542760  66967756\nGer..mark      93929522 136033582 1795560.29 116882053 109220119\nFr..franc       1673631   1795560   28573.49   1859363   1433430\nUK.pound      113542760 116882053 1859363.20 133130962  85746625\nJ..yen         66967756 109220119 1433430.43  85746625 103243878\n\n\nSo yes, this dataset has some multicollinearity.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Eigenanalysis</span>"
    ]
  },
  {
    "objectID": "Ch6a.html#computation-the-power-method",
    "href": "Ch6a.html#computation-the-power-method",
    "title": "11  Eigenanalysis",
    "section": "11.9 Computation: the Power Method",
    "text": "11.9 Computation: the Power Method\nOne way to compute eigenvalues and eigenvectors is the power method, a simple iteration. We begin with an initial guess, \\(x_0\\) for an eigenvector. Substituting in Equation 11.3, we have the next guess:\n\\[\nx_1 = M x_0\n\\]\nWe keep iterating until convergence, generating \\(x_2\\) from \\(x_1\\) and so on. However, the \\(x_i\\) may grow, so we normalize to length 1:\n\\[\nx_i \\leftarrow \\frac{x_i}{||x_i||}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Eigenanalysis</span>"
    ]
  },
  {
    "objectID": "Ch6a.html#application-computation-of-long-run-distribution-in-markov-chains",
    "href": "Ch6a.html#application-computation-of-long-run-distribution-in-markov-chains",
    "title": "11  Eigenanalysis",
    "section": "11.10 Application: Computation of Long-Run Distribution in Markov Chains",
    "text": "11.10 Application: Computation of Long-Run Distribution in Markov Chains\nWe showed in Section 3.3 how matrix inverse can be used to compute the long-run distribution \\(\\nu\\) in a Markov chain. However, this is inefficient for very large transition matrices. For instance, in Google PageRank, there is a Markov state for every page on the Web!\nInstead, we exploit the fact that Equation 3.3 says that the transition matrix has an eigenvector \\(\\nu\\) with eigenvalue 1. Due to the typical huge size of the matrix, the power method or a variant is often used.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Eigenanalysis</span>"
    ]
  },
  {
    "objectID": "Ch6a.html#your-turn",
    "href": "Ch6a.html#your-turn",
    "title": "11  Eigenanalysis",
    "section": "11.11 Your Turn",
    "text": "11.11 Your Turn\n❄️ Your Turn: Show that the diagonalizing matrix \\(P\\) for a symmetric matrix must have determinant \\(\\pm 1\\).\n❄️ Your Turn: Show that if \\(x\\) is an eigenvector of \\(M\\) with eigenvalue \\(\\lambda \\neq 0\\), then for any nonzero number \\(c\\), \\(cx\\) will also be an eigenvector with eigenvalue \\(\\lambda\\).\n❄️ Your Turn: Show that if a matrix \\(M\\) has a 0 eigenvalue, \\(M\\) must be singular. Also prove the converse. (Hint: Consider the column rank.)\n❄️ Your Turn: Consider a projection matrix \\(P_W\\). Show that the only possible eigenvalues are 0 and 1. Hint: Recall that projection matrices are idempotent.\n❄️ Your Turn: Say \\(A\\) is an invertible matrix with eigenvalue \\(\\lambda\\) and eigenvector \\(v\\). Show that \\(v\\) is also an eigenvector of \\(A^{-1}\\), with eigenvalue \\(1/\\lambda\\).\n❄️ Your Turn: Show that if \\(A\\) is nonnegative-definite, its eigenvalues must be nonnegative.\n❄️ Your Turn: Theorem 11.3 says that for any symmetric matrix \\(M\\) is diagonalizable: There is an orthogonal matrix \\(P\\) such that \\(P' M P\\) is equal to a diagonal matrix \\(D\\), the latter consisting of the eigenvalues of \\(M\\). Use this to show that\n\\[\nx' M x \\leq \\lambda_{max} ||x||^{2}\n\\]\nwhere \\(\\lambda_{max}\\) is the largest eigenvalue. Hint: First show that\n\\[\nx'Mx = (Px)' D (Px)\n\\]\n❄️ Your Turn: Show that any Markov transition matrix has an eigenvalue 1, with eigenvector consisting of all 1s.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Eigenanalysis</span>"
    ]
  },
  {
    "objectID": "Ch6b.html",
    "href": "Ch6b.html",
    "title": "12  Principal Components",
    "section": "",
    "text": "12.1 The Second, Third Etc. PCs\nSo in the last chapter we had a very brief introduction to Principal Components Analysis (PCA), and that finding the first component entails finding an eigenvector and eigenvalue. It turns out that this is true for the second, third and all the components. In other words, PCA is basically an eigenvectors and eigenvalues application.\nSay we have a dataset \\(X\\) in matrix form. There are as many principal components as there are columns in \\(X\\). We derived the first PC in @secfirstPC. So, how are the second, third and so on components defined and computed?\nThe key issue is that we want our PCs to be orthogonal, because as we have seen orthogonal random vectors are statistically uncorrelated, and independent in the multivariate normal case. So if we summarize our data with say, the first few PCs, the fact that they are uncorrelated makes for a neater summary.\nSo with \\(u\\) and \\(v\\) denoting the first and second PCs, we want \\(v\\) to maximize \\(v'Cov(X)v\\), so we set the derivative of\n\\[\nv'Cov(X)v + \\omega (v'v - 1) + \\tau (u'v - 0)\n\\]\nwith respect to \\(v\\) to 0.\n\\[\n0 = 2 Cov(X) v + 2 \\omega v + \\tau u\n\\tag{12.1}\\]\nPre-multiply by \\(u'\\):\n\\[\n0 = 2 u' Cov(X) v + 2 \\omega u'v + \\tau u'u = 2 u' Cov(X) v + \\tau 1\n\\tag{12.2}\\]\nAlso, since \\(Cov(X)\\) is symmetric and \\(u\\) is an eigenvector of \\(Cov(X)\\),\n\\[\nu' Cov(X) v = [Cov(X) u]' v = \\gamma u' v = 0\n\\]\nSo Equation 12.2 now tells us that \\(\\tau = 0\\), and thus Equation 12.1 reduces to\n\\[\nCov(X) v = -\\omega v\n\\tag{12.3}\\]\nshowing that the second PC is again an eigenvector of \\(Cov(X)\\). Equation 11.2 then says that \\(-\\omega\\) is the corresponding variance.\nThe story is the same for the remaining PCs. We thus have:",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Principal Components</span>"
    ]
  },
  {
    "objectID": "Ch6b.html#the-second-third-etc.-pcs",
    "href": "Ch6b.html#the-second-third-etc.-pcs",
    "title": "12  Principal Components",
    "section": "",
    "text": "Theorem 12.1 The principle components have the following properties:\n\nThey are orthogonal to each other.\nThey are eigenvectors of \\(Cov(X)\\).\nThe eigenvalue corresponding to a PC \\(z\\) is \\(Var(z'X)\\).\nThe eigenvalues form a nondecreasing sequence.\nSay \\(X\\) is of length \\(m\\). Normalize the PCs \\(u_i\\) (i.e. divide a PC by its length, so that we have a vector of length 1) and form the \\(m\n\\times m\\) (partitioned) matrix\n\\[\nP = (u_1 | u_2 | ... | u_m)\n\\]\nThen \\(P' Cov(X) P = D\\), where \\(D\\) is a diagonal matrix whose entries are \\(Var(u_i'X)\\), and \\(P'P = I\\).",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Principal Components</span>"
    ]
  },
  {
    "objectID": "Ch6b.html#sec-eiglm",
    "href": "Ch6b.html#sec-eiglm",
    "title": "12  Principal Components",
    "section": "12.2 Eigenanalysis Relation between \\(A\\) and \\(Cov(X)\\) in Linear Regression",
    "text": "12.2 Eigenanalysis Relation between \\(A\\) and \\(Cov(X)\\) in Linear Regression\nConsider our usual linear regression setting, in which the matrix \\(A\\) contains the data for our predictor variables \\(X\\). If we center and scale our data, then\n\\[\nCov(X) = A'A\n\\]\nTo see an example of the implications of this, say the PCA of X has a 0 eigenvalue. That implies that some linear combination \\(u'X\\) has 0 variance. Then\n\\[\n0 = Var(u'X) = u' Cov(X) u = u' A'A u = (Au)'Au\n\\]\nThus\n\\[\nAu = 0\n\\]\nSince \\(Au\\) is a linear combination of the columns of \\(A\\), we see that the columns of \\(A\\) are linearly dependent.\nThe same argument works in reverse. If \\(Au = 0\\), we fine that \\(Var(u'X)\n= 0\\), and since the eigenvalues of \\(Cov(X)\\) are variances of linear combinations of \\(X\\), we see that one of those variances is 0.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Principal Components</span>"
    ]
  },
  {
    "objectID": "Ch6b.html#sec-backtoafrica",
    "href": "Ch6b.html#sec-backtoafrica",
    "title": "12  Principal Components",
    "section": "12.3 Back to the African Soils Example",
    "text": "12.3 Back to the African Soils Example\nSo, we remove the ``Y’’ variable, pH, number 3598 as seen above, and proceed. We will also remove the nonnumeric columns, PIDN and Depth. We could use R’s prcomp function here, but to better illustrate the concepts, we do things from scratch.\n\nlibrary(WackyData)\ndata(AfricanSoil)\nx &lt;- AfricanSoil[,-c(1,3595,3598)]\ndim(x)\n\n[1] 1157 3597\n\nxcov &lt;- cov(x)\neigs &lt;- eigen(xcov)\neigs$values[1:100]  # don't print out all 3597!\n\n  [1] 7.600042e+01 9.553189e+00 6.979629e+00 4.209555e+00 2.884526e+00\n  [6] 2.175115e+00 1.954537e+00 1.321853e+00 9.375446e-01 6.453537e-01\n [11] 6.360017e-01 5.484400e-01 4.535480e-01 3.981618e-01 3.665122e-01\n [16] 3.496067e-01 2.597679e-01 2.035621e-01 1.498785e-01 1.248452e-01\n [21] 1.119365e-01 9.545515e-02 8.083867e-02 6.415968e-02 6.079547e-02\n [26] 5.737305e-02 4.468220e-02 3.902647e-02 3.641190e-02 3.056959e-02\n [31] 2.541331e-02 2.037622e-02 1.819728e-02 1.532509e-02 1.297067e-02\n [36] 1.242883e-02 1.073280e-02 9.420125e-03 8.687909e-03 7.980043e-03\n [41] 7.178366e-03 5.445802e-03 4.893268e-03 4.027980e-03 3.903172e-03\n [46] 3.526363e-03 3.341049e-03 3.062258e-03 2.847189e-03 2.723814e-03\n [51] 2.586813e-03 2.315871e-03 2.142482e-03 1.977798e-03 1.771873e-03\n [56] 1.506873e-03 1.474780e-03 1.302539e-03 1.173394e-03 1.141518e-03\n [61] 1.004309e-03 9.438253e-04 8.794233e-04 8.217583e-04 8.137159e-04\n [66] 7.269301e-04 7.187134e-04 6.454513e-04 6.261121e-04 5.256722e-04\n [71] 4.603372e-04 4.256952e-04 4.116083e-04 3.873680e-04 3.802244e-04\n [76] 3.421665e-04 3.144433e-04 3.013745e-04 2.650906e-04 2.561422e-04\n [81] 2.553842e-04 2.436702e-04 2.110897e-04 1.937904e-04 1.851444e-04\n [86] 1.766814e-04 1.569518e-04 1.484314e-04 1.437515e-04 1.263429e-04\n [91] 1.242947e-04 1.154861e-04 1.135908e-04 1.113247e-04 1.003160e-04\n [96] 9.908341e-05 9.180042e-05 8.670191e-05 8.415325e-05 7.907993e-05\n\n\nAh, the eigenvalues fall off rapidly after the first few. So, let’s say we decide to use the first 9 \\(u\\) vectors.\n\neigvecs &lt;- eigs$vectors[,1:9]\ndim(eigvecs)\n\n[1] 3597    9\n\n\nSo, recalling the sequence of equations leading to Equation 11.1, we are ready to replace our old variables by the new ones, which are linear combinations of the old ones.\n\nx &lt;- as.matrix(x)\ndim(x)\n\n[1] 1157 3597\n\nxnew &lt;- x %*% eigvecs\ndim(xnew)  \n\n[1] 1157    9\n\nlmout &lt;- lm(AfricanSoil$pH ~ xnew)\n# and so on",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Principal Components</span>"
    ]
  },
  {
    "objectID": "Ch6b.html#sec-detect",
    "href": "Ch6b.html#sec-detect",
    "title": "12  Principal Components",
    "section": "12.4 PCA in Detection of Multicollinearity",
    "text": "12.4 PCA in Detection of Multicollinearity\nRecall the itemized list in Section 10.1 of various conditions under which we have multicollinearity. We can add the following item to that list:\n\n\\(A\\) has an eigenvalue that is nearly 0.\n\nThis follows from what we found about 0 eigenvalues in Section 12.2.\nIf an eigenvalue is 0, the rank is exactly less than full. If it is approximately 0, the rank is technically full, but there is a linear combination of the variables that is approximately 0.\nIn fact, PCA can warn us of specific linear combinations of \\(Cov(A'A)\\) that are nearly 0, as follows.\n\nSay a PC corresponds to a small eigenvalue.\nThen from Section 11.2.1 that linear combination has small variance.\nRandom variables with small variance are nearly constant.\nThus with high probability, the linear combination is near to \\(c\\) for some number \\(c\\). If we are working with numerical data, that number \\(c\\) is nonzero with high probability (see ?imp-prob0).\nIf our design matrix \\(A\\) includes a 1s column, then the above linear combination, together with this 1s column, gives us a linear combination that is usually close to the 0 vector, thus multicollinear.\n\nIn other words, PCA can point out to us specific linear combinations of our variables that may be problematic. If we are considering solving the problem by removing one or more predictor variables, this analysis could suggest which ones to remove.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Principal Components</span>"
    ]
  },
  {
    "objectID": "Ch6b.html#sec-prob0",
    "href": "Ch6b.html#sec-prob0",
    "title": "12  Principal Components",
    "section": "12.5 An Important Property of All-Numeric Predictor Data",
    "text": "12.5 An Important Property of All-Numeric Predictor Data\nSay we throw a dart at the interval (0,1). If the distribution of the hitting location \\(D\\) is U(0,1) or any other continuous distribution, the probability of hitting a specific point is 0, e.g. \\(P(D = 0.324) = 0\\). Such is the nature of continuous distributions. The problem is that the length of the line segment (0.324,0.324) is 0. In math, we say it has measure 0.\nIf we throw our dart at the unit square, \\((0,1) \\times (0,1)\\), then e.g. the probability that \\(D\\) lies on a specified straight line or some other specified curve is also 0. Here the problem is that the area of that line or curve is 0.\nThis has implications for material in this book. For instance, consider linear regression models. If our \\(A\\) matrix consists of all numeric data, then (in the absence of perfect correlation among the columns), we will have\n\\[\nP(determinant(A'A) = 0) = 0\n\\]\nfor the same 0-measure reasons. In other words, \\(A'A\\) will be invertible with probability 1.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Principal Components</span>"
    ]
  },
  {
    "objectID": "Ch6b.html#how-many-principal-components-should-we-use",
    "href": "Ch6b.html#how-many-principal-components-should-we-use",
    "title": "12  Principal Components",
    "section": "12.6 How Many Principal Components Should We Use?",
    "text": "12.6 How Many Principal Components Should We Use?\nWe chose to use the first 9 principal components in our example here, but that was just for illustration purposes. There are no formal rules for how many to use. Various “rules of thumb” exist.\nIf we are doing prediction, there is a very natural way to choose our number of PCs – do cross-validation (Section 10.4.2), and use whichever number gives the most accurate prediction.\n\n12.6.1 Example: New York City taxi trips\nThis is a well-known dataset, a version of which is included in the qeML package. The object is to predict trip time, given pickup and dropoff locations, trip distance and day of the week.\nIn the raw form of the data, there are just 5 columns, thus 4 predictors. But the pickup and dropoff locations are R factors, coding numerous locations. These must be decoded to dummy variables (values 1 or 0, coding whether or not, say, a given trip began at pickup location 121). If for instance one calls the R linear regression function lm, that function will do the conversion internally, but in our case we will perform the conversion ourselves, using regtools::factorsToDummies. (The regtools package is included by qeML.)\n\nlibrary(qeML)\ndata(nyctaxi)\ndim(nyctaxi)\n\n[1] 10000     5\n\nnyc &lt;- factorsToDummies(nyctaxi,dfOut=T)\ndim(nyc)\n\n[1] 10000   357\n\n\nWow! That’s quite a lot of predictors, and well in excess of the common rule of thumb that one should have no more than \\(\\sqrt{n}\\) predictors, 100 in this case.\nSo, we might try dimension reduction via PCA, then use the PCs as predictors. The function qeML::qePCA combines these two operations. Let’s see how it works.\n\nargs(qePCA)\n\nfunction (data, yName, qeName, opts = NULL, pcaProp, holdout = floor(min(1000, \n    0.1 * nrow(data)))) \nNULL\n\n\nHere qeName indicates which function is desired for prediction, e.g. qeLin for a linear model. (This function wraps lm.) But a key argument here is pcaProp. Recall that:\n\nThe PCs come in order of decreasing variance.\nWe are mainly interested in the first few PCs. The later ones have small variance, which makes them approximately constant and thus of no use to us.\n\nThe name ‘pcaProp’ stands for “proportion of total variance.” If we set this to, say, 0.25, we are saying “Give us whatever number of the first few PCs that have a total variance of at least 25% of the total.” Let’s give that a try:\nqePCA(nyc,'tripTime','qeLin',pcaProp=0.25)$testAcc\n[1] 311.9159\nWe asked to predict the column ‘tripTime’ in the dataset nyc using the qeLin function, based on as many of the PCs that will give us 25% of the total variance. Most qeML prediction functions split the data into a training set and a holdout set. The model is fit to the training set, and then applied to prediction of the holdout set. The output value is the mean absolute prediction error (MAPE).\nHowever, since the holdout set is randomly generated, the MAPE value is random, so we should do multiple runs. Some experimentation showed that MAPE here is highly variable, so we decided to perform 500 runs, e.g.\nmean(sapply(1:500,function(i) qePCA(nyc,'tripTime','qeLin',pcaProp=0.1)$testAcc)) \n[1] 359.663\n\nMean Absolute Predictive Error\n\n\npcaProb\nMAPE\n\n\n\n\n0.1\n359.6630\n\n\n0.2\n359.3068\n\n\n0.3\n403.9878\n\n\n0.4\n397.3783\n\n\n0.5\n430.8958\n\n\n0.6\n423.7299\n\n\n0.7\n517.7917\n\n\n0.8\n969.1304\n\n\n0.9\n2275.091\n\n\n\nAmong other things, this shows the dangers of overfitting, in this case using too many PCs in our linear regression model. It seems best here to use only the first 10 or 20% of the PCs.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Principal Components</span>"
    ]
  },
  {
    "objectID": "Ch6b.html#a-square-root-matrix-and-mv-normal-simulation",
    "href": "Ch6b.html#a-square-root-matrix-and-mv-normal-simulation",
    "title": "12  Principal Components",
    "section": "12.7 A “Square Root” Matrix, and MV Normal Simulation",
    "text": "12.7 A “Square Root” Matrix, and MV Normal Simulation\n\nTheorem 12.2 Any covariance matrix \\(A\\) has a “square root” matrix \\(Q\\), i.e. \\[\nQ^2 = A\n\\]\n\n\nProof. From Theorem 12.1, we have \\(P' A P = D\\) for some matrix \\(P\\) and diagonal matrix \\(D\\), with the entries of the latter being the variances of the PCs \\(\\sigma_i^2\\). That latter point implies that\n\\[\nD_1^2 = D\n\\]\nwhere \\(D_1 = diag(\\sigma_1,\\sigma_2,...,\\sigma_n)\\). Then setting \\(Q = P D_1 P'\\), we have\n\\[\nQ^2 = (P D_1 P') (P D_1 P') = P D_1^2 P' = P D P' = A\n\\]\n\n12.7.1 Implications for simulation of multivariate normal X\nSay we wish to write code to simulate a random vector \\(Q\\) having an m-variate normal distribution with mean vector \\(\\mu\\) and covariance matrix \\(\\Sigma\\). Here is how it works:\n\nWe start with generating \\(Z\\), a vector o \\(m\\) independent N(0,1) variables. That means \\(Z\\) is m-variate normally distributed, with mean vector consisting of \\(m\\) 0s, and covariance matrix \\(I\\), the \\(m \\times m\\) identity matrix.\nWe compute \\(W = \\Sigma^{0.5} Z\\). By Equation 4.6, \\(W\\) will again be multivariate normally distributed, with mean vector consisting of \\(m\\) 0s, and covariance matrix equal to\n\\[\n\\Sigma^{0.5} I \\Sigma^{0.5} = \\Sigma\n\\]\nOur solution is then\n\\[\nQ = \\mu + \\Sigma^{0.5} Z\n\\]\n\n\\(\\square\\)",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Principal Components</span>"
    ]
  },
  {
    "objectID": "Ch6b.html#your-turn",
    "href": "Ch6b.html#your-turn",
    "title": "12  Principal Components",
    "section": "12.8 Your Turn",
    "text": "12.8 Your Turn\n❄️ Your Turn: The reader has likely seen the concept of a cumulative distribution function (CDF). For a scalar random variable \\(X\\), this is \\(F_x(t) = P(X \\leq t)\\). If \\(X\\) is an \\(m\\)-dimensional random vector, the definition is\n\\[\nF_X(t_1,...,t_m) = P(X_1 \\leq t_1,...,X_m \\leq t_m)\n\\]\nWrite an R function with call form\nmultiCDF(mu,Sigma,t,n)\nthat uses simulation to evaluate the multivariate CDF, where \\(t =\n(t_1,...,t_m)\\) and \\(n\\) is the number of replications to simulate.\n❄️ Your Turn: Say \\(X\\) has mean vector \\(\\mu\\) and covariance matrix \\(\\Sigma\\). Show how we can use Theorem 12.2 to find a square, constant matrix \\(A\\) such that \\(Cov(AX) = I\\). If in addition \\(X\\) has a multivariate normal distribution, then \\(AX\\) will then consist of independent random variables with variance 1. Explain why.\n❄️ Your Turn: Modify the code for qePCA for the case of linear regression by adding a component xCoeffs to its return value. This will give the regression coefficients in terms of the original X predictors.\n❄️ Your Turn: Say the symmetric matrix \\(A\\) has block diagonal form\n\\[\nA =\n\\left (\n\\begin{array}{rrr}\nA_1 & 0 & 0 ... \\\\\n0 & A_2 & 0 ...  \\\\\n...\n\\end{array}\n\\right )\n\\]\nwhere \\(A_i\\) (\\(i=1,...,r)\\)\n\nis symmetric\nis of size \\(k_i \\times k_i\\)\nhas eigenvalues \\(\\gamma_{1},...,\\gamma_{k_i}\\)\nhas eigenvectors \\(u_{1,j},...,u_{k,j}\\), where \\(j=1,...,k_i\\)\n\nState the form of the eigenvalues and eigenvectors of \\(A\\).\n❄️ Your Turn: In Section 12.4, there was a statement “\\(c\\) is nonzero with high probability.” Why was that important?\n❄️ Your Turn: Use PCA to do dimension reduction on the s50 dataset.\n❄️ Your Turn: Write an R function with call form\nbestPCAPred(data,yName)\nIt will apply PCA to the X portion of data, then apply lm successivly to the first PC, then the first two, then the first three and so on assessing with cross-validation in each case. It will then return the number of PCs (and the PCs themselves) that predicts best. Try your function on various datasets.\n❄️ Your Turn: Say the symmetric \\(n \\times n\\) matrix \\(A\\) has rank \\(r\\). Show that \\(n-r\\) of its eigenvalues are 0s.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Principal Components</span>"
    ]
  },
  {
    "objectID": "Ch6c.html",
    "href": "Ch6c.html",
    "title": "13  Singular Value Decomposition",
    "section": "",
    "text": "13.1 Basic Idea\nHere is what we are aiming for.\nNote the dimensions:\nLet \\(r\\) denote the rank of \\(A\\). It will turn out that \\(\\sigma_i = 0\\) for \\(i=r+1,...,n\\).",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Singular Value Decomposition</span>"
    ]
  },
  {
    "objectID": "Ch6c.html#basic-idea",
    "href": "Ch6c.html#basic-idea",
    "title": "13  Singular Value Decomposition",
    "section": "",
    "text": "Our target relation:\n\n\n\nGiven an \\(m \\times n\\) matrix \\(A\\), we wish to find orthogonal matrices \\(U\\) and \\(V\\), and a matrix \\(\\Sigma = diag(\\sigma_1,...,\\sigma_n)\\) whose nonzero elements are positive, such that\n\\[\nA = U \\Sigma V'\n\\tag{13.1}\\]\nBy convention the ordering of columns is set so that the \\(\\sigma_i\\) occur in nonincreasing order.\nNote that, by matrix partitioning, Equation 13.1 also says that\n\\[\nA = \\sum_{i=1}^n \\sigma_i u_i v_i'\n\\tag{13.2}\\]\nwhere \\(u_i\\) and \\(v_i\\) are the \\(i^{th}\\) rows of \\(U\\) and \\(V\\).\n\n\n\n\n\\(U\\) is \\(m \\times n\\)\n\\(V\\) is \\(n \\times n\\)\n\\(\\Sigma\\) is \\(n \\times n\\)",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Singular Value Decomposition</span>"
    ]
  },
  {
    "objectID": "Ch6c.html#example-applications",
    "href": "Ch6c.html#example-applications",
    "title": "13  Singular Value Decomposition",
    "section": "13.2 Example Applications",
    "text": "13.2 Example Applications\n\n13.2.1 Recommender Systems\nRecall Section 2.10, which began with movie ratings data. We would like to predict the rating some particular user would give to some particular movie. It will turn out that we can set up SVD in such a way that \\(U\\) contains movie data and \\(V\\) contains user data.\n\n\n13.2.2 Text Classification\nAn oft-used example is that in which we have a collection of newspaper articles that we wish to categorize, say politics, sports, health, finance and so on. Say one of the articles includes the word bonds; is it referring to financial instruments, family relations, former baseball star Barry Bonds etc.?\nIn spite of today’s dazzling array of Large Language Models, a simple problem like this may be better solved using straightforward methods, such as SVD. We set up a document-term matrix, with element \\((i,j)\\) being 1 or 0, according to whether document \\(i\\) contains word \\(j\\). Applying SVD to this matrix, we have a setting with similar appeal to the recommender systems example above, in which \\(U\\) contains our data on documents and \\(V\\) does the same for words.\n\n\n13.2.3 Dimension Reduction\nRecall that in the matrix \\(\\Sigma\\), the eigenvalues are arranged in decreasing order, possibly followed by 0s. This suggests that we can achieve dimension reduction by replacing even some of the smaller eigenvalues by 0s as well, with corresponding adjustments to the columns of \\(U\\) and \\(V\\). We will discuss this below in Section 13.7.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Singular Value Decomposition</span>"
    ]
  },
  {
    "objectID": "Ch6c.html#solution-to-our-svd-goal",
    "href": "Ch6c.html#solution-to-our-svd-goal",
    "title": "13  Singular Value Decomposition",
    "section": "13.3 Solution to Our SVD Goal",
    "text": "13.3 Solution to Our SVD Goal\nThere are various derivations, e.g. along the lines of Section 11.2.1, but let’s go directly to the answer:\n\nSay \\(A\\) is \\(p \\times n\\), so that \\(A'A\\) will be of size \\(n\n\\times n\\). Let \\(r\\) denote the rank of \\(A\\), which will be the same as the rank of \\(A\\) from Theorem 6.5.\nCompute the eigenvalues \\(\\sigma_1,...,\\sigma_n\\) and eigenvectors \\(v_1,...,v_n\\) of \\(A'A\\). Normalize the \\(v_i\\) by dividing by their lengths. (The same eigenvalues will still hold.) Order the \\(\\sigma_i\\) from largest to smallest, and use the same ordering for the \\(v_i\\).\nSince \\(A'A\\) is symmetric and positive-semidefinite, its eigenvalues will be nonnegative and its eigenvectors \\(v_1,...,v_r\\) will be orthogonal as long as \\(\\sigma_1,...,\\sigma_r\\) are distinct, as is the case for numeric data (Section 12.5). Since these eigenvectors diagonalize \\(A'A\\) with the \\(\\sigma_i\\) on the diagonal (Chapter 12), we will have \\(\\sigma_{r+1} = ...  = \\sigma_n = 0\\).\nSet \\(\\Sigma\\) to \\(diag(\\sigma_1,...,\\sigma_n)\\), the nonincreasing list of those eigenvalues. Set the first \\(r\\) columns of \\(V\\) to the corresponding eigenvectors. Use the Gram-Schmidt Method (see Section 8.10) to add \\(n-r\\) more vectors. \\(V\\) will then have orthonormal columns, and thus be an orthogonal matrix.\nSet\n\\[ u_i = \\frac{1}{\\sigma_i} Av_i, ~ i=1,...,r \\]\nThe \\(u_i\\) will be orthogonal: For \\(i \\neq j\\),\n\\[\nu_i' u_j = \\frac{1}{\\sigma_i \\sigma_j} v_i' A'A v_j =\n\\frac{1}{\\sigma_i \\sigma_j} v_i' v_j = 0\n\\]\nwhere we have used the facts that \\(v_j\\) is an eigenvector of \\(A'A\\) and the \\(v_k\\) are orthogonal.\nUsing Gram-Schmidt, we can compute (if \\(r &lt; n\\) necessitates it) vectors \\(u_{r+1},...u_n\\) so that \\(u_1,...,u_n\\) is an orthonormal basis for \\(\\mathcal{R}^m\\). Set \\(U\\), decribed in partitioning terms: to\n\\[ U = (u_1|...|u_n) \\]",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Singular Value Decomposition</span>"
    ]
  },
  {
    "objectID": "Ch6c.html#svd-as-a-basis-for-matrix-generalized-inverse",
    "href": "Ch6c.html#svd-as-a-basis-for-matrix-generalized-inverse",
    "title": "13  Singular Value Decomposition",
    "section": "13.4 SVD as a basis for matrix generalized inverse",
    "text": "13.4 SVD as a basis for matrix generalized inverse\nRecall the example in Section 6.1. We could not have dummy-variable columns for both male and female, as their sum would be a column of all 1s, in addition to a column of 1s the X data matrix already had. The three columns would then have a nonzero linear combination that evaluates to the 0 vector. Then in Equation 5.5, \\(A\\) (i.e. X) would not be of full rank, nor would \\(A'A\\) (Theorem 6.5), so that \\((A'A)^{-1}\\) would not exist.\nAnd yet the equation from which that comes,\n\\[\nA'A b = A'S\n\\tag{13.3}\\]\nis still valid. We could, as in that example, remove one of the gender columns, thus solving the problem of less than full rank, but the use of generalized inverses solves the problem directly. If \\(A\\) has many binary variables, the use of generalized inverses may be more convenient.\nWe will omit the formal definition of generalzed inverses and their properties, returning to the topic in Chapter 14. For now, we note the following:\nOne of the most famous forms of generalized inverse, is the Moore-Penrose pseudoinverse, based on SVD. Given the SVD of a matrix \\(M\\),\n\\[\nM = U_M \\Sigma_M V_{M}'\n\\]\nits Moore-Penrose inverse, denoted by \\(M^{+}\\), is\n\\[\nM^{+} = V_M \\Sigma_M^{+} U_{M}'\n\\]\nwhere \\(\\Sigma_M^{+}\\) is the diagonal matrix obtained from \\(\\Sigma_M\\) by replacing each nonzero element by its reciprocal.\nThe Moore-Penrose solution of \\(Mz = w\\) for vectors \\(z\\) and \\(w\\), is\n\\[\nz = M^{+} w\n\\tag{13.4}\\]\nThe R function MASS::ginv performs the necessary computation for us; we need not call svd().",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Singular Value Decomposition</span>"
    ]
  },
  {
    "objectID": "Ch6c.html#svd-in-linear-models",
    "href": "Ch6c.html#svd-in-linear-models",
    "title": "13  Singular Value Decomposition",
    "section": "13.5 SVD in linear models",
    "text": "13.5 SVD in linear models\nNow apply this to our linear model problem (Equation 13.3). We claim that\n\\[\nb = A^{+} S = V \\Sigma^{+} U' S\n\\]\nsolves the equation. (Actually if \\(A\\) is of less than full rank, there are infinitely many solutions, a point discussed in detail in Chapter 14.)\nLet’s check. Before beginning, recall that the orthogonal nature of \\(U\\) and \\(V\\) implies that \\(U'U = I\\) and \\(V'V = I\\).\n\\[\n\\begin{aligned}\nA'A b &= (U \\Sigma V')' (U \\Sigma V') (U \\Sigma V')^{+} S \\\\\n&= (V \\Sigma U') (U \\Sigma V') (V \\Sigma^{+} U' S) \\\\\n&= V \\Sigma^2 V'(V \\Sigma^{+} U' S) \\\\\n&= V \\Sigma U' S \\\\\n&= A'S\n\\end{aligned}\n\\]\n\\(\\square\\)",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Singular Value Decomposition</span>"
    ]
  },
  {
    "objectID": "Ch6c.html#example-census-data",
    "href": "Ch6c.html#example-census-data",
    "title": "13  Singular Value Decomposition",
    "section": "13.6 Example: Census Data",
    "text": "13.6 Example: Census Data\n\nlibrary(qeML)\ndata(svcensus)\nhead(svcensus)\n\n       age     educ occ wageinc wkswrkd gender\n1 50.30082 zzzOther 102   75000      52 female\n2 41.10139 zzzOther 101   12300      20   male\n3 24.67374 zzzOther 102   15400      52 female\n4 50.19951 zzzOther 100       0      52   male\n5 51.18112 zzzOther 100     160       1 female\n6 57.70413 zzzOther 100       0       0   male\n\nsvc &lt;- svcensus[,-c(2,3)]\n# have only 1 categorical/dichotomous variable, for simple example\nsvc &lt;- factorsToDummies(svc)\nhead(svc)\n\n          age wageinc wkswrkd gender.female gender.male\n[1,] 50.30082   75000      52             1           0\n[2,] 41.10139   12300      20             0           1\n[3,] 24.67374   15400      52             1           0\n[4,] 50.19951       0      52             0           1\n[5,] 51.18112     160       1             1           0\n[6,] 57.70413       0       0             0           1\n\nx &lt;- cbind(1,svc[,-2])\nhead(x)\n\n            age wkswrkd gender.female gender.male\n[1,] 1 50.30082      52             1           0\n[2,] 1 41.10139      20             0           1\n[3,] 1 24.67374      52             1           0\n[4,] 1 50.19951      52             0           1\n[5,] 1 51.18112       1             1           0\n[6,] 1 57.70413       0             0           1\n\nxplus &lt;- MASS::ginv(x)\nbhat &lt;- xplus %*% svc[,2]\nbhat\n\n           [,1]\n[1,] -16022.454\n[2,]    496.747\n[3,]   1372.756\n[4,] -13361.634\n[5,]  -2660.821\n\nlm(wageinc ~ .,svcensus[,-c(2,3)])$coef\n\n(Intercept)         age     wkswrkd  gendermale \n -29384.088     496.747    1372.756   10700.813 \n\n\nSince the two analyses use different sets of predictors, it is not surprising that the intercept terms differ, but otherwise the two approaches are consistent with each other. This includes the gender variables, since\n\\[\n-13361.634-(-2660.821) = -10700.81\n\\]",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Singular Value Decomposition</span>"
    ]
  },
  {
    "objectID": "Ch6c.html#sec-svddimred",
    "href": "Ch6c.html#sec-svddimred",
    "title": "13  Singular Value Decomposition",
    "section": "13.7 Dimension Reduction: SVD as the Best Low-Rank Approximation",
    "text": "13.7 Dimension Reduction: SVD as the Best Low-Rank Approximation\nThe SVD of a rank \\(r\\), \\(q \\times n\\) matrix \\(A\\) can be partitioned as\n\\[\n(U_1 | U_2)\n\\left (\n\\begin{array}{rr}\nD_1 & 0 \\\\\n0 & 0  \\\\\n\\end{array}\n\\right )\n(V_1 | V2)'\n\\]\nwhere \\(U_1\\) is of size \\(q \\times r\\), \\(V_1\\) is of size \\(n \\times r\\), and \\(D_1\\) is \\(r \\times r\\). Simplifying, we have\n\\[\nA = (U_1 D_1 | 0)\n\\left (\n\\begin{array}{r}\nV_1' \\\\\nV_2' \\\\\n\\end{array}\n\\right ) =\nU_1 D_1 V_1'\n\\tag{13.5}\\]\nSo, we’ve reduced the size of memory needed to store the SVD. But there’s more:\nThe eigenvalues of a matrix, arranged from largest to smallest, tend to degrade in a gradual manner, as seen for example in Section 12.3. So, in the context of SVD, with the last \\(n-r\\) eigenvalues being 0s, it will typically be the case that the last few eigenvalues among \\(\\sigma_1,...,\\sigma_r\\) are near 0.\nIn other words, we probably can get a good approximation to the SVD by treating those near-0 eigenvalues as 0s, and removing the corresponding columns of \\(U_1\\) and \\(V_1\\). Why do this?\n\nAchieve a further reduction in storage requirements. for instance in settings in which we have many, many images or videos.\nReduce noise, again for example with images. Small blotches are smoothed out.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Singular Value Decomposition</span>"
    ]
  },
  {
    "objectID": "Ch6c.html#example-image-compression",
    "href": "Ch6c.html#example-image-compression",
    "title": "13  Singular Value Decomposition",
    "section": "13.8 Example: Image Compression",
    "text": "13.8 Example: Image Compression",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Singular Value Decomposition</span>"
    ]
  },
  {
    "objectID": "Ch6c.html#matrix-factorization-in-recommender-systems",
    "href": "Ch6c.html#matrix-factorization-in-recommender-systems",
    "title": "13  Singular Value Decomposition",
    "section": "13.9 Matrix Factorization in Recommender Systems",
    "text": "13.9 Matrix Factorization in Recommender Systems\nLet \\(R\\) denote the ratings matrix, so that the element in row \\(i\\), column \\(j\\) is the rating user \\(i\\) gives to item \\(j\\). Note that most elements of \\(A\\) are unknown, and we hope to predict them with some reasonable amount of accuracy.\nWrite the SVD:\n\\[\nA = U \\Sigma V' = (U \\Sigma^{0.5}) (\\Sigma^{0.5} V') = WH\n\\]\nwhere \\(\\Sigma^{0.5}\\) is the diagonal matrix with elements \\(\\sqrt{\\sigma_i}\\).\nAgain, since we do not know all of \\(A\\), we do not know any of the matrices on the right either. We will return to this problem shortly, but for now pretend the matrices are known.\nThe point is that we have factored \\(A\\) into the product of a matrix \\(W\\) containing information about the users’ ratings and a matrix \\(H\\) that does the same for items. Note that the row \\(i\\), column \\(j\\) element of \\(A\\) is equal to \\(w_i h_j\\), where \\(w_i\\) is row \\(i\\) of \\(W\\) and \\(v_j\\) is column \\(j\\) of \\(H\\).\nThis suggests that the following iterative process may work:\n\nReplace the unknown elements of \\(A\\) by some temporaru values. This could be all 0s, say, or maybe replacing all unknown values in column \\(j\\) by the mean of the intact values in that column.\nFind \\(W\\) and \\(H\\) for that temporary version of \\(A\\), our guess.\nUse the formula \\(w_i h_j\\) to find the unknown elements, updating accordingly to a new guess for \\(A\\).\nGo to Step 2.\n\nWe iterate until convergence.\nAnd though SVD provided the motivation for the model \\(A = WH\\), we can use the model more generally, i.e. without assuming \\(W = U \\Sigma^{0.5}\\) and \\(H = \\Sigma^{0.5} V'\\).\nlme4, ALS with ridge (and mention the 2-way regul standard)",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Singular Value Decomposition</span>"
    ]
  },
  {
    "objectID": "Ch6c.html#svd-as-a-basis-for-the-four-fundamental-subspaces",
    "href": "Ch6c.html#svd-as-a-basis-for-the-four-fundamental-subspaces",
    "title": "13  Singular Value Decomposition",
    "section": "13.10 SVD As a Basis for the Four Fundamental Subspaces",
    "text": "13.10 SVD As a Basis for the Four Fundamental Subspaces\nRecall the four subspaces discussed in Chapter 9. We can quickly obtain much information about them from the SVD.\nLet \\(u_i\\) and \\(v_i\\) denote the colums of \\(U\\) and \\(V\\). Then:\n\n\\(u_1,...,u_r\\) is an orthonormal basis for \\(\\mathcal{C}(A)\\)\n\\(u_{r+1},...,u_m\\) is an orthonormal basis for \\(\\mathcal{R}(A)\\)\n\\(v_1,...,v_r\\) is an orthonormal basis for \\(\\mathcal{R}(A)\\)\n\\(v_{r+1},...,v_n\\) is an orthonormal basis for \\(\\mathcal{N}(A)\\)",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Singular Value Decomposition</span>"
    ]
  },
  {
    "objectID": "Ch6c.html#your-turn",
    "href": "Ch6c.html#your-turn",
    "title": "13  Singular Value Decomposition",
    "section": "13.11 Your Turn",
    "text": "13.11 Your Turn\n❄️ Your Turn: Show that in the SVD factorization, \\(U\\) consists of the eigenvectors of \\(AA'\\).\n❄️ Your Turn: Find a characterization of the left null space of a matrix \\(A\\).\n❄️ Your Turn: Write an R function will call form\ngetLowRank(A,lowrank)\nthat uses SVD to find the best approximation to the matrix A of rank lowrank.\n❄️ Your Turn: Our equation Equation 3.6 resulted from replacing one row (the last, but could have been any) by all 1s, with a corresponding change on the right-hand side. But now that we have pseudoinverses at our disposal, we could add a row rather than replacing a row:\n\\[\n\\left (\n\\begin{array}{r}\nP \\\\\n1 \\\\\n\\end{array}\n\\right ) \\nu =\n\\left (\n\\begin{array}{r}\n0 \\\\\n1 \\\\\n\\end{array}\n\\right )\n\\]\nwhere 1 on the left side means a row of 1s, 0 on the right side is a column of 0s, and 1 on the right side means the scalar 1.\nWrite an R function with call form\nfindNuPseudoInv(p)\nto implement this approach. Here p is a Markov transition matrix. Try your code on a couple of examples.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Singular Value Decomposition</span>"
    ]
  },
  {
    "objectID": "Ch6d.html",
    "href": "Ch6d.html",
    "title": "14  Pseudoinverse and Double Descent",
    "section": "",
    "text": "14.1 SVD as the Minimum-Norm Solution\nIn an overdetermined linear system such as Equation 13.3, there are many solutions. However, an advantage of Moore-Penrose is that it gives us the minimum norm solution. We’ll discuss the significance of this shortly, but let’s prove it first.\nWe will need this:\nto add:\npossible reasons:\nat interp, min norm suddenly gives more than 1 choice; min norm is like min Var; maybe show unbiased by arguing A’A b = A’S for all A, thus A’E(estreg) = A’beta’Y for all A etc.; this of course holds only for estimable x’beta, meaning x is in row space of A (or equiv, A’A)\ncondition number is max at interpolation\n(size of?) second U depends on min nonzero eigenvalue\nmy empirical example\novrf &lt;- function(nreps,n,maxP) { load(‘YearData.save’) # yr &lt;- yr[,1:2] nas &lt;- rep(NA,nreps*(maxP-1)) outdf &lt;- data.frame(p=nas,mape=nas) rownum &lt;- 0 for (i in 1:nreps) { idxs &lt;- sample(1:nrow(yr),n) trn &lt;- yr[idxs,] tst &lt;- yr[-idxs,] for (p in 2:maxP) { rownum &lt;- rownum + 1 # out&lt;-qePolyLin(trn[,1:(p+1)], # ‘V1’,2,holdout=NULL) out &lt;- qeLin(trn[,1:(p+1)],‘V1’,2,holdout=NULL) preds &lt;- predict(out,tst[,2:(p+1)]) mape &lt;- mean(abs(preds - tst[,1])) outdf[rownum,1] &lt;- p outdf[rownum,2] &lt;- mape print(outdf[rownum,]) } } outdf #run through tapply() for the graph }",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Pseudoinverse and Double Descent</span>"
    ]
  },
  {
    "objectID": "Ch6d.html#svd-as-the-minimum-norm-solution",
    "href": "Ch6d.html#svd-as-the-minimum-norm-solution",
    "title": "14  Pseudoinverse and Double Descent",
    "section": "",
    "text": "Theorem 14.1 (Multiplication by an Orthogonal Matrix Preserves Norm) For an orthogonal matrix \\(M\\) and a vector \\(w\\), \\(||Mw|| = ||w||\\).\n\n\nProof. See the Your Turn problem below.\n\n\nTheorem 14.2 (The Moore-Penrose Solution Is Min-Norm) Consider a matrix \\(B\\) and vector \\(q\\). Of all solutions \\(x\\) to\n\\[\nBx = q\n\\tag{14.1}\\]\nthe Moore-Penrose solution minimizes \\(||x||\\).\n\n\nProof. Again, writeAdapted from a derivation by Carlo Tomasi\n\\[\nB = U \\Sigma V',\n\\]\nand consider the residual sum of squares\n\\[\n||Bx - q||^2 = ||U (\\Sigma V' x - U'q)||^2\n\\tag{14.2}\\]\nsince \\(UU'=I\\).\nThus we can remove the factor \\(U\\) in Equation 14.2, yielding\n\\[\n||Bx - q||^2 = ||\\Sigma V' x - U'q||^2\n\\tag{14.3}\\]\nRename \\[V'x\\] to \\(y\\):\n\\[\n||Bx - q||^2 = ||\\Sigma y - U'q||^2\n\\tag{14.4}\\]\nNow bring in the fact that \\(\\sigma_i = 0\\) for \\(i &gt; r\\), by writing everything in partitioned fashion. Break \\(U\\) into \\(r\\) and \\(n-r\\) rows,\n\\[\nU =\n\\left (\n\\begin{array}{r}\nU_1 \\\\\nU_2 \\\\\n\\end{array}\n\\right )\n\\]\nand partition other objects similarly:\n\\[\nV = (V_1 | V_2),\n\\]\n\\[\n\\Sigma =\n\\left (\n\\begin{array}{rr}\n\\Sigma_1 & 0 \\\\\n0 & 0  \\\\\n\\end{array}\n\\right ),\n\\]\nand\n\\[\ny =\n\\left (\n\\begin{array}{r}\ny_1 \\\\\ny_2 \\\\\n\\end{array}\n\\right )\n\\]\nSubstituting into Equation 14.4, we have\n\\[\n||Bx - q||^2\n= ||\n\\left (\n\\begin{array}{r}\n\\Sigma_1 y_1 \\\\\n0 \\\\\n\\end{array}\n\\right ) -\n\\left (\n\\begin{array}{r}\nU_1q \\\\\nU_2q \\\\\n\\end{array}\n\\right )\n||^2\n\\tag{14.5}\\]\nThis means that \\(y_2\\) can be anything at all, without changing the residual sum of squares, confirming that there are infinitely many equally-effective solutions!\nBut if we want \\(x\\) to be of minimum length, we need \\(y\\) to be of minimum length (the shortest \\(y\\) gives us the shortest \\(x = Vy\\), again by the Your Turn problem). And,\n\\[\n||y||^2 = ||y_2||^2 + ||y_2||^2\n\\]\nThus we take \\(y_2 = 0\\).\nNow, reviewing Equation 14.6, note that the equation\n\\[\n\\Sigma_1 y_1 = U_1 q\n\\]\nhas the solution\n\\[\ny_1 = \\Sigma_1^{-1} U_1 q\n\\]\nAn interesting sidelight of this is that it means that\n\\[\n||Bx - q||^2\n= ||\n\\left (\n\\begin{array}{r}\n0  \\\\\nU_2q \\\\\n\\end{array}\n\\right )\n||^2,\n\\tag{14.6}\\]\ni.e. the residual sum of squares depends only on \\(U_2\\), not \\(U_1\\). But returning to our quest for the minimum-norm solution, we map back to \\(x = Vy\\), and have\n\\[\nx_1 = V_1 \\Sigma_1^{-1} U_1'q\n\\]\nwhich is the SVD solution! Thus the SVD solution does have minimum norm.\n\\(\\square\\)",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Pseudoinverse and Double Descent</span>"
    ]
  },
  {
    "objectID": "Ch6d.html#application-explaining-the-mystery-of-double-descent",
    "href": "Ch6d.html#application-explaining-the-mystery-of-double-descent",
    "title": "14  Pseudoinverse and Double Descent",
    "section": "14.2 Application: Explaining the Mystery of ``Double Descent”",
    "text": "14.2 Application: Explaining the Mystery of ``Double Descent”\nAround 2018-2019, one of the statistics field’s most deeply-held notions was thrown off its pedestal, largely by some researchers in machine learning. (This is arguably when the idea first became widespread, but for earlier instances, see Marco Loog et al, A brief prehistory of double descent, PNAS, 2020.) And many of the explanations given have been related to SVD.\n\n14.2.1 Motivating example\nLet’s consider the Million Song Dataset from Section 10.2.\n\n\n14.2.2 Overfitting and BEYOND\nRecall that a well-known rule of thumb is that the number \\(p\\) of predictors should be less than \\(\\sqrt{n}\\), where \\(n\\) is the number of data points. We will abandon this and see what happens.\nWe will add predictors one at a time, to form models of increasing complexity. As usual, let \\(p\\) denote the number of predictors in a given model. We will also use only a small number of rows, say \\(n = 30\\). We start with \\(p=1\\), then \\(p=2\\), eventually reaching \\(p = n-1\\) (accounting for the 1s column in the \\(A\\) matrix in Equation 5.7), and not stopping even there. We go to \\(p=n\\), \\(p=n+1\\) and so on.\nWhen we reach \\(p=n+2\\), the matrix \\(A\\) will have more columns than rows, and \\((A'A)^{-1}\\) will not exist. (See Your Turn problem below.) We can still use SVD to solve for \\(b\\), but intuitively some kind of major change may happen at that point.\nHere is the code and output\novrf &lt;- function(nreps,n,maxP)\n {\n    load('YearData.save')\n    # yr &lt;- yr[,1:2]\n    nas &lt;- rep(NA,nreps*(maxP-1))\n    outdf &lt;- data.frame(p=nas,mape=nas)\n    rownum &lt;- 0\n    for (i in 1:nreps) {\n       idxs &lt;- sample(1:nrow(yr),n)\n       trn &lt;- yr[idxs,]\n       tst &lt;- yr[-idxs,]\n       for (p in 2:maxP) {\n          rownum &lt;- rownum + 1\n          # don't use lm or qeLin, since no SVD; instead, fit polynomial\n          # of degree 1, i.e. linear model\n          out &lt;- qePolyLin(trn[,1:(p+1)],'V1',1,holdout=NULL)\n          preds &lt;- predict(out,tst[,2:(p+1)])\n          mape &lt;- mean(abs(preds - tst[,1]))\n          outdf[rownum,1] &lt;- p\n          outdf[rownum,2] &lt;- mape\n          print(outdf[rownum,])\n       }\n    }\n    outdf  #run through tapply() for the graph\n }\n&gt; set.seed(111111); o1 &lt;- ovrf(1,30,40)\n  p     mape\n1 2 7.865117\n  p     mape\n2 3 7.876853\n  p     mape\n3 4 7.984233\n  p     mape\n4 5 8.631492\n  p     mape\n5 6 8.608561\n  p     mape\n6 7 8.734537\n  p    mape\n7 8 8.74647\n  p     mape\n8 9 8.746996\n   p     mape\n9 10 9.042233\n    p     mape\n10 11 8.795864\n    p    mape\n11 12 8.88431\n    p     mape\n12 13 10.06747\n    p     mape\n13 14 10.17324\n    p     mape\n14 15 10.68381\n    p     mape\n15 16 10.41553\n    p     mape\n16 17 9.892437\n    p    mape\n17 18 9.71409\n    p     mape\n18 19 11.25314\n    p     mape\n19 20 11.88647\n    p     mape\n20 21 17.94607\n    p     mape\n21 22 18.63847\n    p     mape\n22 23 15.26327\n    p     mape\n23 24 19.50255\n    p     mape\n24 25 24.53056\n    p     mape\n25 26 23.60238\n    p     mape\n26 27 25.35561\n    p    mape\n27 28 36.1927\n    p     mape\n28 29 57.97567\n    p     mape\n29 30 754.9635\nP &gt; N. With polynomial terms and interactions, P is 31.\n\n\n    p    mape\n30 31 741.573\nP &gt; N. With polynomial terms and interactions, P is 32.\n\n\n    p  mape\n31 32 477.3\nP &gt; N. With polynomial terms and interactions, P is 33.\n\n\n    p     mape\n32 33 525.8362\nP &gt; N. With polynomial terms and interactions, P is 34.\n\n\n    p     mape\n33 34 465.0518\nP &gt; N. With polynomial terms and interactions, P is 35.\n\n\n    p     mape\n34 35 761.3692\nP &gt; N. With polynomial terms and interactions, P is 36.\n\n\n    p    mape\n35 36 689.829\nP &gt; N. With polynomial terms and interactions, P is 37.\n\n\n    p     mape\n36 37 673.8049\nP &gt; N. With polynomial terms and interactions, P is 38.\n\n\n    p     mape\n37 38 728.8918\nP &gt; N. With polynomial terms and interactions, P is 39.\n\n\n    p     mape\n38 39 666.2301\nP &gt; N. With polynomial terms and interactions, P is 40.\n\n\n    p     mape\n39 40 621.9375\nMAPE here is mean absolute prediction error (calculated on the holdout set).\nHere \\(n = 30\\), so \\(p = 30, 31,...\\) uses SVD, and is overfitting. Indeed, much smaller values of \\(p\\) seem to have overfit as well. But the point is that once we got to the point at which there is no unique solution, MAPE actually improved, a huge shock to the field. It was always assumed that there is no point going beyond the values of \\(p\\) that gives us 0 for the sum of squares.\nThe graph of MAPE is typically a U-shape. In this case, we seem to have only the right half of a U, but still a U. What was shocking was that sometimes there is a second U-shape after we reach 0 sum of squares. Even more shocking, in some case the low point of the second U is lower than that of the first–it pays to radically ovefit.\n\n\n14.2.3 The classic and modern views\nIn other words, here is the sea change that occurred in the field around 2018-2019.\nLet \\(\\kappa\\) denote the complexity of a model. In the case of a linear model, \\(\\kappa\\) would be the number of predictor columns in our dataset, and there are ways of defining it for other methods.\nWe might try several values of \\(\\kappa\\), and then choose the one with smallest MAPE or other accuracy measure.\nBefore 2018-2019, the view was:\n\nIn plotting MAPE against \\(\\kappa\\), the curve will generally be roughly U-shaped, up to the point at which \\(\\kappa\\) gives us a 0 value of RSS in the training set. That value of \\(\\kappa\\), called the interpolation point, will give us “perfect” prediction in the training set, but very poor prediction in the test set, which is what counts.\nWe choose the value of \\(\\kappa\\) at which the curve is lowest. There is no point in trying values of \\(\\kappa\\) past the interpolation point.\n\nThis was taken for granted throughout the statistics and machine learning fields. But around 2018-2019, machine learning engineers were routinely analyzing dataset of extraordinarily large sizes, using unprecedently large values of \\(\\kappa\\). And they discovered that, bizarrely, as \\(\\kappa\\) moved past the interpolation point, the curve often went down–the second “descent”–often tracing its own second U-shape. And most significantly, the low point of the second U was sometimes below that of the first U! Overfitting–grossly so–may pay off!\nSo the modern view is:\n\nThe first U-shaped curve is as in the classic view. But the curve should be plotted past \\(\\kappa\\), and the overall minimum may be in that second U.\n\nThis is a rare sea change in classic quantitative analysis.\n\n\n14.2.4 How can this bizarre effect occur?\nLet’s look further. If at least some our predictors are numeric (with Million Song, they all are), then for \\(p = n-1\\), the (square) matrix \\(A\\) itself will very likely be invertible, and setting\n\\[\nb = A^{-1} S\n\\]\nwill minimize Equation 5.2–with the value of that expression being 0–a perfect fit to the data!\nNow again consider \\(p=n\\). Let \\(A_{new}\\) denote our new \\(A\\) (it will be equal to the old one with a new column added on the right), and \\(b_{new}\\) denote a new version of \\(b\\). We say “a new version” here, because there are now infinitely many versions; recall that the SVD version has minimum norm among them, a point we will return to shortly. Let’s use \\(A_{old}\\) and \\(b_{ol}\\) to denote the quantities we got for \\(p=n-1\\).\nWe can write\n\\[\nA_{new} =\n(A_{old} | c_{n+1})\n\\]\nand\n\\[\nb_{new} = (b_{old} | b_{n+1})'\n\\]\nThus\n\\[\nA_{new}' A_{new} b_{new} =\n(A_{old}' A_{old} + c_{n+1} c_{n+1}')\n\\left (\n\\begin{array}{r}\nb_{old} \\\\\nb_{n+1} \\\\\n\\end{array}\n\\right )\n\\]\nAs noted, we now must use SVD. We will still get a perfect fit. To see this, set \\(b_{n+1} = 0\\) above, which gives us the same fit we got for \\(n\n= p-1\\). In fact, there will be infinitely many values of \\(b\\) that achieve that perfect fit. But remember, the SVD solution has minimum norm among them all, a point we will return to shortly.\nIn considering the effectiveness of an estimator in statistics, we often look at bias and variance. Let’s consider bias first.\nIn our setting here, the quantities of interest are of the form \\(w'\\beta\\) the predicted value for an individual who has the characterics \\(w\\). We don’t know \\(\\beta\\), so we use \\(w'b\\) instead. , estimated by \\(w'b\\). In the non-full rank setting, not all such quantities are physically estimable, but the theory says that any \\(w\\) in the row space of \\(A\\) will be fine. Moreover, it says the least squares estimate of \\(w'\\beta\\) will have 0 bias.\nNow concerning variance, we have\n\\[\nVar(w'b) = w' Cov(b) w\n\\]\nIn general, shorter random vectors will have less variability, which though not the same as variance/covariance at least gives us the feeling that such an estimator has low variance. Recall that this is the rationale for shrinkage estimators – accept a small amount of bias in return for a reduction in variance.\nThus it is at least plausible that we might get a second U-shape, as the impact of the minimum-norm nature of SVD kicks in. On the other hand, as \\(p\\) grows further, \\(b\\) has more and more components, and likely that minmum-norm \\(b\\) will grow in length at some point, hence the typical later turn back upward of the second U.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Pseudoinverse and Double Descent</span>"
    ]
  },
  {
    "objectID": "Ch6d.html#your-turn",
    "href": "Ch6d.html#your-turn",
    "title": "14  Pseudoinverse and Double Descent",
    "section": "14.3 Your Turn",
    "text": "14.3 Your Turn\n❄️ Your Turn: Show that for an orthogonal matrix \\(M\\) and a vector \\(w\\), \\(||Mw|| = ||w||\\).\n❄️ Your Turn: Using properties of matrix rank, show that if \\(p+1 &gt; n\\) in Equation 5.7, the inverse will not exist.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Pseudoinverse and Double Descent</span>"
    ]
  },
  {
    "objectID": "Ch6d.html#your-turn-1",
    "href": "Ch6d.html#your-turn-1",
    "title": "14  Pseudoinverse and Double Descent",
    "section": "14.4 Your Turn",
    "text": "14.4 Your Turn",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Pseudoinverse and Double Descent</span>"
    ]
  },
  {
    "objectID": "Ch8a.html",
    "href": "Ch8a.html",
    "title": "15  Neural Networks",
    "section": "",
    "text": "15.1 Tensors",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Neural Networks</span>"
    ]
  },
  {
    "objectID": "Ch8a.html#sgd",
    "href": "Ch8a.html#sgd",
    "title": "15  Neural Networks",
    "section": "15.2 SGD",
    "text": "15.2 SGD",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Neural Networks</span>"
    ]
  },
  {
    "objectID": "Ch8a.html#sgd-and-double-descent",
    "href": "Ch8a.html#sgd-and-double-descent",
    "title": "15  Neural Networks",
    "section": "15.3 SGD and Double Descent",
    "text": "15.3 SGD and Double Descent",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Neural Networks</span>"
    ]
  },
  {
    "objectID": "Ch8a.html#low-rank-approximation-of-weight-matrices",
    "href": "Ch8a.html#low-rank-approximation-of-weight-matrices",
    "title": "15  Neural Networks",
    "section": "15.4 Low Rank Approximation of Weight Matrices",
    "text": "15.4 Low Rank Approximation of Weight Matrices",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Neural Networks</span>"
    ]
  },
  {
    "objectID": "Ch8a.html#role-of-matrix-condition-number",
    "href": "Ch8a.html#role-of-matrix-condition-number",
    "title": "15  Neural Networks",
    "section": "15.5 Role of Matrix Condition Number",
    "text": "15.5 Role of Matrix Condition Number\nhttps://math.stackexchange.com/questions/4362666/relationship-between-condition-number-of-a-matrix-and-eigenvalues",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Neural Networks</span>"
    ]
  }
]