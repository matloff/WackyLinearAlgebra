[
  {
    "objectID": "preface.html#subtlety-in-the-title",
    "href": "preface.html#subtlety-in-the-title",
    "title": "Preface",
    "section": "Subtlety in the Title",
    "text": "Subtlety in the Title\nLet’s start with the title of this book, Powered by Linear Algebra: The central role of matrices and vector spaces in Data Science. It’s important to understand why the title is NOT “Linear Algebra for Data Scientists.” That latter would connote that people in Data Science (DS)will first learn linear algebra purely as a branch of math in this book, with no hint of connections to DS, then apply that knowledge in subsequent DS courses. Instead, the goal in the title is to emphasize the fact that:\n\nLinear algebra is absolutely fundamental to the Data Science field. For us data scientists, it is “our” branch of math. Almost every concept in this book is first motivated by a Data Science application. Mastering this branch of math, which is definitely within the reach of all, pays major dividends."
  },
  {
    "objectID": "preface.html#philosophy",
    "href": "preface.html#philosophy",
    "title": "Preface",
    "section": "Philosophy",
    "text": "Philosophy\nI learned very early the difference between knowing the name of something and knowing something – physicist Richard Feynman\n…it felt random. “Follow these steps and you get the result you are looking for.” But why does it work? What possessed you to follow this path as opposed to any other? How might I have come up with this myself? – comment by a reader of a famous linear algebra book\nA fundamental philosophy of my book here is to avoid such reader frustration. It’s easy to define, say the dimension of a vector subspace, but that’s definitely not enough. What is the underlying intuition? Why is the concept important, especially in Data Science?\nThe presentation of each concept in this book begins with a problem to be solved, almost always from Data Science, then leading up to a linear algebra solution. Basically, the math sneaks up on the reader, who suddenly realizes they’ve just learned a new general concept! And the reader knows where the concept fits into the Big Picture, and can distill the abstraction into an intuitive summary.\nFor instance, we use Markov chain transition matrices and network graph models (e.g. social networks) to motivate the notion of a matrix and matrix multiplication. An interest in finding the stationary distribution of a Markov chain then leads to the concept of matrix inverses. (Linear models are presented later, after groundwork of matrix rank is laid.)\nThe chapter on matrix rank starts with a dataset right off the bat, and shows that R’s linear model function lm fails if categorical variables are fully specified. This motivates the notion of rank, and the dataset dovetails with the theory throughout the chapter, which culminates in a proof that row rank equals column rank.\nAbove all, this book does not allow rote memorization, merely “knowing the name of something.” The focus on How? and Why? is on every page."
  },
  {
    "objectID": "preface.html#who-is-this-book-for",
    "href": "preface.html#who-is-this-book-for",
    "title": "Preface",
    "section": "Who Is This Book For?",
    "text": "Who Is This Book For?\nOf course the book should work well as a classroom textbook. The “applications first” approach should motivate students, and the use of Quarto enables easy conversion to Powerpoint by instructors.\nBut I also hope the book’s emphasis on the How? and Why? especially appeals to do-it-yourselfers, those whose engagement in self-study is motivated by intellectual curiosity rather than a course grade."
  },
  {
    "objectID": "preface.html#prerequisite-background",
    "href": "preface.html#prerequisite-background",
    "title": "Preface",
    "section": "Prerequisite Background",
    "text": "Prerequisite Background\nBasic data science:\n\nCalculus.\nSome exposure to R is recommended, but the text can be read without it.\nBasics of random variables, expected value and variance.\n\nFor a quick, painless introduction to R, see my fasteR tutorial, say the first 8 lessons."
  },
  {
    "objectID": "preface.html#the-role-of-theory-and-r",
    "href": "preface.html#the-role-of-theory-and-r",
    "title": "Preface",
    "section": "The Role of Theory (and R)",
    "text": "The Role of Theory (and R)\n\n\n\n\n\n\nThis book is “mathematical but not very theoretical.”\n\n\n\nTheorems are mainly limited to results with practical importance, and proofs are sometimes rather informal. But the subject matter is indeed mathematical. The goal is to develop in the reader mathematical skill and intuition into this powerful tool, rather than coding of linear algebra methods.\nThus the many R examples are meant to make the mathematical concepts concrete. This is definitely not an “how to do linear algebra in R” book. In the software context, the Feynman quote above might be, “Knowing how to use code libraries for something is useless if one doesn’t understand the nature of that thing.”\nThat said, the code examples do serve a vital role in making the math operations concrete."
  },
  {
    "objectID": "preface.html#r-packages-used",
    "href": "preface.html#r-packages-used",
    "title": "Preface",
    "section": "R Packages Used",
    "text": "R Packages Used\nqeML\nglmnet\nigraph\ndsld\nnetworkdata\npracma\nregclass\nWackyData"
  },
  {
    "objectID": "preface.html#data-availabilty",
    "href": "preface.html#data-availabilty",
    "title": "Preface",
    "section": "Data Availabilty",
    "text": "Data Availabilty\nThe datasets used are included with the packages."
  },
  {
    "objectID": "Ch1.html",
    "href": "Ch1.html",
    "title": "2  Matrices and Vectors",
    "section": "",
    "text": "3 Partitioned Matrices: an Invaluable Visualization Tool\nHere, “visualization” is not a reference to graphics but rather to highlighting certain submatrices.\n\\[\ng_j = P(X_1 = j)\n= \\sum_{i=1}^k P(X_0 = i) P(X_1 = j | X_0 = i)\n= \\sum_{i=1}^k f_i a_{ij}\n\\]\nwhere \\(a_{ij}\\) is the row i, column j element of the chain’s transition matrix \\(P\\).\nFor example, consider \\(g_5\\). How could we be at state 5 at time 1? We could start in state 1, probability \\(f_1\\), then move to state 5, probability \\(a_{15}\\), for a total probability of \\(f_1 a_{15}\\). Or, we could start in state 2, probability \\(f_2\\), then move to state 5, probability \\(a_{25}\\), for a total probability of \\(f_2 a_{25}\\). And so on.\nSo,\n\\[\ng_j = \\sum_{i=1}^k f_i a_{ij}\n\\]\nPutting this is more explicit matrix terms,\n\\[\ng = \\left (\n\\begin{array}{r}\ng_1 \\\\\ng_2 \\\\\n... \\\\\ng_k \\\\\n\\end{array}\n\\right )\n=\n\\left (\n\\begin{array}{r}\nf_1 a_{11} + f_2 a_{21} + ... + f_k a_{k1} \\\\\nf_1 a_{12} + f_2 a_{22} + ... + f_k a_{k2} \\\\\n... \\\\\nf_1 a_{1k} + f_2 a_{2k} + ... + f_k a_{kk} \\\\\n\\end{array}\n\\right )\n\\]\nThat last expression is\n\\[\nf'P\n\\]\n\\[\ng' = f'P\n\\]\nAnd setting h to the distribution of \\(X_2\\), the same reasoning gives us\n\\[\nh' = g'P\n\\]\n\\[\nh' = f'P^2\n\\]\nand so on. Iterating, we obtain\n\\[\nd_j' = d_0' P^j\n\\]\nwhere \\(d_i\\) is the distribution of \\(X_i\\).\nSimilarly,\n\\[\nd_j' = d_{j-1}' P\n\\]\nFor convenience, let’s take transposes:[Recall that \\((AB)' = B'A'\\).\n\\[\nd_j = P' d_{j-1}\n\\]\nNow suppose our chain as a long-run distribution, as in {Section 2.7}. Let’s call that distribution \\(\\nu\\). By lettting \\(j \\rightarrow \\infty\\) above, we have\n\\[\n\\nu = P' \\nu\n\\]\nSince P is known, this provides us with a way to compute \\(\\nu\\). Yes, finding a high power of P would do this too, but that would involve a lot of computation, and even then it would not yield the exact answer. We will return to this in the next chapter."
  },
  {
    "objectID": "Ch1.html#a-random-walk-model",
    "href": "Ch1.html#a-random-walk-model",
    "title": "2  Matrices and Vectors",
    "section": "2.1 A Random Walk Model",
    "text": "2.1 A Random Walk Model\nLet’s consider a random walk on {1,2,3,4,5} in the number line. Time is numbered 1,2,3,… Our current position is termed our state. The notation Xk = i means that at time k we are in state/position i.\nOur rule will be that at any time k, we flip a coin. If we are currently at position i, we move to either i+1 or i-1, depending on whether the coin landed heads or tails. The exceptions are k = 1 and k = 5, in which case we stay put if tails or move to the adjacent position if heads.\nWe can summarize the probabilities with a matrix, a two-dimensional array:\n\\[\nP_1 =\n\\left (\n\\begin{array}{rrrrr}\n0.5 & 0.5 & 0 & 0 & 0\\\\\n0.5 & 0 & 0.5 & 0 & 0\\\\\n0 & 0.5 & 0 & 0.5 & 0\\\\\n0 & 0 & 0.5 & 0 & 0.5\\\\\n0 & 0 & 0 & 0.5 & 0.5 \\\\\n\\end{array}\n\\right )\n\\]\nFor instance, look at Row 2. There are 0.5 values in Columns 1 and 3, meaning there is a 0.5 chance of a move 2 \\(\\rightarrow\\) 1, and a 0.5 chance of a move 2 \\(\\rightarrow\\) 3. Note that each row in a transition matrix must sum to 1. After, from state i we must go somewhere.\nWe use a subscript 1 here in \\(P_1\\), meaning “one step.” We go from, say, state 2 to state 1 in one step with probability 0.5. \\(P_1\\) is called the one-step transition matrix (or simply the transition matrix) for this process.\nWhat about the two-step transition matrix \\(P_2\\)? From state 3, we could go to state 1 in two steps, by two tails flips of the coin. The probability of that is \\(0.5^2 = 0.25\\). So the row 3, column 1 element in \\(P_2\\) is 0.25. On the other hand, if from state 3 we flip tails then heads, or heads then tails, we are back to state 3. So, the row 3, column 3 element in \\(P_2\\) is 0.25 + 0.25 = 0.5.\nThe reader should verify the correctness here:\n\\[\nP_2 =\n\\left (\n\\begin{array}{rrrrr}\n0.5 & 0.25 & 0.25 & 0 & 0\\\\\n0.25 & 0.5 & 0 & 0.25 & 0\\\\\n0.25 & 0 & 0.5 & 0 & 0.25\\\\\n0 & 0.25 & 0 & 0.5 & 0.25\\\\\n0 & 0 & 0.25 & 0.25 & 0.5 \\\\\n\\end{array}\n\\right )\n\\]\nWell, finding two-step transition probabilities would be tedious in general, but it turns out that is a wonderful shortcut: Matrix multiplication. We will cover this in the next section, but first a couple of preliminaries.\nThe above random walk is a Markov chain. The Markov Property says that the system “has no memory.” If say we land at position 2, we will go to 1 or 3 with probability 1/2 no matter what the previous history of the system was; it doesn’t matter how we got to state 3. That in turn comes from the independence of the successive coin flips.\nNotation: Individual elements of a matrix are usually written with double subscripts. For instance, a25 will mean the row 2, column 5 element of the matrix \\(A\\). If say \\(A\\) has more than 9 rows, its row 11, column 5 element is denoted by a11,5, using the comma to avoid ambiguity."
  },
  {
    "objectID": "Ch1.html#vectors",
    "href": "Ch1.html#vectors",
    "title": "2  Matrices and Vectors",
    "section": "2.2 Vectors",
    "text": "2.2 Vectors\nMatrices are two-dimensional arrays. One-dimensional arrays are called vectors, either in row or column form, e.g.\n\\[\nu = (12,5,13)\n\\]\nand\n\\[\nu =\n\\left (\n\\begin{array}{r}\n12 \\\\\n5 \\\\\n13 \\\\\n\\end{array}\n\\right )\n\\]\nPlease note:\n\nVectors may also be viewed as one-row or one-column matrices.\nWhen not otherwise stated, the term “vector” will mean column form.\nThe term scalar simply means a number, rather than a matrix or vector. It will be used quite frequently in this book."
  },
  {
    "objectID": "Ch1.html#sec-easyops",
    "href": "Ch1.html#sec-easyops",
    "title": "2  Matrices and Vectors",
    "section": "2.3 Addition and Scalar Multiplication",
    "text": "2.3 Addition and Scalar Multiplication\nVectors of the same length may be summed, in elementwise form, e.g.\n\\[\n\\left (\n\\begin{array}{r}\n12 \\\\\n5 \\\\\n13 \\\\\n\\end{array}\n\\right )\n+\n\\left (\n\\begin{array}{r}\n-3 \\\\\n6 \\\\\n18.2 \\\\\n\\end{array}\n\\right ) =\n\\left (\n\\begin{array}{r}\n9 \\\\\n11 \\\\\n31.2 \\\\\n\\end{array}\n\\right )\n\\]\nSimilarly, two matrices may be added, again in elementwise fashion, provided the number of rows is the same for both, as well as the same condition for number of columns.\nVectors and matrices can be multiplied by scalars, again elementwise, e.g.\n\\[\n0.3\n\\left (\n\\begin{array}{r}\n6 \\\\\n15 \\\\\n\\end{array}\n\\right ) =\n\\left (\n\\begin{array}{r}\n1.8 \\\\\n4.5 \\\\\n\\end{array}\n\\right )\n\\]"
  },
  {
    "objectID": "Ch1.html#matrix-matrix-multiplication",
    "href": "Ch1.html#matrix-matrix-multiplication",
    "title": "2  Matrices and Vectors",
    "section": "2.4 Matrix-Matrix Multiplication",
    "text": "2.4 Matrix-Matrix Multiplication\nThis is the most fundamental operation in linear algebra. It is defined as follows:\n\nGiven matrix A of k rows and m columns and matrix B of m rows and r columns, the product C = AB is a \\(k \\textrm{ x } m\\) matrix, whose row i, column j element is\n\\[\na_{i1} b_{i1} +\na_{i2} b_{i1} + ... +\na_{m1} b_{m1}\n\\]\nThis is the “dot product” of row i of A and column j of B: Find the products of the paired elements in the two vectors, then sum.\n\nFor example, set\n\\[\nA = \\left (\n\\begin{array}{rrr}\n5 & 2 & 6 \\\\\n1 & 1 & 8 \\\\\n\\end{array}\n\\right )\n\\]\nand\n\\[\nB = \\left (\n\\begin{array}{rr}\n5 & -1 \\\\\n1 & 0 \\\\\n0 & 8 \\\\\n\\end{array}\n\\right )\n\\]\nLet’s find the row 2, column 2 element of C = AB. Again, that means taking the dot product of row 2 of A and column 2 of B, which we’ve highlighted below.\n\\[\nA = \\left (\n\\begin{array}{rrr}\n5 & 2 & 6 \\\\\n\\color{red}{1} & \\color{red}{1} & \\color{red}{1} \\\\\n\\end{array}\n\\right )\n\\]\nand\n\\[\nB = \\left (\n\\begin{array}{rr}\n5 & \\color{red}{-1} \\\\\n1 & \\color{red}{0} \\\\\n0 & \\color{red}{8} \\\\\n\\end{array}\n\\right )\n\\]\nThe value in question is then\n1 (-1) + 1 (0) + 1 (8) = 7\nLet’s check it, with R:\n.The rbind and cbind functions (“row bind” and “column bind”) are very handy tools for creating matrices.\n\na &lt;- rbind(c(5,2,6),c(1,1,1))\nb &lt;- cbind(c(5,1,0),c(-1,0,8))\na %*% b\n\n     [,1] [,2]\n[1,]   27   43\n[2,]    6    7\n\n\nThe reader should make sure to check the other elements by hand.\n\n\n\n\n\n\nTip\n\n\n\nAlways keep in mind that in the matrix product \\(AB\\), the number of rows of \\(B\\) must equal the number of columns of \\(A\\). The two matrices are then said to be conformable."
  },
  {
    "objectID": "Ch1.html#the-identity-matrix",
    "href": "Ch1.html#the-identity-matrix",
    "title": "2  Matrices and Vectors",
    "section": "2.5 The Identity Matrix",
    "text": "2.5 The Identity Matrix\nThe identity matrix \\(I\\) of size n is the nxn matrix with 1s on the diagonal and 0s elsewhere. \\(IB = B\\) and \\(AI = A\\) for any conformable \\(A\\) and \\(B\\)."
  },
  {
    "objectID": "Ch1.html#vectors-1",
    "href": "Ch1.html#vectors-1",
    "title": "2  Matrices and Vectors",
    "section": "2.6 Vectors",
    "text": "2.6 Vectors\nA matrix that has only one row or only one column is called a vector. Depending on which of those two shapes it has, we may refer to it as a row vector or column vector. Usually we will simply say “vector,” in which case it will be meant as a column vector."
  },
  {
    "objectID": "Ch1.html#sec-introMCs",
    "href": "Ch1.html#sec-introMCs",
    "title": "2  Matrices and Vectors",
    "section": "2.7 Application to Markov Chain Transition Matrices",
    "text": "2.7 Application to Markov Chain Transition Matrices\nNow let’s return to the question of how to easily compute \\(P_2\\), the two-step transition matrix. It turns out that:\n\nLet P denote the transition matrix of a (finite-state) Markov chain. The k-step transition matrix is \\(P^k\\).\n\nAt first, this may seem amazingly fortuitous, but it makes sense in light of the “and/or” nature of the probability computations involved. Recall our computation for the row 1, column 2 element of \\(P_2\\) above. From state 1, we could either stay at 1 for one flip, then move to 2 on the second flip, or we could go to 2 then return to 1. Each of these has probability 0.5, so the total probability is\n\\[\n(0.5)(0.5) + (0.5)(0.5)\n\\]\nBut this is exactly the form of our “dot product” computation in the definition of matrix multiplication,\n\\[\na_{i1} b_{i1} +\na_{i2} b_{i1} + ... +\na_{m1} b_{m1}\n\\]\nThen \\(P\\)3 stores the 3-step probabilities and so on.\nStatisticians and computer scientists like to look at the asymptotic behavior of systems. Let’s see where we might be after say, 6 steps:\n\nmatpow &lt;- function(m,k) {\n   nr &lt;- nrow(m)\n   tmp &lt;- diag(nr)  # identity matrix\n   for (i in 1:k) tmp &lt;- tmp %*% m\n   tmp\n}\n\np1 &lt;- rbind(c(0.5,0.5,0,0,0), c(0.5,0,0.5,0,0), c(0,0.5,0,0.5,0), \n   c(0,0,0.5,0,0.5), c(0,0,0,0.5,0.5))\nmatpow(p1,6) \n\n         [,1]     [,2]     [,3]     [,4]     [,5]\n[1,] 0.312500 0.234375 0.234375 0.109375 0.109375\n[2,] 0.234375 0.312500 0.109375 0.234375 0.109375\n[3,] 0.234375 0.109375 0.312500 0.109375 0.234375\n[4,] 0.109375 0.234375 0.109375 0.312500 0.234375\n[5,] 0.109375 0.109375 0.234375 0.234375 0.312500\n\n\nSo for instance if we start at position 2, there is about an 11% chance that we will be at position 3 at time 6. What about time 25?\n\nmatpow(p1,25)\n\n          [,1]      [,2]      [,3]      [,4]      [,5]\n[1,] 0.2016179 0.2016179 0.1993820 0.1993820 0.1980001\n[2,] 0.2016179 0.1993820 0.2016179 0.1980001 0.1993820\n[3,] 0.1993820 0.2016179 0.1980001 0.2016179 0.1993820\n[4,] 0.1993820 0.1980001 0.2016179 0.1993820 0.2016179\n[5,] 0.1980001 0.1993820 0.1993820 0.2016179 0.2016179\n\n\nSo, no matter which state we start in, at time 25 we are about 20% likely to be at any of the states. In fact, as time n goes to infinity, this probability vector becomes exactly (0.20,0.20,0.20,0.20,0.20), as we will see in the next chapter."
  },
  {
    "objectID": "Ch1.html#network-graph-models",
    "href": "Ch1.html#network-graph-models",
    "title": "2  Matrices and Vectors",
    "section": "2.8 Network Graph Models",
    "text": "2.8 Network Graph Models\nThere has always been lots of analysis of “Who is connected to who,” but activity soared after the advent of Facebook and the film, A Social Network. See for instance Statistical Analysis of Network Data with R by Eric Kolaczy and Gábor Csárdi. As the authors say,\n\nThe oft-repeated statement that “we live in a connected world” perhaps best captures, in its simplicity…From on-line social networks like Facebook to the World Wide Web and the Internet itself, we are surrounded by examples of ways in which we interact with each other. Similarly, we are connected as well at the level of various human institutions (e.g., governments), processes (e.g., economies), and infrastructures (e.g., the global airline network). And, of course, humans are surely not unique in being members of various complex, inter-connected systems. Looking at the natural world around us, we see a wealth of examples of such systems, from entire eco-systems, to biological food webs, to collections of inter-acting genes or communicating neurons.\n\nAnd of course, at the center of it all is a matrix! Here is why:\nLet’s consider the famous Karate Club dataset:\n\n\n# remotes::install_github(\"schochastics/networkdata\") \nlibrary(networkdata)\ndata(karate)\nlibrary(igraph)\nplot(karate)\n\n\n\n\nThere is a link between node 13 and node 4, meaning that club members 13 and 4 are friends.This graph is undirected, as friendship is mutual. Many graphs are directed, but we will assume undirected here.\nSpecifically, the adjacency matrix has row i, column j element as 1 or 0, according to whether a link exists between nodes i and j.\n\nadjK &lt;- as_adjacency_matrix(karate)\nadjK\n\n34 x 34 sparse Matrix of class \"dgCMatrix\"\n                                                                         \n [1,] . 1 1 1 1 1 1 1 1 . 1 1 1 1 . . . 1 . 1 . 1 . . . . . . . . . 1 . .\n [2,] 1 . 1 1 . . . 1 . . . . . 1 . . . 1 . 1 . 1 . . . . . . . . 1 . . .\n [3,] 1 1 . 1 . . . 1 1 1 . . . 1 . . . . . . . . . . . . . 1 1 . . . 1 .\n [4,] 1 1 1 . . . . 1 . . . . 1 1 . . . . . . . . . . . . . . . . . . . .\n [5,] 1 . . . . . 1 . . . 1 . . . . . . . . . . . . . . . . . . . . . . .\n [6,] 1 . . . . . 1 . . . 1 . . . . . 1 . . . . . . . . . . . . . . . . .\n [7,] 1 . . . 1 1 . . . . . . . . . . 1 . . . . . . . . . . . . . . . . .\n [8,] 1 1 1 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n [9,] 1 . 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 . 1 1\n[10,] . . 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\n[11,] 1 . . . 1 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n[12,] 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n[13,] 1 . . 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n[14,] 1 1 1 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\n[15,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 1\n[16,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 1\n[17,] . . . . . 1 1 . . . . . . . . . . . . . . . . . . . . . . . . . . .\n[18,] 1 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n[19,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 1\n[20,] 1 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\n[21,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 1\n[22,] 1 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n[23,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 1\n[24,] . . . . . . . . . . . . . . . . . . . . . . . . . 1 . 1 . 1 . . 1 1\n[25,] . . . . . . . . . . . . . . . . . . . . . . . . . 1 . 1 . . . 1 . .\n[26,] . . . . . . . . . . . . . . . . . . . . . . . 1 1 . . . . . . 1 . .\n[27,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 . . . 1\n[28,] . . 1 . . . . . . . . . . . . . . . . . . . . 1 1 . . . . . . . . 1\n[29,] . . 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 . 1\n[30,] . . . . . . . . . . . . . . . . . . . . . . . 1 . . 1 . . . . . 1 1\n[31,] . 1 . . . . . . 1 . . . . . . . . . . . . . . . . . . . . . . . 1 1\n[32,] 1 . . . . . . . . . . . . . . . . . . . . . . . 1 1 . . 1 . . . 1 1\n[33,] . . 1 . . . . . 1 . . . . . 1 1 . . 1 . 1 . 1 1 . . . . . 1 1 1 . 1\n[34,] . . . . . . . . 1 1 . . . 1 1 1 . . 1 1 1 . 1 1 . . 1 1 1 1 1 1 1 .\n\nadjK[13,4]\n\n[1] 1\n\n\nAccordingly, row 13, column 4 does have a 1 entry.\nAs is the case with Markov transition matrices, powers of an adjacency matrix can yield valuable information. In the Markov case, multiplication gives us sums of paired products, computing probabilities. What about the network graph case?\nHere products are of the form 0x0, 0x1, 1x0 or 1x1. If there is a nonzero entry m in row i, column j of the square of the adjacency matrix, that means there were m 1x1 products in that sum, which would correspond to m paths. Let’s look into this.\n\nadjK2 &lt;- adjK %*% adjK\n\nWe see that adjK2[11,1] is 2. Inspection of adjK shows that its row 11, columns 6 and 7 are 1s, and that rows 6 and 7, column 1 are 1s as well. So there are indeed two two-hop paths from node 11 to node 1, specifically \\(11 \\rightarrow 6 \\rightarrow 1\\) and $ \\(11 \\rightarrow 7 \\rightarrow 1\\). Thus the 2 we see in adjK2[11,1] was correct.\nActually, what is typically of interest is connectivity rather than number of paths. For any given pair of nodes, is there a multihop path between them? Or does the graph break down to several “islands” of connected nodes?\nAgain consider the karate club data.\n\nu &lt;- matpow(adjK,33)\nsum(u == 0)\n\n[1] 0\n\n\nSo, in this graph, no pair of nodes has 0 paths between them. The graph is connected.\nMaking this kind of analysis fully correct requires paying attention to things such as cycles. The details are beyond the scope of this book."
  },
  {
    "objectID": "Ch1.html#recommender-systems",
    "href": "Ch1.html#recommender-systems",
    "title": "2  Matrices and Vectors",
    "section": "2.9 Recommender Systems",
    "text": "2.9 Recommender Systems\nIf you inquire about some item at an online store, the software will also present you with some related items that it thinks would be of interest to you. How does the software make this guess?\nClearly, the full answer is quite complex. But we can begin to see the process by looking at some real data.\n\nsite &lt;- 'http://files.grouplens.org/datasets/movielens/ml-100k/u.data'\nq &lt;- read.table(site)\nnames(q) &lt;- c('user','movie','rating','userinfo')\nhead(q)\n\n  user movie rating  userinfo\n1  196   242      3 881250949\n2  186   302      3 891717742\n3   22   377      1 878887116\n4  244    51      2 880606923\n5  166   346      1 886397596\n6  298   474      4 884182806\n\n\nWe see for instance that user 22 gave movie 242 a rating of 1. If we want to know some characteristics of this user, his/her ID is 878887116, which we can find in the file u.user at the above URL. Other files tell us more about this movie, e.g. its genre, and so on.\nLet’s explore the data a bit:\nHow many users and movies are in this dataset?\n\nlength(unique(q$user))\n\n[1] 943\n\nlength(unique(q$movie))\n\n[1] 1682\n\n\nHow many other users rated movie number 242?\n\nsum(q$movie == 242)\n\n[1] 117\n\n\nDid user 22 rate movie 234, for instance?\n\nwhich(q$user == 22 & q$movie == 234)\n\ninteger(0)\n\n\nNow we can begin to see a solution to the recommender problem. Say we wish to guess whether user 22 would like movie 234. We could look for other users who have rated many of the same movies as user 22, then focus on the ones who rated movie 234. We could average those ratings to obtain a predicted rating for movie 234 by user 22.We could also incorporate the characteristics of user 22 and the others, which may improve our prediction accuracy, but we will not pursue that here.\nIn order to assess interuser similarity of the nature described above, we might form a matrix \\(S\\), as follows. There would be 943 rows, one for each user, and 1682 columns, one for each moview. The element in row \\(i\\), column \\(j\\) would be the rating user \\(i\\) gave to movie \\(j\\). Most of the matrix would be 0s.\nThe point of constructing \\(S\\) is that determining the similarity of users becomes a matter of measuring similarity of rows of \\(S\\). This paves the way to exploiting the wealth of matrix-centric methodology we will develop in this book.\n❄️ Your Turn: Write a function with call form\nmakeSimilarityMatrix(m)\nthat takes as input a matrix or data frame of user-movie-rating data, and returns a matrix as described above, i.e. one row per user, one column per movie, and so on."
  },
  {
    "objectID": "Ch1.html#matrix-algebra",
    "href": "Ch1.html#matrix-algebra",
    "title": "2  Matrices and Vectors",
    "section": "2.10 Matrix Algebra",
    "text": "2.10 Matrix Algebra\n\n2.10.1 Other basic operations\nMatrix multiplication may seem odd at first, but other operations are straightforward.\nAddition: We just add corresponding elements. For instance,\n\\[\nA = \\left (\n\\begin{array}{rrr}\n5 & 2 & 6 \\\\\n1 & 2.6 & -1.2 \\\\\n\\end{array}\n\\right )\n\\]\n\\[\nB = \\left (\n\\begin{array}{rrr}\n0 & 20 & 6 \\\\\n3 & 5.8 & 1 \\\\\n\\end{array}\n\\right )\n\\]\n\\[\nA+B = \\left (\n\\begin{array}{rrr}\n5 & 22 & 12 \\\\\n4 & 8.4 & -0.2 \\\\\n\\end{array}\n\\right )\n\\]\nWe do have to make sure the addends match in terms of numbers of rows and columns, 2 and 3 in the example here.\nScalar multiplication: Again, this is simply elementwise. E.g. with A as above,\n\\[\n1.5 A = \\left (\n\\begin{array}{rrr}\n7.5 & 3 & 9 \\\\\n1.5 & 3.9 & -1.8 \\\\\n\\end{array}\n\\right )\n\\]\nDistributive property:\nFor matrices A, B and C of suitable conformability (A and B match in numbers of rows and columns, and their common number of columns matches the number of rows in C), we have\n(A+B) C = AC + BC\n\n\n2.10.2 Matrix transpose\nThis is a very simple but very important operation: We merely exchange rows and columns of the given matrix. For instance, with A as above, its transpose (signified with “’”), is\n\\[\nA' = \\left (\n\\begin{array}{rr}\n5 & 1 \\\\\n2 & 2.6 \\\\\n6 & -1.2 \\\\\n\\end{array}\n\\right )\n\\]\nThe R function for transpose is t().\nIt can be shown that if A and B are conformable, then\n(AB)’ = B’A’\nFor some matrices C, we have C’ = C. C is then termed symmetric.\nWe will often write a row vector in the form (a,b,c,…). So (5,1,88) means the 1x3 matrix with those elements. If we wish this to be a column vector, we use transpose, so that for instance (5,1,88)’ means a 3x1 matrix.\n\n\n2.10.3 Trace of a square matrix\nThe trace of a square matrix \\(A\\) is the sum of its diagonal elements, \\(tr(A) = \\sum_{i=1}^n A_{ii}\\). This measure has various properties, some obvious (trace of the sum is sum of the traces), and some less so, such as:\n\nSuppose \\(A\\) and \\(B\\) are square matrices of the same size. Then\n\\[\ntr(AB) = tr(BA)\n\\tag{2.1}\\]\nProof: See Your Turn problem.\n\n❄️ Your Turn: Prove Equation 2.1. Hint: Write out the left-hand side as a double sum. Reverse the order of summation, and work toward the right-hand side.\nAnd furthermore:\n\nIt can be shown that trace is invariant under circular shifts, e.g. \\(UVW\\), \\(VWU\\) and \\(WUV\\) all have the same trace."
  },
  {
    "objectID": "Ch1.html#how-it-works",
    "href": "Ch1.html#how-it-works",
    "title": "2  Matrices and Vectors",
    "section": "3.1 How It Works",
    "text": "3.1 How It Works\nConsider a matrix-vector product Mv. Of course, that means that v is a column vector whose length is equal to the number of columns of M. If M is of size rxs, then v is sx1.\nLet’s denote column j of M by Cj. Then we will see later in this chapter that\n\\[\nMv =\nv_1 C_{1} +\nv_2 C_{2} + ... +\nv_s C_{s}  \n\\]\nFor instance, take\n\\[\nA = \\left (\n\\begin{array}{rrr}\n5 & 2 & 6 \\\\\n1 & 2.6 & -1.2 \\\\\n\\end{array}\n\\right )\n\\]\nand\n\\[\nv = \\left (\n\\begin{array}{r}\n10 \\\\\n2 \\\\\n1 \\\\\n\\end{array}\n\\right )\n\\]\nThe reader should check that\n\\[\n10 \\left (\n\\begin{array}{r}\n5 \\\\\n1 \\\\\n\\end{array}\n\\right )\n+\n2 \\left (\n\\begin{array}{r}\n2 \\\\\n2.6 \\\\\n\\end{array}\n\\right )\n+\n1 \\left (\n\\begin{array}{r}\n6 \\\\\n-1.2 \\\\\n\\end{array}\n\\right )\n= Av\n\\]\nwhere the latter is\n\\[\n\\left (\n\\begin{array}{r}\n60 \\\\\n14  \\\\\n\\end{array}\n\\right )\n\\]\nNote that the above expression,\n\\[\n10 \\left (\n\\begin{array}{r}\n5 \\\\\n1 \\\\\n\\end{array}\n\\right )\n+\n2 \\left (\n\\begin{array}{r}\n2 \\\\\n2.6 \\\\\n\\end{array}\n\\right )\n+\n1 \\left (\n\\begin{array}{r}\n6 \\\\\n-1.2 \\\\\n\\end{array}\n\\right ),\n\\]\nis a sum of scalar products of vectors, which is called a linear combination of those vectors. The quantities 10, 2 and 1 are the coefficients in that linear combination.\nIn other words, we have that:\n\nThe product \\(Av\\) of a matrix times a column vector is equal to a linear combination of the columns of the matrix, with coefficients equal to the column vector.\n\nSimilarly,\n\nThe product \\(wA\\) of a row vector and a matrix is equal to a linear combination of the rows of the matrix, with the coefficients coming from the row vector.\n\nTo further illustrate partitioned matrices, write the above matrix \\(A\\) as\n\\[\nA =\n\\left (\n\\begin{array}{rr}\nA_{11} & A_{21} \\\\\n\\end{array}\n\\right )\n\\]\nwhere\n\\[\nA_{11} =\n\\left (\n\\begin{array}{rr}\n5 & 2 \\\\\n1 & 2.6 \\\\\n\\end{array}\n\\right )\n\\]\nand\n\\[\nA_{12} =\n\\left (\n\\begin{array}{r}\n6 \\\\\n-1.2 \\\\\n\\end{array}\n\\right )\n\\]\nSymbolically, \\(A\\) now looks like a 1x2 “matrix.” Similarly, writing\n\\[\nv =\n\\left (\n\\begin{array}{r}\nv_{11} \\\\\nv_{21} \\\\\n\\end{array}\n\\right )\n\\]\nwhere\n\\[\nv_{11} =\n\\left (\n\\begin{array}{r}\n10 \\\\\n2 \\\\\n\\end{array}\n\\right )\n\\]\nand \\(v_{21} = 1\\) (a 1x1 matrix), \\(v\\) looks to be 2x1.\nSo, again pretending, treat the product \\(Av\\) as the multiplication of a 1x2 “matrix” and 2x1 “vector”, yielding a 1x1 result,\n\\[\nA_{11} v_{11} + A_{12} v_{21}\n\\]\nBut all that pretending actually does give the correct answer!\n\\[\nA_{11} v_{11} + A_{12} v_{21} =\n\\left (\n\\begin{array}{rr}\n5 & 2 \\\\\n1 & 2.6 \\\\\n\\end{array}\n\\right )\n\\left (\n\\begin{array}{r}\n10 \\\\\n2 \\\\\n\\end{array}\n\\right )\n+\n\\left (\n\\begin{array}{r}\n6 \\\\\n-1.2 \\\\\n\\end{array}\n\\right )\n1\n=\n\\left (\n\\begin{array}{r}\n60 \\\\\n14 \\\\\n\\end{array}\n\\right )\n\\]\nWe can extend that reasoning further. Say \\(A\\) and \\(B\\) are matrices of sizes \\(m \\textrm{x} n\\) and \\(n \\textrm{x} k\\), and consider the product \\(AB\\). Partition \\(B\\) by its columns,\n\\[\nB = (B^{(1)},B^{(2)},..., B^{(k)})\n\\]\nNow pretending that \\(A\\) is a \\(1 \\textrm{x} 1\\) “matrix” and \\(B\\) is a\\(1 \\textrm{x} k\\) “matrix”, we have\n\\[\nAB = (AB^{(1)},AB^{(2)},..., AB^{(k)})\n\\]\nIn other words,\n\nIn the product \\(AB\\), column \\(j\\) is a linear combination of the columns of \\(A\\), and the coefficients in that linear combination are the elements of column \\(j\\) of \\(B\\).\nA similar result holds for the row of the product.\n\n❄️ Your Turn: Write out the details of that “similar result.”"
  },
  {
    "objectID": "Ch1.html#sec-pagerank",
    "href": "Ch1.html#sec-pagerank",
    "title": "2  Matrices and Vectors",
    "section": "4.1 Application: Google PageRank",
    "text": "4.1 Application: Google PageRank\nWhen Google was first formed, its key internal component was a method to rank Web sites in terms of popularity. They developed such a method, and named it PageRank, a pun combining the term Web page (i.e. Web site) and the name of one of the founders, Larry Page. It’s based on a Markov model.\nThe transition matrix is modeled as follows. Row \\(i\\) has \\(o_i\\) nonzero entries, each of which is equal to \\(1/o_i\\). They define popularity as the resulting long-run distribution, i.e. \\(\\nu\\) in Section 2.7."
  },
  {
    "objectID": "Ch1.html#random-vectors",
    "href": "Ch1.html#random-vectors",
    "title": "2  Matrices and Vectors",
    "section": "4.2 Random Vectors",
    "text": "4.2 Random Vectors\nYou are probably familiar with the concept of a random variable, but of even greater importance is random vectors.\nSay we are jointly modeling height, weight, age, systolic blood pressure and cholesterol, and are especially interested in relations between these quantities. We then have the random vector\n\\[\nX\n=\n\\left (\n\\begin{array}{r}\nX_1 \\\\\nX_2 \\\\\nX_3 \\\\\nX_4 \\\\\nX_5 \\\\\n  \\end{array}\n\\right )\n=\n\\left (\n\\begin{array}{r}\n\\textrm{height} \\\\\n\\textrm{weight} \\\\\n\\textrm{age} \\\\\n\\textrm{bp} \\\\\n\\textrm{chol} \\\\\n  \\end{array}\n\\right )\n\\]\n\n4.2.1 Sample vs. population\nWe may observe \\(n\\) realizations of \\(X\\) in the form of sample data, say on \\(n = 100\\) people. In the statistics world, we treat this data as a random sample from some population, say all Americans. Usually, we are just given the data rather then having actual random sampling, but this view recognizes that there are a lot more people out there than our data.\nWe speak of estimating population quantities. For instance, we can estimate the population value $E(X1), i.e. mean of \\(X_1\\) throughout the population, by the sammple analog,\n\\[\n\\frac{1}{n} \\sum_{i=1}^n X_{1j}\n\\]\nwhere \\(X_{ij}\\) denotes the value of \\(X_i\\) for the \\(j^{th}\\) person in our sample.\nBy contrast, this view is rarely taken in the machine learning community. The data is the data, and the fact that it is a small subset of a much larger group is irrelevant. They will often allude to the randomness of the data by mentioning the “data generating mechanism.”"
  },
  {
    "objectID": "Ch1.html#sec-cov",
    "href": "Ch1.html#sec-cov",
    "title": "2  Matrices and Vectors",
    "section": "4.3 Covariance Matrices",
    "text": "4.3 Covariance Matrices\nRecall the notion in statistics of covariance: Given a pair of random variables \\(U\\) and \\(V\\), their covariance is defined by\n\\[\nCov(U,V) = E[(U - EU)(V - EV)]\n\\]\nLoosely speaking, this measures the degree to which the two random variables vary together. Consider for instance human height \\(H\\) and \\(W\\). Taller people tend to also be heavier. Say we sample many people from a population. Most of those who are taller than average, i.e. \\(H &gt; EH\\) will also be heavier than average, \\(W &gt; EW\\), making \\((H - EH)(W - EW) &gt; 0\\). Similarly, shorter people tend to be lighter, but then we still have \\((H - EH)(W - EW) &gt; 0\\). So, usually \\((H - EH)(W - EW) &gt; 0\\), and though there will be a number of exceptions, they will be rare enough so that \\(E[(U - EU)(V - EV)] &gt; 0\\).Of course, the magnitude of \\((H - EH)(W - EW)\\) plays a role too.\n\\(V\\) is usually large – meaning above its mean \\(EV\\) – when \\(U\\) is large (i.e. above its mean), and they are usually both small together. Then \\(U - EU\\) and \\(V - EV\\) are usually of the same sign, thus have a positive product. Then \\(Cov(U,V) &gt; 0\\). If on the other hand, one is usually small when the other is large and vice versa, \\(Cov(U,V) &lt; 0\\). This will later lead to the concept of correlation, but that intuition will serve us now.\nNote some properties of scalar covariance.\n\n\\(Cov\\) is bilinear, i.e. \\(Cov(aU,bV) = ab Cov(U,V)\\).\n\\(Cov(U,U) = Var(U)\\).\n\\(Var(U+V) = Var(U) + Var(V) + 2 Cov(U,V)\\) .\n\nThe relations between the various components of \\(X\\) are often characterized by the covariance matrix of \\(X\\), whose entries consist of scalar covariances between pairs of components of a random vector.  It is defined as follows for a \\(k\\)-component random vector. The covariance matrix, denoted by \\(Cov(X)\\), is a \\(k \\textrm{ x } k\\) matrix, and for \\(1 \\leq i,j \\leq k\\),The definition is soewhat overloaded. “Cov” refers both to the covariance between two random variables, say height and weight, and to the covariance of a random vector, which is a matrix. But it will always be clear from context which one is being discussed.\n\\[\nCov(X_i,X_j) = E[(X_i - EX_i) (X_j - EX_j)]\n\\]\nAs an example, here is data on major league baseball players:\n\nlibrary(qeML) \ndata(mlb1) \nhead(mlb1) \n\n        Position Height Weight   Age\n1        Catcher     74    180 22.99\n2        Catcher     74    215 34.69\n3        Catcher     72    210 30.78\n4  First_Baseman     72    210 35.43\n5  First_Baseman     73    188 35.71\n6 Second_Baseman     69    176 29.39\n\nhwa &lt;- mlb1[,-1] \ncov(hwa) \n\n           Height    Weight        Age\nHeight  5.3542814  25.61130 -0.8239233\nWeight 25.6113038 433.60211 12.9110576\nAge    -0.8239233  12.91106 18.6145019\n\ncor(hwa)\n\n            Height    Weight         Age\nHeight  1.00000000 0.5315393 -0.08252974\nWeight  0.53153932 1.0000000  0.14371113\nAge    -0.08252974 0.1437111  1.00000000\n\n\nAgain, we’ll be discussing more of this later, but what about that negative correlation between height and age? It’s near 0, and this could be a sampling artifact, but another possibility is that in this sport, shorter players do not survive as well.\nProperties of the matrix version of covariance:\n\nFor statistically independent random vectors \\(Q\\) and \\(W\\) of the same length,\n\n\\[\nCov(Q+W) = Cov(Q) + Cov(W)\n\\tag{4.1}\\]\n\nFor any nonrandom scalar \\(c\\), and \\(Q\\) a random vector, we have \\(Cov(cQ) = c^2 Cov(Q)\\).\nSay we have a random vector \\(X\\), of length \\(k\\), and a nonrandom matrix \\(A\\) of sise \\(m \\textrm{x} k\\). Then \\(A X\\) is a new random vector \\(Y\\) of \\(m\\) components. It turns out that\n\\[\nCov(Y) = A Cov(X) A'\n\\tag{4.2}\\]\nThe proof is straightforward but tedious, and it will be omittted.\n\\(Cov(X)\\) is a symmetric matrix. This follows from the symmmetry of the definition.\nThe diagonal elements of \\(Cov(X)\\) are the variances of the random variables \\(X_i\\). This follows from the definition of the variance of a random variable.\nIf \\(X\\) is a vector of length 1, i.e. a number, then\n\n\\[\nCov(X) = Var(X)\n\\]\n\nFor any length-\\(k\\) column vector \\(a\\),\n\n\\[\nVar(a'X) = a' ~ Cov(X) ~ a\n\\]\n\nThus \\(Cov(X)\\) is nonnegative definite, meaning that for any length-\\(k\\) column vector \\(a\\)\n\n\\[\na' Cov(X) a \\geq 0\n\\]"
  },
  {
    "objectID": "Ch1.html#your-turn",
    "href": "Ch1.html#your-turn",
    "title": "2  Matrices and Vectors",
    "section": "4.4 Your Turn",
    "text": "4.4 Your Turn\n❄️ Your Turn: The long-run probabilities here turned out to be uniform, with value 0.20 for all five states. In fact, that is usually not the case. Make a small change to \\(P_1\\) – remember to keep the row sums to 1 – and compute a high power to check whether the long-run distribution seems nonuniform.\n❄️ Your Turn: Not every Markov chain, even ones with finitely many states, have long-run distributions. Some chains have periodic states. It may be, for instance, that after leaving state \\(i\\), once can return only after an even number of hops. Modify our example chain here so that states 1 and 5 (and all the others) have that property. Then compute \\(P^n\\) for various large values of \\(n\\) and observe oscillatory behavior, rather than long-run convergence.\n❄️ Your Turn: Consider the following model of a discrete-time, single-server queue:\n\nModel parameters are p (probability of job completion), q (probability of new job arriving) and m (size of the buffer).\nJobs arrive, are served (possibly after queuing) and leave.\nOnly one job can be in service at a time.\nAt each time epoch:\n\nThe job currently in service, if any, will complete with probability p.\nSlightly after a possible job completion, a job in the queue, if any, will start service.\n\na Slightly after that, anew job will arrive with probability q. If the queue is empty, this job starts service. If not, and if the queue is not full, it will join the queue. Otherwise, the job is discarded.\nThe system is memoryless.\nThe current state is the number of jobs in the system, taking on the values 0,1,2,..,m+1; that last state means m jobs in the queue and 1 in service.\n\nFor instance, say p = 0.4, q = 0.2, m = 5, Suppose the current state is 3, so there is a job in service and two jobs in the queue. Our next state will be 2 with probability (0.4) (0.8); it will be 3 with probability (0.4) (0.2), and so on.\nAnalyze this system for the case given above.` Find the approximate long-run distribution, and also the proportion of jobs that get discarded."
  },
  {
    "objectID": "Ch2.html#sec-markovsolve",
    "href": "Ch2.html#sec-markovsolve",
    "title": "3  Matrix Inverse",
    "section": "3.1 Example: Computing Long-Run Markov Distribution",
    "text": "3.1 Example: Computing Long-Run Markov Distribution\nAt the end of the last chapter, we found that for a Markov chain with transition matrix P and stationary distribution \\(\\nu\\),\n\\[\n\\nu = P' \\nu\n\\tag{3.1}\\]\nThis suggests a method for computing \\(\\nu\\), by solving the above equation.\nRewrite it using the identity matrix:Recall that for any square matrix C and identity matrix I of the same size, \\(IC = CI = C\\).\n\\[\n(I - P') \\nu = 0\n\\]\nFor the random walk chain in Chapter 1, we hadIn that particular model, P’ = P, but for most chains this is not the case.\n\\[\nP =\n\\left (\n\\begin{array}{rrrrr}\n0.5 & 0.5 & 0 & 0 & 0\\\\\n0.5 & 0 & 0.5 & 0 & 0\\\\\n0 & 0.5 & 0 & 0.5 & 0\\\\\n0 & 0 & 0.5 & 0 & 0.5\\\\\n0 & 0 & 0 & 0.5 & 0.5 \\\\\n\\end{array}\n\\right )\n\\]\nWith \\(\\nu = (\\nu_1,\\nu_2,\\nu_3,\\nu_4,\\nu_5)'\\), the equation to be solved, \\((I-P') \\nu = \\nu\\), is\n\\[\n\\left (\n\\begin{array}{rrrrr}\n0.5 & -0.5 & 0 & 0 & 0 \\\\\n-0.5 & 1 & -0.5 & 0 & 0 \\\\\n0 & -0.5 & 1 & -0.5 & 0 \\\\\n0 & 0 & -0.5 & 1 & -0.5 \\\\\n0 & 0 & 0 & -0.5 & 0.5 \\\\\n\\end{array}\n\\right )\n\\left (\n\\begin{array}{r}\n\\nu_1 \\\\\n\\nu_2 \\\\\n\\nu_3 \\\\\n\\nu_4 \\\\\n\\nu_5 \\\\\n\\end{array}\n\\right ) =\n\\left (\n\\begin{array}{r}\n0 \\\\\n0 \\\\\n0 \\\\\n0 \\\\\n0 \\\\\n\\end{array}\n\\right )\n\\]\nIf we perform the matrix multiplication, we have an ordinary system of linear equations:\n\\[\n\\begin{array}{r}\n0.5 \\nu_1 - 0.5 \\nu_2 = 0 \\\\\n-0.5 \\nu_1 + \\nu_2 - 0.5 \\nu_3 = 0 \\\\\n-0.5 \\nu_2 + \\nu_3 - 0.5 \\nu_4 = 0 \\\\\n-0.5 \\nu_3 + \\nu_4 - 0.5 \\nu_5 = 0 \\\\\n-0.5 \\nu_4 + 0.5 \\nu_5 = 0 \\\\\n\\end{array}\n\\]\nThis is high school math, and we could solve the equations that way. But this is literally what linear algebra was invented for, solving systems of equations! We will use matrix inverse.\nBut first, we have a problem to solve: The only solution to the above system is with all \\(\\nu_i = 0\\). We need an equation involving a nonzero quantity.\nBut we do have such an equation. The vector \\(\\nu\\) is a stationary distribution for a Markov chain, and thus must sum to 1.0. Let’s replace the last row by that relation:\n\\[\n\\left (\n\\begin{array}{rrrrr}\n0.5 & -0.5 & 0 & 0 & 0 \\\\\n-0.5 & 1 & -0.5 & 0 & 0 \\\\\n0 & -0.5 & 1 & -0.5 & 0 \\\\\n0 & 0 & -0.5 & 1 & -0.5 \\\\\n1 & 1 & 1 & 1 & 1 \\\\\n\\end{array}\n\\right )\n\\left (\n\\begin{array}{r}\n\\nu_1 \\\\\n\\nu_2 \\\\\n\\nu_3 \\\\\n\\nu_4 \\\\\n\\nu_5 \\\\\n\\end{array}\n\\right ) =\n\\left (\n\\begin{array}{r}\n0 \\\\\n0 \\\\\n0 \\\\\n0 \\\\\n1 \\\\\n\\end{array}\n\\right )\n\\]\nMore compactly,\n\\[\nB \\nu = d\n\\]\nMany square matrices \\(A\\) have a multiplicative inverse, denoted by \\(A^{-1}\\), with the property that\n\\[\nA^{-1} A = A A^{-1} = I\n\\]\n(We will often speak of the inverse of \\(A\\). In fact, if \\(A\\) is invertible, its inverse is unique.)\nIf our matrix \\(B\\) is invertible, we can premultiply both sides of our equation above, yielding\n\\[\nB^{-1} d = B^{-1} B \\nu = \\nu\n\\]\nSo, we have obtained our solution for the stationary distribution \\(\\nu\\). We can evaluate it numerically via the R solve function, which finds matrix inverse:\n\nB &lt;-\nrbind(c(0.5,-0.5,0,0,0), c(-0.5,1,-0.5,0,0), c(0,-0.5,1,-0.5,0),\n   c(0,0,-0.5,1,-0.5), c(1,1,1,1,1))\nB\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]  0.5 -0.5  0.0  0.0  0.0\n[2,] -0.5  1.0 -0.5  0.0  0.0\n[3,]  0.0 -0.5  1.0 -0.5  0.0\n[4,]  0.0  0.0 -0.5  1.0 -0.5\n[5,]  1.0  1.0  1.0  1.0  1.0\n\nBinv &lt;- solve(B)\n# check the inverse\nBinv %*% B  # yes, get I (of course with some roundoff error)\n\n              [,1]         [,2]          [,3]          [,4]          [,5]\n[1,]  1.000000e+00 1.942890e-16 -1.387779e-16  1.942890e-16  8.326673e-17\n[2,]  1.942890e-16 1.000000e+00  3.053113e-16 -2.775558e-17  8.326673e-17\n[3,] -2.775558e-17 0.000000e+00  1.000000e+00  0.000000e+00  2.775558e-17\n[4,] -1.665335e-16 4.996004e-16 -3.608225e-16  1.000000e+00 -2.775558e-17\n[5,] -1.665335e-16 7.216450e-16 -4.996004e-16  5.551115e-17  1.000000e+00\n\nnu &lt;- Binv %*% c(0,0,0,0,1)\nnu\n\n     [,1]\n[1,]  0.2\n[2,]  0.2\n[3,]  0.2\n[4,]  0.2\n[5,]  0.2\n\n\nThis confirms our earlier speculation based on powers of \\(P'\\)."
  },
  {
    "objectID": "Ch2.html#sec-matalg",
    "href": "Ch2.html#sec-matalg",
    "title": "3  Matrix Inverse",
    "section": "3.2 Matrix Algebra",
    "text": "3.2 Matrix Algebra\nSeveral properties to note:\n\nIf the inverses of \\(A\\) and \\(B\\) exist, and \\(A\\) and \\(B\\) are conformable, then \\((AB)^{-1}\\) exists and is equal to \\(B^{-1} A^{-1}\\).\nProof: Consider the product \\((AB) (B^{-1} A^{-1})\\). The $B factors give us \\(I\\), leaving \\(A A^{-1}\\), which too is \\(I\\).\n\\((A')^{-1}\\) exists and is equal to \\((A^{-1})'\\).\nProof: Follows immediately from \\(A A^{-1} = I\\).\nIf \\(A\\) is invertible and symmetric, then \\((A^{-1})'\\) is also symmetric.\nProof: For convenience, let \\(B\\) denote \\(A^{-1}\\). Then\n\\[\nAB = I\n\\]\nRecalling from Section 2.10.2 that the transpose of a product is the reverse product of the transposes, we have\n\\[\nI = I' = (AB)' = B'A'\n\\]\nBut since \\(A' = A\\), we have\n\\[\nI = B' A\n\\]\nIn other words, not only is \\(B\\) the inverse of \\(A\\), \\(B'\\) is too! So, \\(B = B'\\)."
  },
  {
    "objectID": "Ch2.html#computation-of-the-matrix-inverse",
    "href": "Ch2.html#computation-of-the-matrix-inverse",
    "title": "3  Matrix Inverse",
    "section": "3.3 Computation of the Matrix Inverse",
    "text": "3.3 Computation of the Matrix Inverse\nFinding the inverse of a large matrix – in data science applications, the number of rows and columns \\(n\\) can easily be hundreds or more – can be computationally challenging. The run time is proportional to \\(n^3\\), and roundoff error can be an issue. Sophisticated algorithms have been developed, such as QR and Choleski decompositions. So in R, we should use, say, qr.solve rather than solve if we are working with sizable matrices.\nThe classic “pencil and paper” method for matrix inversion is instructive, and will be presented here.\n\n3.3.1 Pencil-and-paper computation\nThe basic idea follows the pattern the reader learned for solving systems of linear equations, but with the added twist of involving some matrix multiplication.\nLet’s take as our example\n\\[\nA =\n\\left (\n\\begin{array}{rr}\n4 & 7 \\\\\n-8 & 15  \\\\\n\\end{array}\n\\right )\n\\]\nWe aim to transform this to the 2x2 identity matrix, via a sequence of row operations.\n\n\n3.3.2 Use of elementary matrices\nLet’s multiply row 1 by 1/4, to put 1 in the first element:\n\\[\n\\left (\n\\begin{array}{rr}\n1 & \\frac{7}{4} \\\\\n-8 & 15  \\\\\n\\end{array}\n\\right )\n\\]\nIn matrix terms, that operation is equivalent to premultiplying \\(A\\) by\n\\[\nE_1=\n\\left (\n\\begin{array}{rr}\n\\frac{1}{4} & 0  \\\\\n0 & 1  \\\\\n\\end{array}\n\\right )\n\\]\nWe then add 8 times row 1 to row 2, yielding\n\\[\n\\left (\n\\begin{array}{rr}\n1 & \\frac{7}{4} \\\\\n0 & 29  \\\\\n\\end{array}\n\\right )\n\\]\nThe premultiplier for this operation is\n\\[\nE_2=\n\\left (\n\\begin{array}{rr}\n1 & 0  \\\\\n8 & 1  \\\\\n\\end{array}\n\\right )\n\\]\nMultiply row 2 by 1/29:\n\\[\n\\left (\n\\begin{array}{rr}\n1 & \\frac{7}{4} \\\\\n0 & 1  \\\\\n\\end{array}\n\\right )\n\\]\ncorresponding to\n\\[\nE_3=\n\\left (\n\\begin{array}{rr}\n1 & 0  \\\\\n8 & 1/29  \\\\\n\\end{array}\n\\right )\n\\]\nAnd finally, add -7/4 row 2 to row 1.\n\\[\n\\left (\n\\begin{array}{rr}\n1 & 0 \\\\\n0 & 1  \\\\\n\\end{array}\n\\right )\n\\]\nThe premultiplier is\n\\[\nE_4 =\n\\left (\n\\begin{array}{rr}\n1 & \\frac{7}{4} \\\\\n0 & 1  \\\\\n\\end{array}\n\\right )\n\\]\nNow, how does that give use \\(A^{-1}\\)? The method your were taught probably set up the partioned matrix \\((A,I)\\). The row operations that transformed \\(A\\) to \\(I\\) also transformed \\(I\\) to \\(A^{-1}\\). Here’s why:\nAs noted, the row operations are such that\n\\[\nE_4 E_3 E_2 E_1 A\n\\]\ngive us the final transformed result, i.e. the matrix \\(I\\):\n\\[\n(E_4 E_3 E_2 E_1) A = I\n\\]\nAha! We have found the inverse of \\(A\\)–it’s \\(E_4 E_3 E_2 E_1\\).\nNote too thatRecall that the inverse of a product (if it exists is the reverse product of the inverses.\n\\[\nA = (E_4 E_3 E_2 E_1)^{-1} I = E_4^{-1} E_3^{-1} E_2^{-1} E_1^{-1}\n\\]\nSo apparently the inverses of the elementary matrices \\(E_i\\) also exist, and in fact we can obtain them easily:\nEach \\(E_i^{-1}\\) simply “undoes” its partner. \\(E_1\\), for instance, multiplies the row 1, column 1 element by 1/4, which is “undone” by multiplying that element by 4,\n\\[\nE_1^{-1} =\n\\left (\n\\begin{array}{rr}\n4 & 0 \\\\\n0 & 1  \\\\\n\\end{array}\n\\right )\n\\]\nAlso, to undo the operation of adding 8 times row 1 to row 2, we add -8 times row 1 to row 2:\n\\[\nE_2^{-1} =\n\\left (\n\\begin{array}{rr}\n1 & 0 \\\\\n-8 & 1 \\\\\n\\end{array}\n\\right )\n\\]"
  },
  {
    "objectID": "Ch2.html#sec-noinverse",
    "href": "Ch2.html#sec-noinverse",
    "title": "3  Matrix Inverse",
    "section": "3.4 Nonexistent Inverse",
    "text": "3.4 Nonexistent Inverse\nSuppose our matrix \\(A\\) had been slightly different:\n\\[\nA =\n\\left (\n\\begin{array}{rr}\n4 & 7 \\\\\n-8 & -14  \\\\\n\\end{array}\n\\right )\n\\]\nThis would have led to\n\\[\n\\left (\n\\begin{array}{rr}\n1 & \\frac{7}{4} \\\\\n0 & 0  \\\\\n\\end{array}\n\\right )\n\\]\nThis cannot lead to \\(i\\), indicating that \\(A^{-1}\\) does not exist, and the matrix is said to be singular. And it’s no coincidence that row 2 of \\(A\\) is double row 1. This has many implications, as will be seen in our chapter on vector spaces."
  },
  {
    "objectID": "Ch2.html#determinants",
    "href": "Ch2.html#determinants",
    "title": "3  Matrix Inverse",
    "section": "3.5 Determinants",
    "text": "3.5 Determinants\nThis is a topic that is quite straightforward and traditional, even old-fashioned–in fact, too old-fashioned, according to mathematician Sheldon Axler. The theme of his book, Linear Algebra Done Right, is that determinants are overemphasized. He relegates the topic to the very end of the book. Yet determinants do appear often in applied linear algebra settings. Moreover, they will be convenient to use in explaing very concepts in this book on linear algebra in Data Science.\nBut why place the topic in this particular chapter? The answer lies in the fact that earlier in this chapter we had the proviso “If \\((A'A)^{-1}\\) exists.” The following property of determinants is then relevant:\n\nA square matrix \\(G\\) is invertible if and only if \\(det(G) \\neq 0\\).\n\nThere are better ways to ascertain invertibility than this, but it is conceptually helpful. Determinants play a similar role in the topic of eigenvectors in Chapter 5.\n\n3.5.1 Definition\nThe standard definition is one of the ugliest, in all of mathematics. Instead we will define the term using one of the methods for calculating determinants.\n\nConsider an \\(r \\textrm{ x } r\\) matrix \\(G\\). For \\(r = 2\\), write \\(G\\) as\n\\[\nG =\n\\left (\n\\begin{array}{rr}\na & b \\\\\nc & d  \\\\\n\\end{array}\n\\right )\n\\]\nand define \\(\\det(G)\\) to be \\(ad -bc\\). For \\(r &gt; 2\\), define submatrices as follows.\n\\(G_i\\) is the \\((r-1) \\textrm{ x } (r-1)\\) submatrix obtained by removing row 1 and column \\(j\\) from \\(G\\). Then \\(\\det(G)\\) is defined recursively as\n\\[\n\\sum_{i=1}^r (-1)^{i+1} \\det(G_i)\n\\]\n\nActually, we can alternatively remove row \\(i\\) instead of row 1. If \\(i\\) is an odd number, the same recursive formula holds, but for even \\(i\\), replace \\((-1)^{i+1}\\) by \\((-1)^i\\).\nFor instance, consider\n\\[\nM =\n\\left (\n\\begin{array}{rrr}\n5 & 1 & 0 \\\\\n3 & -1 & 7 \\\\\n  0 & 1 & 1 \\\\\n  \\end{array}\n\\right )\n\\]\nThe \\(\\det(M) = 5(-1 - 7) - 1(3 - 0) + 0 = -43\\).\nA glimpse at the classical definition:\nUsing the same approach as in the last computation, we would find that the determinant of a general \\(3 \\textrm{ x } 3\\) matrix\n\\[\n\\left (\n\\begin{array}{rrr}\na & b & c \\\\\nd & e & f \\\\\ng & h & i \\\\\n  \\end{array}\n\\right )\n\\]\nis\n\\[\naei + bfg + cdh - afh - bdi - ceg\n\\]\nEach term here is involves a product of 3 of the elements of the matrix. In general, the determinant involves sums and differences of permuted products of distinct elements of the matrix, as we see above. The formation of the terms in general, and the determination of + and - signs, is done in complex but precise manner that we will not present here. But the reader should at least keep in mind that each term is a product of \\(n\\) elements of the matrix, a fact that will be relevant in the sequel.\n\n\n3.5.2 Properties\nWe state these without proof:\n\n\\(G^{-1}\\) exists if and only if \\(\\det(G) \\neq 0\\)\n\\(\\det(GH) = \\det(G) \\det(H)\\)"
  },
  {
    "objectID": "Ch2.html#sec-mvn",
    "href": "Ch2.html#sec-mvn",
    "title": "3  Matrix Inverse",
    "section": "3.6 The Multivariate Normal Distribution Family",
    "text": "3.6 The Multivariate Normal Distribution Family\nThe familiar “bell-shaped curve” refers to the normal (or Gaussian) family, whose densities have the form\n\\[\n\\frac{1}{\\sigma \\sqrt{2 \\pi}}\ne^{-0.5 (\\frac{t-\\mu}{\\sigma})^2}\n\\]\nThe values of \\(\\mu\\) and \\(\\sigma\\) are the mean and standard deviation. But what if we have a random vector, say of length \\(k\\)? Is there a generalized normal family?\n\n3.6.1 Example: k = 2\nThe answer is yes. Here is an example for \\(k = 2\\):\n\n\n\n3D bell density\n\n\n\n\n3.6.2 General form\nWell, then, what is the form of the \\(k\\)-dimensional density function? Just as the univariate normal family is parameterized by mean and variance, the multivariate one is parameterized via its mean vector \\(\\mu\\) and a covariance matrix \\(\\Sigma\\), a term we will define in a later chapter. The specific form is\n\\[\n(2\\pi)^{-k/2} \\det(\\Sigma)^{-k/2}\ne^{-0.5 (t-\\mu)'(\\Sigma)^{-1}(t-\\mu)}\n\\]\nNote the intuition:\n\nInstead of \\(1/\\sigma^2\\), i.e. instead of dividing by variance, we “divide by \\(\\Sigma\\),” intuitively viewing matrix inverse as a “reciprocal” of a matrix.\nIn other words, covariance matrices operate roughly like generalized variances.\nInstead of squaring the scalar \\(t - \\mu\\), we “square” it in the vector case by peforming a \\(w'w\\) operation, albeit with \\(\\Sigma^{-1}\\) in the middle.\n\nClearly, we should not stretch these analogies very far, but they do help our intuition here.\n\n\n3.6.3 Properties\n\nTheorem 3.1 if a random vector is MV normally distributed, then the conditional distribution of any one of its components \\(Y\\), given the others \\(X_{others} = t\\) (note that \\(t\\) is a vector if \\(k &gt; 2\\)) has the following properties:\n\nIt has a (univariate) normal distribution.\nIts mean \\(E(Y | X_{others}) = t\\) linear in \\(t\\).\nIts variance \\(Var(Y | X_{others}) = t\\) does not involve \\(t\\).\n\nThese of course are the classical assumptions of linear regression models. They actually come from the MV normal model.\n\n\nProof. This comes out of writing down the conditional density (overall density divided by marginal), and then doing some algebra.\n\\(\\square\\)\n\n\nTheorem 3.2 If \\(X\\) is a multivariate-normal random vector, then so is \\(AX\\) for any conformable nonrandom matrix \\(A\\).\n\n\nProof. Again, perform direct evaluation of the density.\n\\(\\square\\)\n\n\nTheorem 3.3 A random vector \\(X\\) has a multivariate normal distribution if and only if \\(w'X\\) has a univariate normal distribution for all conformable nonrandom vectors \\(w\\).\n\n::: {#thm-mvclt}"
  },
  {
    "objectID": "Ch2.html#the-multivariate-central-limit-theorem",
    "href": "Ch2.html#the-multivariate-central-limit-theorem",
    "title": "3  Matrix Inverse",
    "section": "3.7 The Multivariate Central Limit Theorem",
    "text": "3.7 The Multivariate Central Limit Theorem\nLet \\(X_1, X_2,...\\) be a sequence on statistically independent random vectors, with common distribution multivariate normal with mean vector \\(\\mu\\) and covariance matrix \\(\\Sigma\\). Write\n\\[\n\\bar{X} = \\frac{X_1+...+X_n}{n}\n\\]\nThen the distribution of the random vector\n\\[\nW_n = \\sqrt{n} (\\bar{X} - \\mu)\n\\]\ngoes to multivariate normal with the 0 vector as mean and covariance matrix \\(\\Sigma\\).The usual form would involve the “square root” of a matrix, but we will not discuss that concept until our chapter on inner product spaces.\n\nProof. Theorem 3.3 reduces the problem to the univariate case, where we know the Central Limit Theorem holds.\n\\(\\square\\)"
  },
  {
    "objectID": "Ch2.html#your-turn",
    "href": "Ch2.html#your-turn",
    "title": "3  Matrix Inverse",
    "section": "3.8 Your Turn",
    "text": "3.8 Your Turn\n❄️ Your Turn: Prove the assertions in Section 3.2. Note that for the identity matrix \\(I\\), \\(I' = I\\).\n❄️ Your Turn: In Section 3.3.2, find the inverses of \\(E_3\\) and \\(E_4\\) using similar reasoning, and thus find \\(A^{-1}\\).\n❄️ Your Turn: If you are familiar with recursive calls, write a function \\(\\verb+dt(a)+\\) to compute the determinant of a square matrix \\(A\\).\n❄️ Your Turn: The determinant of a 3 x 3 matrix\n\\[\nM =\n\\left (\n\\begin{array}{rrr}\na & b & d \\\\\nd & e & f \\\\\ng & h & i \\\\\n\\end{array}\n\\right )\n\\]\nis\n\\[\naei+bfg+cdh-ceg-bdi-afh\n\\]\nSuppose the elements of \\(M\\) are independent random variables with uniform distributions on (0,1). Argue that P(M is invertible) = 1.\n❄️ Your Turn: Show that in the scalar context,\n\\[\nCov(X,Y) = E(XY) - EX ~ EY\n\\]\n❄️ Your Turn: Show that in the vector context,\n\\[\nCov(X) = E(X X') - (EX) (EX)'\n\\]"
  },
  {
    "objectID": "Ch2a.html#sec-cov",
    "href": "Ch2a.html#sec-cov",
    "title": "4  Covariance Matrices, MV Normal Distribution, Delta Method",
    "section": "4.1 Covariance",
    "text": "4.1 Covariance\n\n4.1.1 Scalar covariance\nRecall first the notion in statistics of covariance: Given a pair of random variables \\(U\\) and \\(V\\), their covariance is defined by\n\\[\nCov(U,V) = E[(U - EU)(V - EV)]\n\\]\nLoosely speaking, this measures the degree to which the two random variables vary together. Consider for instance human height \\(H\\) and \\(W\\). Taller people tend to also be heavier. Say we sample many people from a population. Most of those who are taller than average, i.e. \\(H &gt; EH\\), will also be heavier than average, \\(W &gt; EW\\), making \\((H - EH)(W - EW) &gt; 0\\). Similarly, shorter people tend to be lighter, i.e. we often have \\(H &lt; EH\\) and \\(W &lt; EW\\), but then we still have \\((H - EH)(W - EW) &gt; 0\\). So, one way or the other, usually \\((H - EH)(W - EW) &gt; 0\\), and though there will be a number of exceptions, they will be rare enough so that\n\\(E[(H - EH)(W - EW)] &gt; 0\\).Of course, the magnitude of \\((H - EH)(W - EW)\\) plays a role too.\nIn other words, \\(Cov(H,W) &gt; 0\\). Similarly if \\(U\\) is often large when \\(V\\) is small, and vice versa, we will likely have \\(Cov(H,W) &lt; 0\\).\nIf this sounds like correlation to you, then your hunch is correct. Covariance will indeed later lead to the concept of correlation, but that intuition will serve us now.\nNote some properties of scalar covariance.\n\nSymmetry:\n\n\\[\nCov(U,V) = Cov(V,U)\n\\]\n\n\\(Cov\\) is bilinear:\n\n\\[\nCov(aU,bV) = ab Cov(U,V)\n\\]\n\nVariance as a special case:\n\n\\[\nCov(U,U) = Var(U)\n\\]\n\nCross-product term:\n\n\\[\nVar(U+V) = Var(U) + Var(V) + 2 Cov(U,V)\n\\]\n\n“Short cut” formula:\n\n\\[\nCov(U,V) = E(UV) - (EU) (EV)\n\\tag{4.1}\\]\n\n\n4.1.2 Covariance matrices\nThe relations between the various components of \\(X\\) are often characterized by the covariance matrix of \\(X\\), whose entries consist of scalar covariances between pairs of components of a random vector.  It is defined as follows for a \\(k\\)-component random vector \\(X\\) . The covariance matrix, denoted by \\(Cov(X)\\), is a \\(k \\textrm{ x } k\\) matrix, and for \\(1 \\leq i,j \\leq k\\), its row \\(i\\), column \\(j\\) element isThe notation is somewhat overloaded. “Cov” refers both to the covariance between two random variables, say height and weight, and to the covariance matrix of a random vector. But it will always be clear from context which one is being discussed.\n\\[\nCov(X)_{ij} = Cov(X_i,X_j)\n\\]\nAs an example, here is data on major league baseball players:\n\nlibrary(qeML) \ndata(mlb1) \nhead(mlb1) \n\n        Position Height Weight   Age\n1        Catcher     74    180 22.99\n2        Catcher     74    215 34.69\n3        Catcher     72    210 30.78\n4  First_Baseman     72    210 35.43\n5  First_Baseman     73    188 35.71\n6 Second_Baseman     69    176 29.39\n\nhwa &lt;- mlb1[,-1] \ncov(hwa) \n\n           Height    Weight        Age\nHeight  5.3542814  25.61130 -0.8239233\nWeight 25.6113038 433.60211 12.9110576\nAge    -0.8239233  12.91106 18.6145019\n\ncor(hwa)\n\n            Height    Weight         Age\nHeight  1.00000000 0.5315393 -0.08252974\nWeight  0.53153932 1.0000000  0.14371113\nAge    -0.08252974 0.1437111  1.00000000\n\n\nAgain, we’ll be discussing more of this later, but what about that negative correlation between height and age? It’s near 0, and this could be a sampling artifact, but another possibility is that in this sport, shorter players do not survive as well.\nProperties of the matrix version of covariance:\n\nMatrix form of definition:\n\\[ Cov(X) = E[(X - X) (X - EX)']\n\\tag{4.2}\\]\n(Note the dimensions: \\(X\\) is a column vector, say \\(k \\textrm{ x } 1\\), so \\((X - X) (X - EX)'\\) is \\(k \\textrm{ x } k\\). The expected value is then of that size as well.)\nFor statistically independent random vectors \\(Q\\) and \\(W\\) of the same length,\n\n\\[\nCov(Q+W) = Cov(Q) + Cov(W)\n\\tag{4.3}\\]\n\nFor any nonrandom scalar \\(c\\), and \\(Q\\) a random vector, we have\n\n\\[\nCov(cQ) = c^2 Cov(Q)\n\\]\n\nSay we have a random vector \\(X\\), of length \\(k\\), and a nonrandom matrix \\(A\\) of size \\(m \\textrm{ x } k\\). Then \\(A X\\) is a new random vector \\(Y\\) of \\(m\\) components. It turns out that\n\\[\nCov(Y) = A Cov(X) A'\n\\tag{4.4}\\]\nThe proof is straightforward but tedious, and will be omittted.\n\\(Cov(X)\\) is a symmetric matrix. This follows from the symmetry of the definition.\nThe diagonal elements of \\(Cov(X)\\) are the variances of the random variables \\(X_i\\). This follows from \\(Cov(U,U) = Var(U)\\) for scalar \\(U\\).\nIf \\(X\\) is a vector of length 1, i.e. a number, then\n\n\\[\nCov(X) = Var(X)\n\\]\n\nFor any length-\\(k\\) column vector \\(a\\),\n\n\\[\nVar(a'X) = a' ~ Cov(X) ~ a\n\\tag{4.5}\\]\n\nThus \\(Cov(X)\\) is nonnegative definite, meaning that for any length-\\(k\\) column vector \\(a\\)\n\n\\[\na' Cov(X) a \\geq 0\n\\]\n\n\n4.1.3 Cross-covariance\nThe matrix \\(Cov(X)\\) represents the covariance of a vector \\(X\\) with itself. But we can also speak of the covariance of one vector \\(X\\) with another vector \\(Y\\), termed the cross-covariance between them. Its definition is a natural extension of covariance matrices:\n\\[\nCov(X,Y) = E[(X - EX) (Y- EY)]\n\\]\nSay \\(X\\) and \\(Y\\) are of lengths \\(m\\) and \\(n\\). Then \\(Cov(X,Y)\\) will be of size \\(m \\textrm{ x } n\\), with\n\\[\nCov(X,Y)_{ij} = Cov(X_i,Y_j)\n\\]\nwhere the latter covariance is scalar."
  },
  {
    "objectID": "Ch2a.html#sec-mvn",
    "href": "Ch2a.html#sec-mvn",
    "title": "4  Covariance Matrices, MV Normal Distribution, Delta Method",
    "section": "4.2 The Multivariate Normal Distribution Family",
    "text": "4.2 The Multivariate Normal Distribution Family\nThe familiar “bell-shaped curve” refers to the normal (or Gaussian) family, whose densities have the form\n\\[\n\\frac{1}{\\sigma \\sqrt{2 \\pi}}\ne^{-0.5 (\\frac{t-\\mu}{\\sigma})^2}\n\\]\nThe values of \\(\\mu\\) and \\(\\sigma\\) are the mean and standard deviation. But what if we have a random vector, say of length \\(k\\)? Is there a generalized normal family?\n\n4.2.1 Example: k = 2\nThe answer is yes. Here is an example for \\(k = 2\\):\n\n\n\n3D bell density\n\n\n\n\n4.2.2 General form\nWell, then, what is the form of the \\(k\\)-dimensional density function? Just as the univariate normal family is parameterized by mean and variance, the multivariate one is parameterized via mean vector \\(\\mu\\) and covariance matrix \\(\\Sigma\\). The form is\n\\[\n(2\\pi)^{-k/2} \\det(\\Sigma)^{-k/2}\ne^{-0.5 (t-\\mu)'(\\Sigma)^{-1}(t-\\mu)}\n\\]\nNote the intuition:\n\nInstead of \\(1/\\sigma^2\\), i.e. instead of dividing by variance, we “divide by \\(\\Sigma\\),” intuitively viewing matrix inverse as a “reciprocal” of a matrix.\nIn other words, covariance matrices operate roughly like generalized variances.\nInstead of squaring the scalar \\(t - \\mu\\), we “square” it in the vector case by peforming a \\(w'w\\) operation, albeit with \\(\\Sigma^{-1}\\) in the middle.\n\nClearly, we should not stretch these analogies very far, but they do help our intuition here.\n\n\n4.2.3 Properties\n\nTheorem 4.1 If a random vector is MV normally distributed, then the conditional distribution of any one of its components \\(Y\\), given the others \\(X_{others} = t\\) (note that \\(t\\) is a vector if \\(k &gt; 2\\)) has the following properties:\n\nIt has a (univariate) normal distribution.\nIts mean \\(E(Y | X_{others}) = t\\) is linear in \\(t\\).\nIts variance \\(Var(Y | X_{others} = t)\\) does not involve \\(t\\).\n\nThese of course are the classical assumptions of linear regression models. They actually come from the MV normal model.\n\n\nProof. This comes out of writing down the conditional density (overall density divided by marginal), and then doing some algebra.\n\\(\\square\\)\n\n\nTheorem 4.2 If \\(X\\) is a multivariate-normal random vector, then so is \\(AX\\) for any conformable nonrandom matrix \\(A\\).\n\n\nProof. Again, perform direct evaluation of the density.\n\\(\\square\\)\n\n\nTheorem 4.3 (Cramer-Wold Theorem) A random vector \\(X\\) has a multivariate normal distribution if and only if \\(w'X\\) has a univariate normal distribution for all conformable nonrandom vectors \\(w\\).\n\n\nProof. “Only if” follows from above. “If” part too complex to present here.\n% NM \\(\\square\\)\n\n\nTheorem 4.4 (The Multivariate Central Limit Theorem) Let \\(X_1, X_2,...\\) be a sequence on statistically independent random vectors, with common distribution multivariate normal with mean vector \\(\\mu\\) and covariance matrix \\(\\Sigma\\). Write\n\\[\n\\bar{X} = \\frac{X_1+...+X_n}{n}\n\\]\nThen the distribution of the random vector\n\\[\nW_n = \\sqrt{n} (\\bar{X} - \\mu)\n\\]\ngoes to multivariate normal with the 0 vector as mean and covariance matrix \\(\\Sigma\\).The usual form would involve the “square root” of a matrix, but we will not discuss that concept until our chapter on inner product spaces.\n\n` ::: {.proof}\nTheorem 3.3 reduces the problem to the univariate case, where we know the Central Limit Theorem holds.\n\\(\\square\\)\n:::"
  },
  {
    "objectID": "Ch2a.html#multinomial-random-vectors-have-approximate-multivariate-normal-distributions",
    "href": "Ch2a.html#multinomial-random-vectors-have-approximate-multivariate-normal-distributions",
    "title": "4  Covariance Matrices, MV Normal Distribution, Delta Method",
    "section": "4.3 Multinomial Random Vectors Have Approximate Multivariate Normal Distributions",
    "text": "4.3 Multinomial Random Vectors Have Approximate Multivariate Normal Distributions\nRecall that a multinomial random vector is the mathematial analog of an R factor variable – a categorical variable with \\(k\\) levels/categories. Just as a binomial random variable represents the number of “successes” in \\(n\\) “trials,” a multinomial random vector represents the numbers of successes in each of the \\(k\\) categories.\nLet’s write such a random vector as\n\\[\nX =\n\\left (\n\\begin{array}{r}\nN_1 \\\\\n... \\\\\nN_k \\\\\n\\end{array}\n\\right )\n\\]\nLet \\(p_i\\) be the probability of a trial having outcome \\(i-1,...,k\\). Note the following:\n\n\\(N_1+...+N_k = n\\)\nThe marginal distribution of \\(N_i\\) is binomial, with success probability \\(p_i\\) and \\(n\\) trials.\n\nSo for instance if we roll a fair die 10 times, then \\(N_i\\) is the number of trials in which the roll’s outcome was \\(i\\) dots and \\(p_i = 1/6\\) , \\(i=1,2,3,4,5,6\\).\nLet’s find \\(Cov(X)\\). Define the indicator vector\n\\[\nI_i =\n\\left (\n\\begin{array}{r}\nI_{i1} \\\\\n... \\\\\nI_{ik}  \\\\\n\\end{array}\n\\right )\n\\]\nHere \\(I_{ij}\\) is 1 or 0, depending on whether trial \\(i\\) resulted in category \\(j\\). (A 1 “indicates” that caregory \\(j\\) occurred.)\nThe key is that\n\\[\nX = \\sum_{i=1}^n I_i\n\\tag{4.6}\\]\nFor instance, in the die-rolling example, the first component on the right-hand side is the number of rolls in which we got 1 dot, and that is by definition the same as \\(N_1\\), the first component of \\(X\\).\nSo there we have it – \\(X\\) is a sum of independent, identically distributed random vectors, so by the Multivariate Central Limit Theorem, \\(X\\) has an approximate multivariate normal distribution. Now, what are the mean vector and covariance matrix in that distribution?\nFrom our discussion above, we know that\n\\[\nEX =\n\\left (\n\\begin{array}{r}\nnp_1 \\\\\n... \\\\\nnp_k \\\\\n\\end{array}\n\\right )\n\\]\nWhat about \\(Cov(X)\\)? Again, recognizing that Equation 4.6 is a sum of independent terms, Equation 4.3 tells us that\n\\[\nCov(X) = Cov(\\sum_{i=1}^n I_i) = \\sum_{i=1}^n Cov(I_i) = n Cov(I_1),\n\\]\nthat last equality reflecting that the \\(I_i\\) are identically distributed (the trials all have the same probabilistic behavior).\nNow to evaluate that covariance matrix, consider two specific elements \\(I_{1j}\\) and \\(I_{im}\\) of \\(I_1\\). Recall, those elements are equal to 1 or 0, depending on whether the first trial results in Categories j and m, respectively. Then what is \\(Cov(I_{1j},I_{1m})\\)? From Equation 4.1, we have\n\\[\nCov(I_{1j},I_{1m}) = E(I_{1j} I_{1m}) - (EI_{1j}) (EI_{1m})\n\\tag{4.7}\\]\nConsider the two cases:\n\n\\(i=j\\): Here \\(E(I_{1j}^2) = E(I_{1j}) = p_j\\). Thus\n\n\\[\nCov(I_{1j},I_{1m}) = p_j (1-p_j)\n\\]\n\n\\(i \\neq j\\): Each \\(I_s\\) consists of one 1 and \\(k-1\\) 0s. Thus \\(E(I_{1j} I_{1m}) = 0\\) and\n\n\\[\nCov(I_{1j},I_{1m}) = -p_j p_m\n\\]"
  },
  {
    "objectID": "Ch2a.html#the-delta-method",
    "href": "Ch2a.html#the-delta-method",
    "title": "4  Covariance Matrices, MV Normal Distribution, Delta Method",
    "section": "4.4 The Delta Method",
    "text": "4.4 The Delta Method\nThis is one of the most useful simple tools in statistics.\n\n4.4.1 Review: confidence intervals, standard errors\nTo set the stage, let’s review the statistical concepts of confidence interval and standard error. Say we have an estimator \\(\\widehat{\\theta}\\) of some population parameter \\(\\theta\\), e.g. \\(\\bar{X}\\) for a population mean \\(\\mu\\).\n\nLoosely speaking, the term standard error of is our estimate of \\(\\sqrt{Var(\\widehat{\\theta})}\\). More precisely, suppose that \\(\\widehat{\\theta}\\) is asymptotically normal. The standard error is an estimate of the standard deviation of that normal distribution. For this reason, It is customary to write \\(AVar(\\widehat{\\theta})\\) rather than \\(Var(\\widehat{\\theta})\\).\nThis can be used to form a confidence interval (see below), but also stands on its own as an indication of the accuracy of \\(\\widehat{\\theta}\\).\nA, say 95%, confidence interval for \\(\\mu\\) is then\n\n\\[\n\\widehat{\\theta} \\pm 1.96 \\textrm{ s.e.}(\\widehat{\\theta})\n\\]\nThe 95% figure means that of all possible samples of the given size from the population, 95% of the resulting confidence intervals will contain \\(\\theta\\).\n\n\n4.4.2 Delta method: motivating example\nNow, for the delta method, as a first example, say we are estimating a population mean \\(\\mu\\) and are also interested in estimating \\(\\log(\\mu)\\).\nWe will probably use the sample mean \\(\\bar{X}\\) to estimate \\(\\mu\\), and thus use \\(W = \\log{\\bar{X}}\\) to estimate \\(\\log(\\mu)\\).  But how do we obtain a standard error for \\(W\\)?If we just need to form a confidence interval for \\(\\log(\\mu)\\), we can form a CI for \\(\\mu\\) and then take the log of both endpoints. But again, standard errors are of interest in their own right.\n\n\n4.4.3 Use of the Central Limit Theorem\nThe Central Limit Theorem tells as that \\(\\bar{X}\\) is asymptotically normally distributed. But what about \\(\\log{\\bar{X}}\\)?\nFrom calculus, we know that a smooth function \\(f\\) can be written as a Taylor series,\n\\[\nf(x) = f(x_0) + f'(x_0) (x-x_0) + f''(x_0) (x-x_0)^2 /2 + ...\n\\]\nwhere here “’” denotes derivative rather than matrix transpose.\nIn our case here, setting \\(f(t) = \\log{t}\\), \\(x_0 = \\mu\\) and \\(x = \\bar{X}\\), we have\n\\[\nW = \\log{\\mu} + \\log'({\\mu}) (\\bar{X}-\\mu) + \\log''({\\mu}) (\\bar{X}-\\mu)^2 /2 + ...\n\\]\nand \\(\\log'(t) = 1/t\\) and so on.\nThe key point is that as n grows, \\(\\bar{X}-\\mu\\) goes to 0, and \\((\\bar{X}-\\mu)^2\\) goes to 0 even faster. Using theorems from probability theory, one can show that, in the sense of distribution,\n\\[\nW \\approx log(\\mu) + log'(\\mu) (\\bar{X}-\\mu)\n\\]\nIn other words, \\(W\\) has an approximate normal distribution that has mean \\(log(\\mu)\\) and variance\n\\[\n\\frac{1}{\\mu^2} \\sigma^2/n\n\\]\nwhere \\(\\sigma^2\\) is the population variance \\(Var(X)\\) . We estimate the latter by the usual \\(S^2\\) quantity, and thus have our standard error,\n\\[\n\\textrm{s.e.}(W) = \\frac{S}{\\bar{X} \\sqrt{n}}\n\\]\n\n\n4.4.4 Use of the Multivariate Central Limit Theorem\nNow, what if the function \\(f\\) has two arguments instead of one? The above linear approximation is now\n\\[\nf(v,w) \\approx f(v_0,w_0) + f_1(v_0,w_0) (v-v_0) + f_2(v_0,w_0)(w-w_0)\n\\tag{4.8}\\]\nwhere \\(f_1\\) and \\(f_2\\) are partial derivatives,A partial derivative of a function of more than one variable is the derivative with respect to one of those variables. E.g. \\(\\partial/\\partial v ~ vw^2 = w^2\\) and \\(\\partial/\\partial w ~ vw^2 = 2vw\\).\n\\[\nf_1(v,w) = \\frac{\\partial}{\\partial v} f(v,w)\n\\]\n\\[\nf_2(v,w) = \\frac{\\partial}{\\partial w} f(v,w)\n\\]\nSo if we are estimating, for instance, a population quantity \\((\\alpha,\\beta)'\\) by \\((Q,R)'\\), standard error of the latter is\n\\[\n\\sqrt{\nf_1^2 (Q,R) AVar(Q) +\nf_2^2 (Q,R) AVar(R) +\n2 f_1(Q,R) f_2(Q,R) ACov(Q,R)\n}\n\\]\nAs usual, use of matrix notation can help clean up messy expressions like this. The gradient of \\(f\\), say in the two-argument case as above, is the vector\n\\[\n\\nabla f =\n\\left (\n\\begin{array}{r}\nf_1 (v_0,w_0) \\\\\nf_2 (v_0,w_0) \\\\\n\\end{array}\n\\right )\n\\]\nso that Equation 4.8 can be written as\n\\[\nf(v,w) \\approx f(v_0,w_0) + (\\nabla f)'\n\\left (\n\\begin{array}{r}\nv-v_0 \\\\\nw-w_0 \\\\\n\\end{array}\n\\right )\n\\]\nThen from Equation 4.5,\n\\[\nAVar[f(Q,R)] = (\\nabla f)' AV(Q,R) (\\nabla f)\n\\tag{4.9}\\]\n\n\n4.4.5 Example: ratio of two means\nSay our sample data consists of mother-daughter pairs,\n\\[\n\\left (\n\\begin{array}{r}\nM \\\\\nD \\\\\n\\end{array}\n\\right )\n\\]\nrepresenting the heights of mother and daughter. Denote the population mean vector by\n\\[\n\\nu =\n\\left (\n\\begin{array}{r}\n\\mu_M \\\\\n\\mu_D \\\\\n\\end{array}\n\\right )\n\\]\nWe might be interested in the ratio \\(\\omega = \\mu_D / \\mu_M\\). Our estimator will be \\(\\widehat{\\omega} = \\bar{D} / \\bar{M}\\), the ratio of the sample means.\nSo take \\(Q = \\bar{D}\\) and \\(R = \\bar{M}\\). Then in Equation 4.9, with \\(f(q,r) = q/r\\)\n\\[\n\\nabla{f} =\n\\left (\n\\begin{array}{r}\n1/r \\\\\n-q/r^2  \\\\\n\\end{array}\n\\right )\n\\]\nwhich in our application here we would approximate by\n\\[\n\\nabla{f} =\n\\left (\n\\begin{array}{r}\n1/\\bar{M} \\\\\n-\\bar{D}/\\bar{M}^2  \\\\\n\\end{array}\n\\right )\n\\]\nAs to \\(AVar(v,w)\\) in Equation 4.9, we would use the multivariate analog of the usual \\(S^2\\) in the univariate case, taking advantage of Equation 4.2. One must be careful, though, making sure we choose the appropriate quantity for \\(AVar(v,w)\\).\nFor instance, in our ratio example here, \\((Q,R)'\\) is \\((\\bar{M},\\bar{D}\\). To obtain \\(AVar(v,w)\\), let’s first look at the covariance matrix \\(\\Sigma\\),\n\\[\n\\Sigma = E[(M - EM) (D - ED)']\n\\]\nThis is the average of \\((M - EM) (D - ED)\\) in the population. The sample analog average isThe reader may recall that in estmating a variance, it is customary to divide by \\(n-1\\) instead of \\(n\\), due to unbiasedness. The same is true for the multivariate analog of variance, i.e. covariance matrices, but we will use \\(n\\) to retain the sample analog theme.\n\\[\n\\widehat{\\Sigma} =\n\\frac{1}{n}\n\\sum_{i=1}^n (M_i - \\bar{M}) (D_i - \\bar{D})'\n\\]\nwhere \\((M_i,D_i)\\) represents the \\(i^{th}\\) mother-daughter pair in our dataset.\nBut \\(\\widehat{{\\Sigma}}\\) is not our \\(AVar(v,w)\\) here, because we need, for instance, the variance of \\(\\bar{M}\\) rather than the variance of \\(M\\). Recall that the former is \\(1/n\\) times the latter. So in this case we have\n\\[\nAVar(v,w) = \\frac{1}{n} \\widehat{\\Sigma}\n\\]\nSo now we can obtain a standard error for \\(\\widehat{\\omega}\\):\n\\[\n\\sqrt{\n\\frac{1}{n}\n\\nabla f' \\widehat{\\Sigma} \\nabla f\n}\n\\]\nfrom which we can form a confidence interval.\nIn R functions to do parametric regression modeling, Maximum Likelihood Estimation and so on, \\(AVar(v,w)\\) is available from the function’s return value."
  },
  {
    "objectID": "Ch2a.html#example-iranian-churn-data",
    "href": "Ch2a.html#example-iranian-churn-data",
    "title": "4  Covariance Matrices, MV Normal Distribution, Delta Method",
    "section": "4.5 Example: Iranian Churn Data",
    "text": "4.5 Example: Iranian Churn Data\nHere we predict whether a telecom customer will move to another provider. Here we illustrate how to obtain \\(AVar(v,w)\\).\n\ndata(IranianChurn)\nglmOut &lt;- glm(Exited ~ ., data = iranChurn, family = binomial)\nvcov(glmOut)\n\n                   (Intercept)   CreditScore GeographyGermany GeographySpain\n(Intercept)       5.993100e-02 -5.051370e-05    -1.589878e-04  -1.388436e-03\nCreditScore      -5.051370e-05  7.859301e-08    -2.683572e-07  -2.454056e-07\nGeographyGermany -1.589878e-04 -2.683572e-07     4.579770e-03   1.678887e-03\nGeographySpain   -1.388436e-03 -2.454056e-07     1.678887e-03   4.989715e-03\nGenderMale       -1.350817e-03  2.156023e-07     2.008595e-05  -2.200094e-05\nAge              -2.685656e-04 -7.969688e-09     6.042109e-06  -2.107579e-06\nTenure           -4.049072e-04  1.614151e-09    -6.645863e-06   3.828636e-06\nBalance          -2.936332e-08  1.033099e-13    -1.358788e-08  -2.156656e-11\nNumOfProducts    -3.775016e-03 -8.271320e-08    -3.655500e-04  -3.357316e-05\nHasCrCard1       -2.595873e-03  2.066409e-07    -7.611739e-05   3.603876e-05\nIsActiveMember1   5.010389e-04 -3.102862e-07    -1.008754e-04  -3.954646e-05\nEstimatedSalary  -2.301952e-08  1.070775e-12    -7.124867e-11  -8.778366e-11\n                    GenderMale           Age        Tenure       Balance\n(Intercept)      -1.350817e-03 -2.685656e-04 -4.049072e-04 -2.936332e-08\nCreditScore       2.156023e-07 -7.969688e-09  1.614151e-09  1.033099e-13\nGeographyGermany  2.008595e-05  6.042109e-06 -6.645863e-06 -1.358788e-08\nGeographySpain   -2.200094e-05 -2.107579e-06  3.828636e-06 -2.156656e-11\nGenderMale        2.968982e-03 -4.697135e-06 -7.714322e-06 -9.609423e-10\nAge              -4.697135e-06  6.633235e-06 -2.330915e-07  4.769766e-11\nTenure           -7.714322e-06 -2.330915e-07  8.751351e-05 -1.419504e-11\nBalance          -9.609423e-10  4.769766e-11 -1.419504e-11  2.644146e-13\nNumOfProducts     5.853316e-05  2.887014e-06 -2.575910e-06  6.486034e-09\nHasCrCard1       -8.288460e-06  1.405830e-07 -1.356629e-05  6.276417e-10\nIsActiveMember1   2.637202e-05 -3.924088e-05  1.978922e-05 -5.129968e-10\nEstimatedSalary   1.976874e-10  1.824674e-11 -7.850909e-11 -3.430272e-15\n                 NumOfProducts    HasCrCard1 IsActiveMember1 EstimatedSalary\n(Intercept)      -3.775016e-03 -2.595873e-03    5.010389e-04   -2.301952e-08\nCreditScore      -8.271320e-08  2.066409e-07   -3.102862e-07    1.070775e-12\nGeographyGermany -3.655500e-04 -7.611739e-05   -1.008754e-04   -7.124867e-11\nGeographySpain   -3.357316e-05  3.603876e-05   -3.954646e-05   -8.778366e-11\nGenderMale        5.853316e-05 -8.288460e-06    2.637202e-05    1.976874e-10\nAge               2.887014e-06  1.405830e-07   -3.924088e-05    1.824674e-11\nTenure           -2.575910e-06 -1.356629e-05    1.978922e-05   -7.850909e-11\nBalance           6.486034e-09  6.276417e-10   -5.129968e-10   -3.430272e-15\nNumOfProducts     2.221635e-03 -3.379445e-06   -3.685755e-05   -4.030670e-10\nHasCrCard1       -3.379445e-06  3.521179e-03    6.930480e-05    3.401694e-11\nIsActiveMember1  -3.685755e-05  6.930480e-05    3.327629e-03    2.676211e-10\nEstimatedSalary  -4.030670e-10  3.401694e-11    2.676211e-10    2.243567e-13\n\n\nThere are several categorical variables here, so after expansion to dummies, \\(AVar(v,w)\\) is \\(12 \\textrm{ x } 12\\). This is the covariance matrix for the vector of estimated logistic regression coefficients \\(\\widehat{\\beta}\\). There are many different functions \\(f(\\beta)\\) that might be of interest."
  },
  {
    "objectID": "Ch2a.html#example-motherdaughter-height-data",
    "href": "Ch2a.html#example-motherdaughter-height-data",
    "title": "4  Covariance Matrices, MV Normal Distribution, Delta Method",
    "section": "4.6 Example: Mother/Daughter Height Data",
    "text": "4.6 Example: Mother/Daughter Height Data\n\nlibrary(WackyData)\ndata(Heights)\nhead(heights)\n\n  Mheight Dheight\n1    59.7    55.1\n2    58.2    56.5\n3    60.6    56.0\n4    60.7    56.8\n5    61.8    56.0\n6    55.5    57.9\n\nm &lt;- heights[,1]\nd &lt;- heights[,2]\nmeanm &lt;- mean(m)\nmeand &lt;- mean(d)\nfDel &lt;- matrix(c(1/meanm,-meand/meanm^2),ncol=1)\nn &lt;- length(m)\nsigma &lt;- (1/n) * cov(cbind(m,d))\nse &lt;- sqrt(t(fDel) %*% sigma %*% fDel)\nse\n\n            [,1]\n[1,] 0.001097201\n\nmeanmd &lt;-meanm / meand\nmeanmd\n\n[1] 0.9796356\n\nc(meanmd - 1.96*se, meanmd + 1.96*se)\n\n[1] 0.9774850 0.9817861"
  },
  {
    "objectID": "Ch2a.html#regarding-those-pesky-derivatives",
    "href": "Ch2a.html#regarding-those-pesky-derivatives",
    "title": "4  Covariance Matrices, MV Normal Distribution, Delta Method",
    "section": "4.7 Regarding Those Pesky Derivatives",
    "text": "4.7 Regarding Those Pesky Derivatives\nThough finding expressions for the derivatives in the above example was not onerous, the function \\(f\\) can be rather complex, with the expressions for its derivatives even more complicated. Typically such tedious and error-prone operations can be avoided, by having the software calculate approximate derivatives.\nRecall the definition of derivative:\n\\[\nf'(x) = \\lim_{w \\rightarrow 0}\n\\frac{f(x+w) - f(x)}{w}\n\\]\nSo an aproximate value of \\(f'(x)\\) is obtained by choosing some small value of \\(w\\) and evaluating\n\\[\n\\frac{f(x+w) - f(x)}{w}\n\\]\nThough of course there is an issue with one’s choice of \\(w\\), the point is that one can code the software to find approximate derivatives automatically using this device. This is very common in Data Science libraries.\nFor example, the R package numDeriv will compute numerical derivatives."
  },
  {
    "objectID": "Ch2a.html#your-turn",
    "href": "Ch2a.html#your-turn",
    "title": "4  Covariance Matrices, MV Normal Distribution, Delta Method",
    "section": "4.8 Your Turn",
    "text": "4.8 Your Turn\n❄️ Your Turn: Show that in the scalar context,\n\\[\nCov(X,Y) = E(XY) - EX ~ EY\n\\]\n❄️ Your Turn: Show that in the vector context,\n\\[\nCov(X) = E(X X') - (EX) (EX)'\n\\]\n❄️ Your Turn: Suppose\n\\[\nW = X \\beta + \\alpha S\n\\]\nfor a random vector \\(X\\), a scalar random variable \\(S\\), a nonrandom vector \\(\\beta\\) and a nonrandom scalar \\(\\alpha\\). Show that\n\\[\nCov(W,S) = \\beta' Cov(X,S) + \\alpha Var(S)\n\\]"
  },
  {
    "objectID": "Ch3.html#linear-regression-through-the-origin",
    "href": "Ch3.html#linear-regression-through-the-origin",
    "title": "5  Linear Statistical Models",
    "section": "5.1 Linear Regression through the Origin",
    "text": "5.1 Linear Regression through the Origin\nLet’s consider the Nile dataset built-in to R. It is a time series, one measurement per year.\n\nhead(Nile)\n\n[1] 1120 1160  963 1210 1160 1160\n\n# predict current year from previous year?\nn1 &lt;- Nile[-(length(Nile))]\nhead(n1)\n\n[1] 1120 1160  963 1210 1160 1160\n\nn2 &lt;- Nile[-1]\nhead(n2)\n\n[1] 1160  963 1210 1160 1160  813\n\nplot(n1,n2,cex=0.4,xlim=c(0,1400),yli=c(0,1400))\n\n\n\n\nWe would like to fit a straight line through that data point cloud. We might have two motivations for doing this:\n\nThe line might serve as nice summary of the data.\nMore formally, let \\(C\\) and \\(V\\) denote the current and previous year’s measurements..Then the model\n\n\\[E(C | V) = \\beta V\\]\nmay be useful. Here the slope \\(\\beta\\) is an unknown value to be estimated from the data. Readers with some background in linear regression models should note that this assumption of a linear trend through the origin is the only assumption we are making here. Nothing on normal distributions etc.\n\n\n\n\n\n\nModel Validity\n\n\n\nThe great statistician George Box once said, “All models are wrong but some are useful.” All data scientists should keep this at the forefronts of their minds.\n\n\n\n5.1.1 Least squares approach\nWe wish to estimate \\(\\beta\\) from our data, which we regard as a sample from the data generating process. Denote our data by \\((C_i,V_i), i = 1,...,100\\). Let \\(\\widehat{\\beta}\\) denote our estimate. How should we obtain it?This too is something all data scientists should keep at the forefronts of their minds. We are always working with sample data, subject to intersample variations. The quantity \\(\\widehat{\\beta}\\) is a random variable.The “hat” notation ‘^’ is statistical convention for “estimate of”; \\(\\widehat{\\beta}\\) is our estimate of the unknown parameter \\(\\beta\\).\nPretend for a moment that we don’t know, say, \\(C_{28}\\). Using our estimated \\(\\beta\\), our predicted value would be \\(\\widehat{\\beta} V_{28}\\). Our squared prediction error would then be \\((C_{28} - \\widehat{\\beta} W_{28})^2\\).\nWell, we actually do know \\(C_{28}\\) (and the others in our data), so we can answer the question:\n\nIn our search for a good value of \\(\\widehat{\\beta}\\), we can ask how well we would predict our known data, using that candidate value of \\(\\widehat{\\beta}\\) in our data. Our total squared prediction error would be\n\\[\n\\sum_{i=1}^{100} [C_{i} - \\widehat{\\beta} V_{i} )^2\n\\]\nA natural choice for \\(b\\) would be the value that minimizes this quantity. Why not look at the absolute value instead of the square? The latter makes the math flow well, as will be seen shortly.\n\n\n\n5.1.2 Calculation\nAs noted, our choice for \\(b\\) will be the minimizer of\n\\[\n\\sum_{i=1}^{100} (C_{i} - b V_{i})^2\n\\]\nover all possible values of \\(b\\). We then set \\(\\widehat{\\beta}\\) to that minimizing value of \\(b\\).\nThis is a straightforward calculus problem. Setting\n\\[\n0 = \\frac{d}{db} \\sum_{i=1}^{100} (C_{i} - b V_{i} )^2 =\n-2 \\sum_{i=1}^{100} (C_{i} - b V_{i}) V_i\n\\]\nand solving \\(b\\), we find that\n\\[\nb = \\frac{\\sum_{i=1}^n C_i V_i}{\\sum_{i=1}^nV_i^2}\n\\]\n\n\n5.1.3 R code\n\nlm(n2 ~ n1-1)\n\n\nCall:\nlm(formula = n2 ~ n1 - 1)\n\nCoefficients:\n  n1  \n0.98  \n\n\nThis says, “Fit the model \\(E(C | V) = \\beta V\\) to the data, with the line constrained to pass through the origin.” The constraint is specified by the -1 term.\nWe see that the estimate regression line is\n\\[E(C | V) = 0.98 V\\]"
  },
  {
    "objectID": "Ch3.html#linear-regression-model-with-intercept-term",
    "href": "Ch3.html#linear-regression-model-with-intercept-term",
    "title": "5  Linear Statistical Models",
    "section": "5.2 Linear Regression Model with Intercept Term",
    "text": "5.2 Linear Regression Model with Intercept Term\nSay we do not want to constrain the model to pass the line through the origin. Our model is then\n\\[E(C | V) = \\beta_0 + \\beta_1 V\\]\nwhere we now have two unknown parameters to be estimated.\n\n5.2.1 Least-squares estimation, single predictor\nOur sum of squared prediction errors is now\n\\[\n\\sum_{i=1}^{100} [O_{i} - (b_0 + b_1 V_{i}) ]^2\n\\]\nThis means setting two derivatives to 0 and solving. Since the derivatives involve two different quantities to be optimized, \\(b_0\\) and \\(b_1\\), the derivatives are termed partial, and the \\(\\partial\\) symbol is used instead of ‘d’.\n\\[\\begin{align}\n0 &=\n\\frac{\\partial}{\\partial b_0}\\sum_{i=1}^{100} [C_{i} - (b_0 + b_1 V_{i })]^2 \\\\\n&=\n-2 \\sum_{i=1}^{100} [C_{i} - (b_0 + b_1 V_{i}) ]\n\\end{align}\\]\nand\n\\[\\begin{align}\n0 &=\n\\frac{\\partial}{\\partial b_1}\\sum_{i=1}^{100} [C_{i} - (b_0 + b_1 V_{i})]^2 \\\\\n&=\n-2 \\sum_{i=1}^{100} [C_{i} - (b_0 + b_1 V_{i})] V_i\n\\end{align}\\]\nWe could then solve for the \\(b_i\\), but let’s go straight to the general case."
  },
  {
    "objectID": "Ch3.html#least-squares-estimation-general-number-of-predictors",
    "href": "Ch3.html#least-squares-estimation-general-number-of-predictors",
    "title": "5  Linear Statistical Models",
    "section": "5.3 Least-Squares Estimation, General Number of Predictors",
    "text": "5.3 Least-Squares Estimation, General Number of Predictors\n\n5.3.1 Nile example\nAs we have seen, systems of linear equations are natural applications of linear algebra. The equations setting the derivatives to 0 can be written in matrix terms as\n\\[\n\\left (\n\\begin{array}{r}\n\\sum_{i=1}^n C_i \\\\\n\\sum_{i=1}^n C_i V_i \\\\\n\\end{array}\n\\right ) =\n\\left (\n\\begin{array}{rr}\n100 & \\sum_{i=1}^n V_i \\\\\n\\sum_{i=1}^n V_i & b_1 \\sum_{i=1}^n V_i^2 \\\\\n\\end{array}\n\\right )\n\\left (\n\\begin{array}{r}\nb_0 \\\\\nb_1 \\\\\n\\end{array}\n\\right )\n\\]\nActually, that matrix equation can be derived more easily by using matrices to begin with:\nDefine \\(S\\) and \\(T\\):\n\\[\nS =\n\\left (\n\\begin{array}{r}\nC_1 \\\\\nC_2 \\\\\n... \\\\\nC_{100} \\\\\n\\end{array}\n\\right )\n\\]\nand\n\\[\nT =\n\\left (\n\\begin{array}{r}\nV_1 \\\\\nV_2 \\\\\n... \\\\\nV_{100} \\\\\n\\end{array}\n\\right )\n\\]\nThen our linear assumption, \\(E(C | V) = \\beta _0 + \\beta_1 V\\), applied to \\(S\\) and \\(T\\), is\n\\[\nE(S | T) =\nA \\beta\n\\]\nwhere\n\\[\nA =  \n\\left (\n\\begin{array}{rr}\n1 & V_1 \\\\\n1 & V_2 \\\\\n... & ... \\\\\n1 & V_{100} \\\\\n\\end{array}\n\\right )\n\\]\nand \\(\\beta = (\\beta_0,\\beta_1)'.\\)\nOur predicted error vector, using our candidate estimate \\(b\\) of \\(\\beta\\), is very simply expressed:\n\\[\nS - Ab\n\\]\nAnd since for any column vector \\(u\\), the sum of its squared elements is\n\\[\nu'u\n\\]\nour sum of squared prediction errors is\n\\[\n(S - Ab)'(S - Ab)\n\\tag{5.1}\\]\nNow how we will minimize that matrix expression with respect to the vector \\(b\\)? That is the subject of the next section.\n\n\n5.3.2 Matrix derivatives\nThe (column) vector of partial derivatives of a scalar quantity is called the gradient of that quantity. For instance, with\n\\[\nu = 2x + 3y^2 + xy\n\\]\nwe have that its gradient is\n\\[\n\\left (\n\\begin{array}{r}\n2 + y \\\\\n6y + x \\\\\n\\end{array}\n\\right )\n\\]\nWith care, we can compute gradients entirely at the matrix level, using easily derivable properties, without ever resorting to returning to the scalar expressions. Let’s apply them to the case at hand in the last section,\n\\[\n(S - Ab)'(S - Ab)\n\\tag{5.2}\\]\n\n\n5.3.3 Differentiation purely in matrix terms\nIt can be shown that for a column vector \\(a\\),\n\\[\n\\frac{d}{da} a'a = 2a\n\\tag{5.3}\\]\nEquation 5.2 is indeed of the form \\(a'a\\), but the problem here is that \\(a\\) in turn is a function of \\(b\\), This calls for the Chain Rule, which does exist at the matrix level:\nFor example if \\(u = Mv + w\\), with \\(M\\) and \\(w\\) constants (i.e. not functions of \\(v\\), then\n\\[\n\\frac{d}{dv} u'u = 2M'u\n\\]We must keep in mind that we are working with vectors and matrices, so that \\(M'u\\), say \\(r \\textrm{x} s\\) times \\(s \\textrm{ x} 1\\), is conformable matrix multiplication while \\(uM'\\) is mathematical nonsense.\nIn our case at hand, we have \\(M = -A\\) and \\(w = S\\), so that\n\\[\n\\frac{d}{db} [(S - Ab)'(S - Ab) = -2 A'(S - Ab)\n\\]\nSo, set\n\\[\n0 = A'(S - Ab) = A'S - A'A b\n\\]\nyield our minimizing \\(b\\):\n\\[\n\\widehat{\\beta} = (A'A)^{-1} A'S\n\\tag{5.4}\\]\nproviding the inverse exists (more on this in the next chapter).\nLet’s check this with the Nile example:\n\nA &lt;- cbind(1,n1)\nS &lt;- n2\nAp &lt;- t(A)  # R matrix transpose\nsolve(Ap %*% A) %*% Ap %*% S  # R matrix inverse\n\n          [,1]\n   452.7667508\nn1   0.5043159\n\n# check via R\nlm(n2 ~ n1)\n\n\nCall:\nlm(formula = n2 ~ n1)\n\nCoefficients:\n(Intercept)           n1  \n   452.7668       0.5043  \n\n\n\ndet(Ap %*% A) \n\n[1] 277463876\n\n\nNonzero! So \\((A'A)^{-1}\\) does exist, as we saw.\n\n\n5.3.4 The general case\nSay our data consists of \\(n\\) points, each of which is of length \\(p\\). Write the \\(j^{th}\\) element of the \\(i^{th}\\) data point as \\(X_{ij}\\). Then set\n\\[\nA =\n\\left (\n\\begin{array}{rrrr}\n1 & X_{11} & ... & X_{p1} \\\\\n1 & X_{21} & ... & X_{p2} \\\\\n... & ... & ... & ... \\\\\n1 & X_{n1} & ... & X_{np} \\\\\n\\end{array}\n\\right )\n\\tag{5.5}\\]\nContinue to set \\(S\\) to the length-\\(n\\) column vector of our response variable. Our model is\n\\[\nE(S | A) = A \\beta\n\\]\nfor an unknown vector \\(\\beta\\) of length \\(p+1\\). Our estimated of that vector based on our data will be denoted\n\\[\nb = (b_0,b_1,...,b_p)'\n\\]\nThen, using the same reasoning as before, we have the minimizing value of \\(b\\):\n\\[\n\\widehat{\\beta} = (A'A)^{-1} A'S\n\\tag{5.6}\\]\nagain providing that the inverse exists.\n\n\n5.3.5 Example: mlb1 data\nAs an example, let’s take the mlb1 from my qeML (’Quick and Easy Machine Learning package.  The data is on major league baseball players. We will predict weight from height and age.Dataset kindly provided by the UCLA Dept. of Statistics\n\nlibrary(qeML)\ndata(mlb1)\nhead(mlb1)\n\n        Position Height Weight   Age\n1        Catcher     74    180 22.99\n2        Catcher     74    215 34.69\n3        Catcher     72    210 30.78\n4  First_Baseman     72    210 35.43\n5  First_Baseman     73    188 35.71\n6 Second_Baseman     69    176 29.39\n\nourData &lt;- as.matrix(mlb1[,-1]) # must have matrix to enable %*%\nhead(ourData)\n\n  Height Weight   Age\n1     74    180 22.99\n2     74    215 34.69\n3     72    210 30.78\n4     72    210 35.43\n5     73    188 35.71\n6     69    176 29.39\n\nA &lt;- cbind(1,ourData[,c(1,3)])\nAp &lt;- t(A)\nS &lt;- as.vector(mlb1[,3])\nsolve(Ap %*% A) %*% Ap %*% S\n\n               [,1]\n       -187.6381754\nHeight    4.9235994\nAge       0.9115326\n\n# check via R\nlm(Weight ~ .,data=mlb1[,-1])\n\n\nCall:\nlm(formula = Weight ~ ., data = mlb1[, -1])\n\nCoefficients:\n(Intercept)       Height          Age  \n  -187.6382       4.9236       0.9115  \n\n\nSo, if we have a player of known height and age, we would predict the weight to be\n-187.6382 + 4.9236 x height + 0.9115 x age\n\n\n5.3.6 Homogeneous variance case\nIn addition to\n\\[\nE(S | A) = A \\beta\n\\]\nit is often assumed thatIn some applications, \\(A\\) is actually chosen by an experimenter, so that it is not random. But even in the random case, it is standard to condition on \\(A\\). By Equation 8.2, a conditional confidence interval, say, at the 95% level also has the level unconditionally.\n\\[\nCov(S | A) = \\sigma^2 I\n\\]\nwhere \\(\\sigma\\) is an unknown constant to be estimated from the data.\nSo \\(Var(S_i) = \\sigma^2\\) for all \\(i\\). It is usually not a realistic assumption. Say for instance we are predicting human weight from height. There should be more variation in weight among taller people than above shorter people. But it’s a simplifying assumption without really good alternatives, so it is commonly used.\nAnd in that setting, our formulas from {?sec-covar} come in handy, as follows.\nRecall that \\(\\widehat{\\beta}\\) is our estimate of the unknown population parameter \\(\\beta\\), based on our random sample data. But that means that \\(\\widehat{\\beta}\\) is a random vector, and thus has a covariance matrix. Using Equation 4.4 and setting \\(R = (A'A)^{-1} A'\\)$, we have\n\\[\nCov(\\widehat{\\beta}|A) = R Cov(S|A) R' = R \\sigma^2 I R' = \\sigma^2 R R' =\n\\sigma^2 (A'A)^{-1}\n\\tag{5.7}\\]\nClassical statistical formulas use this relation to find standard errors for \\(\\widehat{\\beta}_i\\) etc."
  },
  {
    "objectID": "Ch3.html#update-formulas",
    "href": "Ch3.html#update-formulas",
    "title": "5  Linear Statistical Models",
    "section": "5.4 Update Formulas",
    "text": "5.4 Update Formulas\nOne important theme in developing prediction models (linear regression, neural networks etc.) is the avoidance of overfitting, meaning that we fit an overly elaborate model to our data. We simply are estimating too many things for the amount of data we have, “spreading our data too thin.”\nA common example is using too many predictor variables, so that we are estimating a large number of coefficients \\(\\beta_i\\).\nOr we may draw a histogram with too many bins:\n\nlibrary(qeML)\ndata(forest500)  # data on forest ground cover\nhist(forest500$V1,breaks=10)\n\n\n\nhist(forest500$V1,breaks=100)\n\n\n\n\nIn a histogram, we are estimating the heights of the bins. With 10 bins we obtained a smooth graph, but with 100 bins it became choppy. So again, we are estimating too many things, given the capacity of the data.A histogram is an estimate of the probability density function of the observed random variable. Having more, thus narrower, bins reduces bias but increases variance.\nWith larger datasets, we can use more predictor variables, more histogram bins, and so on. The question then arises is, for instance, How many predictors, how many bins and so on, can we afford to use with our given data?\nThe typical solution is to fit several models of different complexity, then choose the one that predicts the best. But we must do this evaluation on “fresh” data; we should not predict on the same data on which we fitted our model.\nWe thus rely on partitioning our data, into a training set to which we fit our model, and a test set, on which we predict using the fitted model. We may wish to do this several times.\nA special case is the Leaving One Out method, in which the holdout set size is 1. It might go like this, for a dataset d:\nsumErrs = 0\nfor i = 1,...,n  # dataset has n datapoints\n   fit lm to d[-i,]\n   use result to predict d[i,]\n   add prediction error to sumErrs\nreturn sumErrs\nThis can become computationally challenging, as we would need to refit the model each time. Each call to lm involves a matrix inversion (equivalent), and we must do this \\(n\\) times.\nIt would be nice if we could have an “update” formula that would quickly recalculate the model found on the full dataset. Then e would need to perform matrix inversion just once. In the case of linear models, such a formula exists, in the Sherman-Morrison-Woodbury relation:\n\nGiven an invertible matrix \\(B\\) and row vectors \\(u\\) and \\(v\\) having lengths equal to the number of columns of \\(B\\). form the matrix\n\\[\nC = B + uv'\n\\]\nThen \\(C^{-1}\\) exists and is equal toNote that the quantity \\(uv'\\) is a square matrix the size of \\(B\\), so the sum and product make sense.\n\\[\nB^{-1} - \\frac{1}{1+v'B^{-1}u} B^{-1} (uv') B^{-1}\n\\]\n\nNow, how can we apply this to the Leave One Out method? In the matrix \\(A\\) in Equation Equation 5.5, we wish to remove row \\(i\\); call the result \\(A_{-i}\\). Our new version of \\(A'A\\) is then\n\\[\nA_{-i}' A_{-i}\n\\]\nSo our main task is to obtain\n\\[\n(A_{-i}' A_{-i})^{-1}\n\\]\nby updating \\((A'A)^{-1}\\), which we already have from our computation in the full dataset.\nWe can do this as follows. Denote row \\(i\\) of \\(A\\) by \\(a_i\\), and set\n\\[\nu = -a_i, v = a_i\n\\]\nTo show why these choices for \\(u\\) and \\(v\\) work, consider the case in which we delete the last row of \\(A\\). (The analysis would be similar for other cases.) Write the latter as a partitioned matrix,\n\\[\n\\left (\n\\begin{array}{r}\nA_{(-n)} \\\\\na_n \\\\\n\\end{array}\n\\right )\n\\]\nWe pretend it is a \\(2 \\textrm{x} 1\\) “matrix,” and \\(A'\\) is then “\\(1 \\textrm{x} 2\\)”:\n\\[\nA' =\n\\left (\n\\begin{array}{rr}\nA_{(-n)}' & a_n' \\\\\n\\end{array}\n\\right )\n\\]\nThus\n\\[\nA'A =\nA_{-i}' A_{-i} + a_n a_n'\n\\]\nyielding\n\\[\nA_{-i}' A_{-i} = A'A - a_n a_n'\n\\]\njust what we need for Sherman-Morrison-Woodbury: With \\(B = A'A\\), we have\n\\[\n[A_{-i}' A_{-i}]^{-1} =\nB^{-1} + \\frac{1}{1-a_i'B^{-1}a_i}\nB^{-1} (a_i a_i') B^{-1}\n\\]\nLet’s check it:\n\na &lt;- rbind(c(1,3,2),c(1,0,5),c(1,1,1),c(1,9,-3)) \napa &lt;- t(a) %*% a\napai &lt;- solve(apa)\na2 &lt;- a[-2,]\napa2 &lt;- t(a2) %*% a2\napa2i &lt;- solve(apa2)\n# prepare for S-M-W\nadel &lt;- matrix(a[2,],ncol=1)\nw1 &lt;- 1/(1 - t(adel) %*% apai %*% adel)\nw1 &lt;- as.numeric(w1)\nuvt &lt;- adel %*% t(adel)\nw2 &lt;- apai %*% uvt %*% apai\n# S-M-W says this will be apa2i\napai + w1 * w2\n\n           [,1]       [,2]      [,3]\n[1,]  3.4140625 -0.7109375 -1.015625\n[2,] -0.7109375  0.1640625  0.234375\n[3,] -1.0156250  0.2343750  0.406250\n\napa2i\n\n           [,1]       [,2]      [,3]\n[1,]  3.4140625 -0.7109375 -1.015625\n[2,] -0.7109375  0.1640625  0.234375\n[3,] -1.0156250  0.2343750  0.406250"
  },
  {
    "objectID": "Ch3.html#your-turn",
    "href": "Ch3.html#your-turn",
    "title": "5  Linear Statistical Models",
    "section": "5.5 Your Turn",
    "text": "5.5 Your Turn\n❄️ Your Turn: Show Equation 5.3. Write \\(a = (a_1,...,a_k)'\\) and find the gradient “by hand.” Compare to \\(2a\\).\n❄️ Your Turn: Show that\n\\[\n\\frac{d}{du} u'Qu = 2Qu\n\\]\nfor a constant symmetric matrix \\(Q\\) and a vector \\(u\\). (\\(u'Qu\\) is called a quadratic form.)"
  },
  {
    "objectID": "Ch4.html#sec-censusrref",
    "href": "Ch4.html#sec-censusrref",
    "title": "6  Matrix Rank",
    "section": "6.1 Example: Census Data",
    "text": "6.1 Example: Census Data\nThis dataset is also from qeML. It is data for Silicon Valley engineers in the 2000 Census. Let’s focus on just a few columns.\n\ndata(svcensus) \nhead(svcensus) \n\n       age     educ occ wageinc wkswrkd gender\n1 50.30082 zzzOther 102   75000      52 female\n2 41.10139 zzzOther 101   12300      20   male\n3 24.67374 zzzOther 102   15400      52 female\n4 50.19951 zzzOther 100       0      52   male\n5 51.18112 zzzOther 100     160       1 female\n6 57.70413 zzzOther 100       0       0   male\n\nsvc &lt;- svcensus[,c(1,4:6)] \nhead(svc) \n\n       age wageinc wkswrkd gender\n1 50.30082   75000      52 female\n2 41.10139   12300      20   male\n3 24.67374   15400      52 female\n4 50.19951       0      52   male\n5 51.18112     160       1 female\n6 57.70413       0       0   male\n\nlm(wageinc ~ .,data=svc) \n\n\nCall:\nlm(formula = wageinc ~ ., data = svc)\n\nCoefficients:\n(Intercept)          age      wkswrkd   gendermale  \n   -29384.1        496.7       1372.8      10700.8  \n\n\nSo, we estimate that, other factors being equal, men about paid close to $11,000 more than women. This is a complex issue, but for our purposes here, how did gender become gendermale, no explicit mention of women?\nLet’s try to force the issue:\n\nsvc$man &lt;- as.numeric(svc$gender == 'male')\nsvc$woman &lt;- as.numeric(svc$gender == 'female')\nsvc$gender &lt;- NULL\nhead(svc)\n\n       age wageinc wkswrkd man woman\n1 50.30082   75000      52   0     1\n2 41.10139   12300      20   1     0\n3 24.67374   15400      52   0     1\n4 50.19951       0      52   1     0\n5 51.18112     160       1   0     1\n6 57.70413       0       0   1     0\n\nlm(wageinc ~ .,data=svc)\n\n\nCall:\nlm(formula = wageinc ~ ., data = svc)\n\nCoefficients:\n(Intercept)          age      wkswrkd          man        woman  \n   -29384.1        496.7       1372.8      10700.8           NA  \n\n\nWell, we couldn’t force the issue after all. Why not? We hinted above that \\(A' A\\) may not be invertible. Let’s take a look.\n\nA &lt;- cbind(1,svc[,-2])\nA &lt;- as.matrix(A)\nApA &lt;- t(A) %*% A\nApA\n\n               1        age  wkswrkd      man    woman\n1        20090.0   794580.7   907240  15182.0   4908.0\nage     794580.7 33956543.6 35869770 600860.8 193719.9\nwkswrkd 907240.0 35869770.5 45252608 692076.0 215164.0\nman      15182.0   600860.8   692076  15182.0      0.0\nwoman     4908.0   193719.9   215164      0.0   4908.0\n\n\nIs this matrix invertible? Let’s apply the elementary row operations introduced in Section 3.3.1:\n\nlibrary(pracma)\nrref(ApA) \n\n        1 age wkswrkd man woman\n1       1   0       0   0     1\nage     0   1       0   0     0\nwkswrkd 0   0       1   0     0\nman     0   0       0   1    -1\nwoman   0   0       0   0     0\n\n\nAha! Look at that row of 0s! The row operations process ended prematurely. This matrix will be seen to have no inverse. We say that the matrix has rank 4 – meaning 4 nonzero rows – when it needs to be 5.\nThough we have motivated the concept of matrix rank here with a linear model example, and will do so below, the notion pervades all of linear algebra, as will be seen in the succeeding chapters.\nWe still have not formally defined rank, just building intuition, but toward that end, let us first formalize the row operations process."
  },
  {
    "objectID": "Ch4.html#reduced-row-echelon-form-rref-of-a-matrix",
    "href": "Ch4.html#reduced-row-echelon-form-rref-of-a-matrix",
    "title": "6  Matrix Rank",
    "section": "6.2 Reduced Row Echelon Form (RREF) of a Matrix",
    "text": "6.2 Reduced Row Echelon Form (RREF) of a Matrix\nWe formalize and extend Section 3.3.1.\n\n6.2.1 Elementary row operations\nThese are:\n\nMultiply a row by a nonzero constant.\nAdd a multiple of one row to another.\nSwap two rows.\n\nAgain, each operation can be implemented via pre-multiplying the given matrix by a corresponding elementary matrix. The latter is the result of applying the given operation to the identity matrix \\(I\\). For example, here is the matrix corresponding to swapping rows 2 and 3:\n\\[\n\\left ( \\begin{array}{rrr}\n1 & 0 & 0 \\\\\n0 & 0 & 1 \\\\\n0 & 1 & 0 \\\\\n\\end{array} \\right )\n\\]\nFor example:\n\ne &lt;- rbind(c(1,0,0),c(0,0,1),c(0,1,0))\ne\n\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    0    0    1\n[3,]    0    1    0\n\na &lt;- matrix(runif(12),nrow=3)\na\n\n          [,1]      [,2]      [,3]      [,4]\n[1,] 0.2981126 0.8630180 0.8557441 0.3872760\n[2,] 0.4526713 0.4371274 0.5401154 0.5009746\n[3,] 0.0424277 0.3434724 0.5158489 0.6168227\n\ne %*% a  # matrix mult. is NOT e * a!\n\n          [,1]      [,2]      [,3]      [,4]\n[1,] 0.2981126 0.8630180 0.8557441 0.3872760\n[2,] 0.0424277 0.3434724 0.5158489 0.6168227\n[3,] 0.4526713 0.4371274 0.5401154 0.5009746\n\n\nYes, rows 2 and 3 were swapped.\n\n\n6.2.2 The RREF\nBy applying these operations to a matrix \\(A\\), we can compute its reduced row echelon form \\(A_{rref}\\), which is defined by the following properties:\n\nEach row, if any, that consists of all 0s is at the bottom of the matrix.\nThe first nonzero entry in a row, called the pivot, is a 1.\nEach pivot will be to the right of the pivots in the rows above it.\nEach pivot is the only nonzero entry in its column.\n\nThe reader should verify that the matrix at the end of Section 6.1 has these properties.\nIt is much easier to gain insight from RREF by viewing it in partitioned-matrix form:\n\nPartioned-Matrix View\nSay \\(A\\) is of size \\(m \\textrm{ x } n\\). Then \\(A_{rref}\\) has the form\n\\[\n\\left (\n\\begin{array}{rr}\nI_{s} & U \\\\\n0 & 0  \\\\\n\\end{array}\n\\right )\n\\tag{6.1}\\] where \\(I_s\\) is an identity matrix of some size \\(s\\) and \\(U\\) has dimension \\(s \\textrm{ x } (n-s)\\).\n\nClearly, the rank of \\(A_{rref}\\) is \\(s\\). It will turn out the \\(s\\) is the rank of \\(A\\) as well.\n\n\n6.2.3 Recap of Section 3.3.1\n\nEach row operation can be performed via premultiplication by an elementary matrix. Each such matrix is invertible, and its inverse is an elementary matrix.\nThus\n\n\\[\nB_{rref} = E_k ... E_2 E_1 B\n\\tag{6.2}\\]\nfor a sequence of invertible elementary matrices.\n\nAnd\n\\(B = (E_1)^{-1} (E_2)^{-1} ... (E_k)^{-1} B_{rref}\\),\nwhere each \\((E_i)^{-1}\\) is itself an elementary row operation."
  },
  {
    "objectID": "Ch4.html#formal-definitions",
    "href": "Ch4.html#formal-definitions",
    "title": "6  Matrix Rank",
    "section": "6.3 Formal Definitions",
    "text": "6.3 Formal Definitions\n\nDefinition: A linear combination of vectors \\(v_1,v_2,...v_k\\) is a sum of scalar multiples of the vectors, i.e.\n\\[\na_1 v_1 + ... + a_k v_k\n\\]\nwhere the \\(a_i\\) are scalars.\n\n(The reader may wish to review Section 2.3.)\n\nDefinition: We say a set of vectors is linearly dependent if some linear combination of them (excluding the trivial case in which all the coefficients are 0) equals 0, as in both cases above. If no nontrival linear combination of the vectors is 0, we say the vectors are linearly independent.\n\nFor instance, consider the matrix\n\\[\n\\left ( \\begin{array}{rrr}\n1 & 3 & 1 \\\\\n1 & 9 & 4 \\\\\n0 & 8 & 0 \\\\\n\\end{array} \\right )\n\\]\nDenote its columns by \\(c_1\\), \\(c_2\\) and \\(c_3\\). Observe that\n\\[\n(-1) c_1 + (1) c_2 + (-2) c_3 =\n\\left (\n\\begin{array}{r}\n0 \\\\\n0 \\\\\n0 \\\\\n\\end{array}\n\\right )\n\\]\nThen \\(c_1\\), \\(c_2\\) and \\(c_3\\) are not linearly independent.\nSo, here is the formal definition of rank:\n\nDefinition The rank of a matrix \\(B\\) is its maximal number of linearly independent rows."
  },
  {
    "objectID": "Ch4.html#rank-in-relation-to-sets-of-linear-dependentindependent-vectors",
    "href": "Ch4.html#rank-in-relation-to-sets-of-linear-dependentindependent-vectors",
    "title": "6  Matrix Rank",
    "section": "6.4 Rank in Relation to Sets of Linear Dependent/Independent Vectors",
    "text": "6.4 Rank in Relation to Sets of Linear Dependent/Independent Vectors\nHere we use rank as our path to key concepts about sets of vectors.\n\n6.4.1 Motivation\nRecall that in the nonfull rank example we presented in Section 3.4, one row was double another,\nrow2 + 2 row1 = 0\nIn the census example,\n\nhead(A)\n\n  1      age wkswrkd man woman\n1 1 50.30082      52   0     1\n2 1 41.10139      20   1     0\n3 1 24.67374      52   0     1\n4 1 50.19951      52   1     0\n5 1 51.18112       1   0     1\n6 1 57.70413       0   1     0\n\n\nthe sum of the last two columns of \\(A\\) was equal to the first column:\ncolumn1 - column4 - column5 = 0\nWrite this more fully as an explicit linear combination of the columns of this matrix:\n1 column1 + 0 column2 + 0 column3 + (-1) column4 + (-1) column5 = 0\nThe vector of coefficients is (1,0,0,-1,-1).\n\n\n6.4.2 Row and column rank\nWe have defined the rank of a matrix to be the number of maximally linearly independent rows. We’ll now call that the row rank, and define the column rank to be the number of maximally linearly independent columns.\nAs will be seen below, the row and column ranks will turn out to be equal. Thus in the sequel, we will use the term rank to refer to their common value. For now, though, the unmodified term “rank” will mean row rank."
  },
  {
    "objectID": "Ch4.html#some-properties-of-ranks",
    "href": "Ch4.html#some-properties-of-ranks",
    "title": "6  Matrix Rank",
    "section": "6.5 Some Properties of Ranks",
    "text": "6.5 Some Properties of Ranks\nIn Section 6.1, we found the matrix \\(A'A\\) to be noninvertible, as its RREF had a row of 0s. We mentioned a relation to rank, but didn’t formalize it, which we will do now.\n\n6.5.1 Rank is preserved in pre-/post-multiplication by invertible matrices\n\nTheorem 6.1 Let \\(A\\) be any matrix, and let \\(V\\) and \\(W\\) be square, invertible matrices with sizes conformable with the products below. Then the column rank of \\(VA\\) is equal to that of \\(A\\), and the row rank of \\(AW\\) is equal to that of \\(A\\).\n\n\nProof. Say \\(A\\) has column rank \\(r\\). Then there exist \\(r\\) (and no more than \\(r\\)) linearly independent columns of \\(A\\). For convenience of notation, say these are the first \\(r\\) columns, and call them \\(c_1,...,c_r\\). Then the first \\(r\\) columns of \\(VA\\) are \\(Vc_1,...,Vc_r\\), by matrix partitioning.\nNote that for a vector \\(x\\), \\(Vx = 0\\) if and only if \\(x = 0\\). (Just multiply \\(Vx = 0\\) on the left by \\(V^{-1}\\).) So a linear combination \\(\\lambda_1 c_1+...+\\lambda_r c_r\\) is 0 if and only if the corresponding linear combination \\(\\lambda_1 Vc_1+...+\\lambda_r Vc_r\\) is 0. Thus the vectors \\(Vc_i\\) are linearly independent, \\(VA\\) has column rank \\(r\\).\nThe argument for the case of \\(AW\\) is identical, this time involving rows of \\(A\\).\n\\(\\square\\)\n\n\nTheorem 6.2 The column rank of a matrix \\(B\\) is equal to the column rank of its RREF, \\(B_{rref}\\), which in turn is \\(s\\) in Equation 6.1.\n\n\nProof. The above Theorem 6.1 says that pre-postmultiplying \\(B\\) will not change its column rank. Then invoke Equation 6.2.\nIt is clear that any column in \\(U\\) in Equation 6.1 can be written as a linear combination of the columns of \\(I_s\\). Thus the column rank of \\(B\\) is \\(s\\).\n\\(\\square\\)\n\n\nLemma 6.1 (The application of an elementary row operation on a matrix will will leave the row rank unchanged.)  \n\n\nProof. Let \\(r_i, i=1,...,m\\) denote the rows of the matrix. Consider a linear combination\n\\[\na_1 r_1 + ... + a_m r_m \\neq 0\n\\]\nFor any of the three elementary operations, a slightly modified linear combination works with the modified matrix:\n\nSwap rows \\(i\\) and \\(j\\): Swap \\(a_i\\) and \\(a_j\\).\nMultiplying row \\(i\\) by a constant \\(c\\). Since $r_i \\(c r_i\\), set \\(a_i \\rightarrow (1/c) a_i\\).\nAdd c times row \\(j\\) to row \\(i\\): Set \\(a_j \\rightarrow a_j - c a_j\\).\n\n\\(\\square\\)\n\n\nTheorem 6.3 (The row rank and column rank of a matrix are equal.)  \n\n\nProof. The lemma shows that every nonzero linear combination of the rows of \\(A\\) corresponds to one for the rows of \\(A_{rref}\\), and vice versa. Thus the row rank of \\(A\\) is the same as that of \\(A_{rref}\\). But that is \\(s\\) in Equation 6.1, which we found earlier was equal to the column rank of \\(A\\).\n\\(\\square\\)\n\n\n\n6.5.2 Rank is bounded above by the “narrow” dimension of the matrix\nIn Section 6.1, we found the matrix \\(A'A\\) to not be of full rank. It is surprising that so was \\(A\\):\n\nqr(ApA)$rank  # qr is a built-in R function\n\n[1] 4\n\nqr(A)$rank\n\n[1] 4\n\n\nThis is rather startling. \\(A\\) has over 20,000 rows — yet only 4 linearly independent ones? But it follows from this fact:There are many subsets of 4 rows that are linearly independent. But no sets of 5 or more are linearly independent.\n\nTheorem 6.4 The (common value of) row and column ranks of an \\(m \\textrm{ x } n\\) matrix \\(B\\) must be less than or equal to the minimum of the number of rows and columns of \\(B\\) .\n\n\nProof. The row rank of \\(b\\) equals its column rank. The former is bounded above by \\(m\\) while the latter’s bound is \\(n\\).\n\\(\\square\\)"
  },
  {
    "objectID": "Ch4.html#your-turn",
    "href": "Ch4.html#your-turn",
    "title": "6  Matrix Rank",
    "section": "6.6 Your Turn",
    "text": "6.6 Your Turn\n❄️ Your Turn: Show that a set of vectors is linearly dependent if and only if one of the vectors is a linear combination of the others.\n❄️ Your Turn: Consider an \\(m \\textrm{x} n\\) matrix \\(A\\) with \\(m \\geq n\\). Then consider the partitioned matrix\n\\[\nB =\n\\left (\n\\begin{array}{r}\nA \\\\\nI \\\\\n\\end{array}\n\\right )\n\\]\nwhere $I is the \\(n \\textrm{x} n\\) identity matrix. Prove that \\(B\\) is of full rank.\n❄️ Your Turn: Consider the three basic elementary matrices discussed here: Swap rows \\(i\\) and \\(j\\); multiply row \\(i\\) by a constant \\(b\\); adding \\(c\\) times row \\(i\\) to row \\(j\\), for a constant \\(c\\). Find general formulas for the determinants of the three matrices.\n❄️ Your Turn: Prove that if the matrix \\(A\\) has a 0 row, then \\(det(A) = 0\\).\n❄️ Your Turn: Show that a set of vectors is linearly dependent if and only if one of the vectors is a linear combination of the others."
  },
  {
    "objectID": "Ch5a.html#review-of-matrix-rank-properties",
    "href": "Ch5a.html#review-of-matrix-rank-properties",
    "title": "7  Vector Spaces",
    "section": "7.1 Review of Matrix Rank Properties",
    "text": "7.1 Review of Matrix Rank Properties\nIn the last chapter, we presented the concepts of matrix row and column rank, defined to be the maximal number of linearly independent combinations of the rows or columns of the matrix, respectively. We proved that\n\nFor any matrix \\(B\\),\n\\[\n\\textrm{rowrank}(B)\n= \\textrm{colrank}(B)\n= \\textrm{rowrank}(B_{rref})\n= \\textrm{colrank}(B_{rref})\n\\]\n\nWe can say something stronger:\n\nTheorem: Let \\(\\cal V\\) denote the span of the rows of \\(B\\),i.e. the set of all possible linear combinations of rows of \\(B\\). Define \\(\\cal V_{rref}\\) similarly. Then\n\\[\n\\cal V = \\cal V_{rref}\n\\]\nThe analogous result holds for columns.\nProof: Actually, we already proved this in the proof regarding rank in ?sec-yessamerank, in which we wrote, ``any nonzero linear combination of rows in \\(B_{rref}\\) will correspond to a nonzero linear combination of the rows of \\(B\\),’’ and vice versa. This showed a one-to-one correspondence between the two sets of linear combinations.\n\nSo, not only do the two matrices have the same maximal numbers of linearly independent rows, they also generate the same linear combinations of those rows.\nThe sets \\(\\cal V\\) and \\(\\cal V_{rref}\\) are called the row spaces of the two matrices, and yes, they are examples of vector spaces, as we will now see."
  },
  {
    "objectID": "Ch5a.html#vector-space-definition",
    "href": "Ch5a.html#vector-space-definition",
    "title": "7  Vector Spaces",
    "section": "7.2 Vector Space Definition",
    "text": "7.2 Vector Space Definition\n\nA set of objects \\(\\cal W\\) is called a vector space if it satisfies the following conditions:\n\nSome form of addition between vectors \\(u\\) and \\(v\\), denoted \\(u+v\\), is defined in \\(\\cal W\\), with the result that u+v is also in \\(\\cal W\\). We describe that latter property by saying \\(\\cal W\\) is closed under addition.\nThere is a unique element called ``0’’ such that \\(u + 0 = 0 + u = u\\).\nSome form of scalar multiplication is defined, so that for any number \\(c\\) and and \\(u\\) in \\(\\cal W\\), \\(cw\\) exists and is in \\(\\cal W\\). We describe that latter property by saying \\(\\cal W\\) is closed under scalar multiplication\nThis being a practical book with just a dash of theory, we’ll skip the remaining conditions, involving algebraic properties such as commutativity of addition (\\(u+v = v+u\\)).\n\n\n\n7.2.1 Examples\n\n\n7.2.2 \\(\\cal R^{n}\\)\nIn the vast majority of examples in this book, our vector space will be \\(\\cal R^{n}\\).\nHere \\(\\cal R\\) represents the set of all real numbers, and \\(\\cal R^{n}\\) is simply the set of all vectors consisiting of \\(n\\) real numbers. In an \\(m \\textrm{x} k\\) matrix the rows are members of so \\(\\cal R^{m}\\) and the columns are in \\(\\cal R^{k}\\).\n\n\n7.2.3 The set C(0,1) of all continuous functions on the interval [0,1]\nNo surprises here. Vector addition and scalar multiplication are done as functions. If say \\(u\\) is the squaring function and \\(v\\)$ is the sine function, then\n\\[\n3u = 3x^{0.5}\n\\]\nand\n\\[\nu+v = x^{0.5} + \\sin(x)\n\\]\n\n\n7.2.4 The set \\(\\cal RV(\\Omega)\\) of all random variables defined on some probability space \\(\\Omega\\)\nConsider the example in {Section 5.3.5} on major league baseball players. We choose a player at random. Denote weight, height and age by \\(W\\), \\(H\\) and \\(A\\).\nVector addition and scalar multiplication are defined in a straightforward manner. For instance, the sum of \\(H\\) and \\(A\\) is simply height + age. This may seem like a rather nonsensical sum, but it fits the technical definition, and moreover, we have already been doing things like this! This after all is what is happening in our prediction expression from that section,\n\\[\n\\textrm{predicted weight} =\n-187.6382 + 4.9236 H+ 0.9115 A\n\\]\nIn fact, in this vector space, the above is a linear combination of the random variables \\(1\\), \\(H\\) and \\(A\\). Note that random variables such as \\(HW^{1.2}\\) and so on are also members of this vector space, essentially any function of \\(W\\), \\(H\\) and \\(A\\)."
  },
  {
    "objectID": "Ch5a.html#subspaces",
    "href": "Ch5a.html#subspaces",
    "title": "7  Vector Spaces",
    "section": "7.3 Subspaces",
    "text": "7.3 Subspaces\nSay \\(\\cal W_{1}\\) a subset of a vector space \\(\\cal W\\), such that \\(\\cal W_{1}\\) is closed under addition and scalar multiplication. \\(\\cal W_{1}\\) is called a subspace of \\(\\cal W\\). Note that a subspace is also a vector space in its own right.\n\n7.3.1 Examples\n\\(R^3\\) and \\(R^n\\):\nFor instance, take \\(\\cal W_{1}\\) to be all vectors of the form (a,b,0). Clearly, \\(\\cal W_{1}\\) is closed under addition and scalar multiplication.\nAnother subspace of \\(R^3\\) is the set of vectors (a,a,b), i.e. those vectors whose first two element are equal. What about vectors of the form (a,b,a+b)? Yes.\nWe saw earlier that the row space of an \\(m \\textrm{x} n\\) matrix \\(A\\), consisting of all linear combinations of the rows of \\(A\\), is a subspace of \\(R^m\\). Similarly, the column space is a subspace of \\(R^n\\).\n\\(C(0,1)\\):\nOne subspace is the set of all polynomial functions. Again, the sum of two polynomials is a polynomial, and the same holds for scalar multiplication, so the set of polynomials is closed under those operations, and is a subspace.\n\\(\\cal RV(\\Omega)\\):\nThe set of all random variables that are functions of \\(H\\), say, is a subspace. Another subspace is the set of all random variable with mean 0.\n\n\n7.3.2 Span of a set of vectors\nThe span of a set of vectors \\(G = {v_1,...,v_k}\\) is the set of all linear combinations of those vectors. It’s a subspace of the main space."
  },
  {
    "objectID": "Ch5a.html#basis",
    "href": "Ch5a.html#basis",
    "title": "7  Vector Spaces",
    "section": "7.4 Basis",
    "text": "7.4 Basis\nConsider a set of vectors \\(u_1,...u_r\\) in a vector space \\(\\cal W\\). Recall that the span of these vectors is defined to be the set of all linear combinations of them. In verb form, we say that \\(u_1,...u_r\\) spans \\(\\cal W\\) if we can generate the entire vector space from those vectors via linear combinations. It’s even nicer if the vectors are linearly independent:\n\nWe say the vectors \\(u_1,...u_r\\) in a vector space \\(\\cal W\\) form a basis for \\(\\cal W\\) if they are linearly independent and span \\(\\cal W\\).\n\n\n7.4.1 Examples\n\\(\\cal R^3\\):\nThe vectors (1,0,0), (0,1,0) and (0,0,1) are easily seen to be a basis here. They are linearly independent, and clearly span \\(\\cal R^3\\). For instance, to generate (3,1,-0.2), we use this linear combination:Our convention has been that vectors are considered in matrix terms as column vectors by default. However, in nonmatrix contexts, it will be convenient to write \\(\\cal R^n\\) vectors as rows.\n(3,1,-0.2) = 3 (1,0,0) + 1 (0,1,0) + (-0.2) (0,0,,1)\nBut bases are not unique; for instance, the set (1,0,0, (0,1,0), (0,1,1) works equally well as a basis for this space, as are (infinitely) many others..\nA basis for the subspace of vectors of the form (a,a,b) is (1,1,0) and (0,0,1).\n\\(C(0,1)\\):\nAlas, there is no finite basis here. Even infinite ones have issues in their mathematical formulation.For those with a background in mathematical analysis, here is how the problem is handled. One starts with a subspace that is dense in the full space, i.e. any vector in the full space can be approximated to any precision by some linear combination of vectors in the dense set. For instance, the famoua Stone-Weierstrauss Theorem says that the set of all polynomials is dense in \\(C(0,1)\\). One then defines the dense set to be a ``basis.’’ The use of trig functions as the dense set is much more common than use of polynomials, in the familiar Fourier series.\n\\(\\cal RV(\\Omega)\\):\nThe situation here is the same as for C(0,1)."
  },
  {
    "objectID": "Ch5a.html#dimension",
    "href": "Ch5a.html#dimension",
    "title": "7  Vector Spaces",
    "section": "7.5 Dimension",
    "text": "7.5 Dimension\nGeometrically, we often refer to what is called \\(\\cal R^3\\) here as ``3-dimensional.’’ We extend this to general vector spaces as follows:\n\nThe dimension of a vector space is the number of vectors in any of its bases.\n\nThere is a bit of a landmine in that definition, as it presumes that all the bases do consist of the same number of vectors. This is true, but must be proven. Here we follow the elegant proof AF Beardon (Algebra and Geometry, 2005, Cambridge).\n\nTheorem: The number of vectors is the same in every basis.\nProof: Consider two bases, \\(b_u = u_1,...,u_m\\) and \\(b_v = v_1,...,v_n\\) By definition, each \\(u_i\\) in \\(b_u\\) can be represented by \\(b_v\\) and vice versa:\n\\[\nu_i = r_{i1} v_1 + ... + r_{in} v_n = \\sum_{q=1}^n r_{iq} v_q\n\\tag{7.1}\\]\n\\[\nv_j = s_{j1} u_1 + ... + s_{jm} u_m \\sum_{w=1}^m s_{jw} u_w\n\\tag{7.2}\\]\nSubstituting the second equation into the first, we have\n\\[\nu_k = \\sum_{q=1}^n r_{kq} \\sum_{w=1}^m s_{qw} u_w =\n\\sum_{q=1}^n \\sum_{w=1}^m r_{kq}s_{qw} u_w\n\\]\nSince the \\(u_w\\) form a basis, then the below Your Turn problem on the uniqueness of coefficients in a basis representation says we can equate coefficients on both sides of the last equation. The coefficient of \\(u_k\\) on the right side is\n\\[\n\\sum_{q=1}^n r_{kq} s_{qk}\n\\]\nwhereas on the left side, it is 1. So,\n\\[\n1 = \\sum_{q=1}^n r_{kq} s_{qk}\n\\]\nSumming over \\(k\\), we obtain\n\\[\nn = \\sum_{k=1}^m \\sum_{q=1}^n r_{kq} s_{qk}\n\\]\nBut going through the same steps as above, but now substituting Equation 7.1 into Equation 7.2, we would have\n\\[\nm = \\sum_{p=1}^m \\sum_{q=1}^m\nr_{pq}\ns_{qp}\n\\]\nThe sums in the last two equations are the same!\nTherefore \\(m = n\\), i.e. the two bases are of the same size."
  },
  {
    "objectID": "Ch5a.html#your-turn",
    "href": "Ch5a.html#your-turn",
    "title": "7  Vector Spaces",
    "section": "7.6 Your Turn",
    "text": "7.6 Your Turn\n❄️ Your Turn: Say \\(U\\) and \\(V\\) are subspaces of \\(W\\). Show that \\(U \\cap V\\) is also a subspace, and that its dimension is at most the minimum of the dimensions of \\(U\\) and \\(V\\). Show by counterexample tha result does not hold for union.\n❄️ Your Turn: Citing the properties of expected value, E(), show that the set of all random variable with mean 0 latter set is indeed a subspace of \\(\\cal RV(\\Omega)\\).\n❄️ Your Turn: Prove that the coefficients in basis representations are unique. In other words, in a representation of the vector \\(x\\) in terms of a basis \\(u_1,...,u_n\\),"
  },
  {
    "objectID": "Ch5b.html#geometric-aspirations",
    "href": "Ch5b.html#geometric-aspirations",
    "title": "8  Inner Product Spaces",
    "section": "8.1 Geometric Aspirations",
    "text": "8.1 Geometric Aspirations\nYou may recall from your high school geometry course the key concept of perpendicularity, represented by the ⊥ symbol. You may also recall that in 2-dimensional space, given a point P and a line L, the line drawn from point P to the closest point P’ within L is perpendicular to L. The same is true if L is a plane. The point P’ is called the projection of P onto L.\nThis was shown in this book’s cover, shown here:\n\n\n\nProjections\n\n\nThe point at the end of the green vector is projected onto the mustard-colored plane, producing the red vector. It in turn is projected onto the blue line. There are right angles in each case.\nThe early developers of linear algebra wanted to extend such concepts to abstract vector spaces. This aids intuition, and has very powerful applications."
  },
  {
    "objectID": "Ch5b.html#definition",
    "href": "Ch5b.html#definition",
    "title": "8  Inner Product Spaces",
    "section": "8.2 Definition",
    "text": "8.2 Definition\nYou may have seen dot products in a course on vector calculus or physics. For instance, the dot product of the vectors (3,1,1.5)’ and (0,5,6)’ is\n3x0 + 1x5 + 1.5x6 = 14\nThis in fact is a standard inner product on \\(\\cal R^3\\), but the general definition is as follows.\n\nAn inner product on a vector space \\(\\cal V\\), denoted by the ``angle brackets’’ notation \\(&lt;u,v&gt;\\), is a function with two vectors as arguments and a numerical output, with the following properties:\n\n\\(&lt;u,v&gt; = &lt;v,u&gt;\\)\nThe function is bilinear:\n\\[\n&lt;u,av+bw&gt; = a &lt;u,v&gt; + b &lt;u,w&gt;\n\\]\n\\(&lt;u,u&gt; ~ \\geq 0\\), with equality if and only if \\(u = 0\\)."
  },
  {
    "objectID": "Ch5b.html#examples",
    "href": "Ch5b.html#examples",
    "title": "8  Inner Product Spaces",
    "section": "8.3 Examples",
    "text": "8.3 Examples\n\\(\\cal R^n\\):\nAs noted, ordinary dot product is the most common inner product on this space.\n\\(&lt;(a_1,...,a_n),(b_1,...,b_n&gt; = a_1 b_1 + ... + a_n b_n\\)\nAn \\(n ~ \\textrm{x} ~ n\\) symmetric matrix \\(M\\) is called positive definite if\n\\[\nw'Mw &gt; 0\n\\]\nfor all nonzero vectors \\(w\\). (And the term used is nonnegative definite if \\(&gt;\\) is replaced by $ $.) Show that \\(&lt;u,v&gt; = u'Mv\\) is an inner product on \\(\\cal R^n\\).)\nC(0,1):\nOne inner product on this space is\n\\[\n&lt;f,g&gt; = \\int_{0}^{1} f(t) g(t) ~ dt\n\\]\nFor instance, with \\(f(t) = t^2\\) and \\(g(t) = \\sin(t)\\), the inner product can be computed with R:\n\nf &lt;- function(t) t^2\ng &lt;- function(t) sin(t)\nfg &lt;- function(t) f(t) * g(t)\nintegrate(fg,0,1)\n\n0.2232443 with absolute error &lt; 2.5e-15\n\n\nThis clearly fits most requirements for inner products, but what about \\(&lt;f,f&gt; = 0\\) only if \\(f = 0\\)? A non-0 \\(f\\) will have \\(f^2(t) &gt; 0\\) for at least one \\(t\\), and by continuity, \\(f^2(t) &gt; 0\\) on an interval containing that \\(t\\), thus making a nonzero contribution to the integral and thus to the inner product.Note that the 0 vector in this space is the function that is identically 0, not just 0 at some points\n\\(\\cal RV(\\Omega)\\):\nFrom here on, we’ll restrict to the vector space of all random variables on \\(\\Omega\\) having finite variance. We define\n\\[\n&lt;X,Y&gt; = E(XY)\n\\]\nThe properties of expected value, e.g. linearity, show most of the requirements for an inner product hold. But again, we need to show that\n\\[\n&lt;X,X&gt; = E(X^2) &gt; 0\n\\]\nfor any nonzero \\(X\\), i.e. any \\(X\\) such that \\(P(X = 0) &lt; 1\\)."
  },
  {
    "objectID": "Ch5b.html#norm-of-a-vector",
    "href": "Ch5b.html#norm-of-a-vector",
    "title": "8  Inner Product Spaces",
    "section": "8.4 Norm of a Vector",
    "text": "8.4 Norm of a Vector\nThis concept extends the notion of a the length of a vector, as we know it in \\(\\cal R^2\\) and \\(\\cal R^3\\).\nDefinition:\n\nThe norm of a vector \\(x\\), denoted \\(||x||\\), is\n\\[\n(&lt;x,x&gt;)^{0.5}\n\\]\nThe distance from a vector \\(x\\) to a vector \\(y\\) is\n\\[\n||y - x||\n\\]"
  },
  {
    "objectID": "Ch5b.html#important-inequalities",
    "href": "Ch5b.html#important-inequalities",
    "title": "8  Inner Product Spaces",
    "section": "8.5 Important Inequalities",
    "text": "8.5 Important Inequalities\nThese relations are highly useful, as we will see in the sequel.\n\n8.5.1 The Cauchy-Schwarz Inequality\n\nTheorem 8.1 (Cauchy-Schwarz Inequality) Say \\(u\\) and \\(v\\) are vectors in an inner product space. Then\n\\[\n|&lt;u,v&gt;| \\leq ||u|| ~ ||v||\n\\]\n\n\nProof. See the Your Turn problem below.\n\n\n\n8.5.2 Application: Correlation\nCorrelation coefficients are ubiquitous in data science. It is well known that their values fall into the interval [-1,1]. Let’s prove that.\nRecall from Section 4.1 the notion of the covariance between two random variables (from which the covariance matrix of a random vector is formed). We remarked that covariance is intuitively like correlation,but that latter is a scaled form. Formally,\n\\[\n\\rho(X,Y) =\n\\frac{E[(X-EX)(Y-EY)]}{\\sqrt{Var(X)} \\sqrt{Var(Y)}}\n\\]\nBy dividing the covariance by the product of the standard deviations, we obtain a unitless quantity, i.e. free of units such as centimeters and degrees.\nNow, \\(X\\) and \\(Y\\) are in \\(\\cal RV(\\Omega)\\). To simplify the algebra, consider the case \\(EX = EY = 0\\).Actually, we should say, ``Make the transformation \\(X \\rightarrow X - EX\\), and note that it leaves both sides of the above correlation formula unchanged. We can thus assume \\(EX = EY = 0\\).’’ This is a common strategy, and should be kept in mind.\nRecalling our inner product for this space, we have\n\\[\n&lt;X,Y&gt; = E(XY)\n\\]\nand\n\\[\n||X||^2 = &lt;X,X&gt; = E(X^2) = Var(X)\n\\]\nwith the analogous relations for \\(Y\\).\nCauchy=Schwarz then says\n\\[\n|E(XY)| \\leq \\sqrt{Var(X)} \\sqrt{Var(Y)}\n\\]\nwhich says the correlation is between -1 and 1 inclusive.\n\n\n8.5.3 The Triangle Inequality\n\nIn the world of ordinary physical geometry, we know the following\nThe distance from A to B is less than or equal to the sum of the distances from A to C and C to B.\n\nTheorem 8.2 (Triangle Inequality) In a general inner product space,\n\\[\n||x - z|| \\leq ||x - y|| + ||y - z||\n\\]\n\n::: {.proof}\nSee the Your Turn problem below."
  },
  {
    "objectID": "Ch5b.html#sec-projections",
    "href": "Ch5b.html#sec-projections",
    "title": "8  Inner Product Spaces",
    "section": "8.6 Projections",
    "text": "8.6 Projections\nAs mentioned, the extension of classical geometry to abstract vector spaces has powerful applications. There is no better example of this than the idea of projections.\n\n8.6.1 Theorem\nWe say that vectors \\(u\\) and \\(v\\) are orthogonal if \\(&lt;u,v&gt; = 0\\). This is the general extension of the notion of perpendicularity in high school geometry. Then we have the following:\n\nTheorem 8.3 (Projection Theorem) Consider an inner product space \\(\\cal V\\), with subspace \\(\\cal W\\). Then for any vector \\(x\\) in \\(\\cal W\\), there is a unique vector \\(z\\) in \\(\\cal V\\), such that \\(z\\) is the closest vector to \\(x\\) in \\(\\cal W\\).\nFurthermore, \\(x-z\\) is orthogonal to any vector \\(r\\) in \\(\\cal W\\).\n\nThe full proof is beyond the scope of this book, as it requires background in real analysis. Indeed, even the statement of the theorem is not mathematically tight.For readers who do have such background, this is the Hilbert Projection Theorem. “Closest” is defined in terms of infimum, and \\(\\cal W\\) needs to be a topologically closed set. See for example the Wikipedia entry.l\nFor example, in the case of \\(R^n\\), It will be seen shortly that for each \\(\\cal W\\) in the theorem, there is a matrix \\(P_{W}\\) that implements the projection, i.e.\n\\[\nz = P_W x\n\\]\nNote that projection operators are idempotent, meaning that if you apply a projection twice, the effect is the same as applying it once. In the matrix equation above, this means \\(P_W^2 = P_W\\). This makes sense; once you drop down to the subspace, there is no further dropping down to that same space.\nThe case of \\(\\cal RV(\\Omega)\\) deserves its own section, coming up next.\n\n\n8.6.2 The Pythagorean Theorem\nThat this ancient theorem in geometry still holds in general inner product spaces is a tribute to the power of abstraction.\n\nThe Pythagorean Theorem\nIf vectors \\(X\\) and \\(Y\\) are orthogonal, then\n\\[\n||X+Y||^2 = ||X||^2 + ||Y||^2\n\\tag{8.1}\\]"
  },
  {
    "objectID": "Ch5b.html#projections-incal-rvomega",
    "href": "Ch5b.html#projections-incal-rvomega",
    "title": "8  Inner Product Spaces",
    "section": "8.7 Projections in\\(\\cal RV(\\Omega)\\)",
    "text": "8.7 Projections in\\(\\cal RV(\\Omega)\\)\nHere is what this very abstract vector space becomes useful in practical applications, such as will be presented in Section 8.11. We will need to build a bit of infrastructure.The material in this section is rather involved, but it is needed for the section on racial, gender etc. fairness in machine learning, Section 8.11, which hopefully makes it worthwhile.\n\n8.7.1 Conditional expectation\nOne of the Your Turn problems at the end of this chapter covers this setting:\n\nSay we roll a die once, producing \\(X\\) dots. If \\(X = 6\\), we get a bonus roll, yielding \\(B\\) additional dots; otherwise, \\(B = 0\\). Let \\(Y = X+B\\).\n\nThe basic form:\nNow, what is \\(E(Y | B = 2)\\)? If \\(B = 2\\), then we got the bonus roll, so \\(X = 6\\) and \\(Y = X + B = 8\\):\n\\[\nE(Y | B = 2) = 8\n\\]\nMore generally,\n\\[\nE(Y | B = i) =\n\\begin{cases}\n3 & i = 0 \\\\\n6 + i & i = 1,2,3,4,5\n\\end{cases}\n\\]\nThe random variable form:\nThe quantity \\(E(Y | B = i)\\), a number, can be converted to a random variable, in the form of a function of \\(B\\), which we will call \\(Q\\), and denoted \\(E(Y | B)\\), where\n\\[\nQ =\n\\begin{cases}\n3 & B = 0 \\\\\n6 + B & B = 1,2,3,4,5\n\\end{cases}\n\\]\n\\(B\\) is random, so \\(r(B)\\) is also random, and here is its distribution:\n\n\\[\nQ \\textrm{ takes on the values } 3,7,8,9,10,11 \\textrm{ with\nprobabilities }\n\\] \\[\n1/6, 1/36, 1/36, 1/36, 1/36, 1/36\n\\]\n\nWe need one more thing:\nThe Law of Iterated Expectation:\nFor random variable \\(U\\) and \\(V\\), set\n\\[\nR = E[V |U]\n\\]\nThen\n\\[\nE(R) = E(V)\n\\]\nMore concisely:\n\\[\nE[E(V|U)] = E(V)\n\\tag{8.2}\\]\nIntuitive explanation: Say we wish to compute the mean height \\(E(H)\\) of all students at a university. We might ask each department \\(D\\) to measure their own students, and report to us the resulting mean \\(E(H|D)\\). We could then average all those departmental means to get the overall mean for the university:\n\\[\nE[E(H|D)] = E(H)\n\\]\nNote, though that that outer \\(E()\\) (the first ‘E’) is a weighted average, since some departments are larger than others. The weights are the distribution of \\(D\\).\n\n\n8.7.2 Projections in \\(\\cal RV(\\Omega)\\): how they work\nConsider random variables \\(X\\) and \\(Y\\). Let \\(W\\) be the set of all functions of \\(X\\) with finite variance, which is a subspace of the vector space \\(\\cal RV(\\Omega)\\). The above theorem talks of a closest vector \\(C\\) in \\(W\\) to \\(Y\\). Let’s see what form \\(C\\) might take in this vector space.\nRemember, the (squared) distance from \\(Y\\) to \\(C\\) is\n\\[\n||Y - C||^2 = &lt;Y-C,Y-C&gt; = E[(Y-C)^2]\n\\]\nThat last term is\n\\[\nE[ E((Y-C)^2 | X) ]\n\\]\nFor any random variable \\(Q\\) of finite variance, the minimum value of \\(E[(Q-d)^2]\\) over all constants \\(d\\) is attained by \\(d = E(Q)\\). (See Your Turn problem below.) So, the minimum of \\(E((Y-C)^2 | C)\\), for all random variables \\(C\\), is attained by the conditional mean,Note that since we are conditioning on the random variable \\(C\\), it becomes a constant, like \\(d\\) above.\n\\[\nC = E(Y | X)\n\\]\nIn other words:\n\nProjections in \\(\\cal RV(\\Omega)\\) are conditional means.\n\nMoreover:\nSince the difference between a vector and its projection onto a subspace is orthogonal to that subspace we have:\n\nThe vector \\(Y - E(Y|X)\\) is uncorrelated with \\(E(Y|X)\\). In other words, the prediction error (also called the residual has 0 correlation with the prediction itself."
  },
  {
    "objectID": "Ch5b.html#projections-in-the-linear-model",
    "href": "Ch5b.html#projections-in-the-linear-model",
    "title": "8  Inner Product Spaces",
    "section": "8.8 Projections in the Linear Model",
    "text": "8.8 Projections in the Linear Model\nThe case of the linear model will deepen our understanding, and will lead to a method for outlier detection that is commonly used in practice.\n\n8.8.1 The least-squares solution is a projection\nArmed with our new expertise on inner product spaces, we see that Equation 5.1 is\n\\[\n&lt;S-Ab,S-Ab&gt;\n\\]\nin the vector space \\(R^n\\), where \\(n\\) is the number of our data points. Since we are minimizing that quantity with respect to \\(b\\), the solution, \\(A \\widehat{\\beta}\\), is the projection of \\(Y\\) onto the subspace.Of course, “data points” means rows in the data frame. In the statistics realm, people often speak of “observations.”\nBut wait – what subspace? Well, it is the subspace consisting of all vectors of the form \\(Ab\\):\n\nThe linear model projects the vector \\(Y\\) onto the column space of \\(A\\).\n\n\n\n8.8.2 Application: identifying outliers\nAn outlier is a data point that is rather far from the others. It could be an error, or simply an anomalous case. Even in the latter situation, such a data point could distort our results, so in both cases, identifying outliers is important.\nRecall Equation 5.4, the general solution to our linear regression model:\n\\[\n\\widehat{\\beta} = (A'A)^{-1} A'S\n\\]\nThe projection itself is then\n\\[\nA \\widehat{\\beta} = A(A'A)^{-1} A'S = HS\n\\]\nwhere the matrix\n\\[\nH = A(A'A)^{-1} A'\n\\]\nwhich projects \\(S\\) onto the column space of \\(A\\), is called the hat matrix.\nAs a projection, \\(H\\) is idempotent, which one can easily verify by multiplication. \\(H\\) is also symmetric.\nLet \\(h_{ii}\\) denote element \\(i\\) of the diagonal of \\(H\\), with \\(x_i\\) denoting row \\(i\\) of A. One can show that\n\\[\nh_{ii} = x_{i} (A'A)^{-1} x_i'\n\\tag{8.3}\\]\nThe quantity \\(h_{ii}\\) is called the leverage for datapoint \\(i\\).\nAh, so we know \\(h_{ii} \\geq 0\\). But also, using the material on circular shifts in Section 2.10.3, we have\n\\[\ntr(H) =\ntr[\\underbrace{A(A'A)^{-1}} A'] =\ntr[A'\\underbrace{A(A'A)^{-1}}] = tr(I) = p\n\\]\nfor \\(A\\) of size \\(n \\textrm{x} p\\).\nThus the average value of \\(h_{ii}\\) is \\(p/n\\). Accordingly, we might suspect an outlier if \\(h_{ii}\\) is considerably larger than \\(p/n\\).\nFor example, let’s look at the census data we’ve seen earlier:\n\nlibrary(qeML)\ndata(mlb1)\nourData &lt;- as.matrix(mlb1[,-1]) # must have matrix to enable %*%\nA &lt;- cbind(1,ourData[,c(1,3)])\ndim(A)\n\n[1] 1015    3\n\nS &lt;- as.vector(mlb1[,3])\nH &lt;- A %*% solve(t(A) %*% A) %*% t(A)\nhist(diag(H))\n\n\n\n\nThe ratio \\(p/n\\) here is 3/1015, about 0.003. We might take a look at the observations having \\(h_{ii}\\) above 0.01, say."
  },
  {
    "objectID": "Ch5b.html#orthogonal-bases",
    "href": "Ch5b.html#orthogonal-bases",
    "title": "8  Inner Product Spaces",
    "section": "8.9 Orthogonal Bases",
    "text": "8.9 Orthogonal Bases\nIt turns out that a basis for a vector space is especially useful if its members are orthogonal to each other. We’ll see why, and see how to generate such a basis from a nonorthogonal one.\nAn orthogonal basis in which every vector has length 1 is called orthonormal. Recall that \\(x \\neq 0\\) then \\(x/||x||\\) has length 1, so that any orthogonal basis can easily be converted to orthonormal.\n\n8.9.1 Motivation\nSay we have a vector space \\(\\cal V\\), a subspace \\(\\cal W\\), and a vector \\(x\\) in \\(\\cal V\\). We know \\(x\\) has a projection in \\(\\cal W\\); call it \\(z\\). But how do we find \\(z\\)?\nLet \\(u_1,...,u_k\\) be a basis for \\(\\cal W\\). Then there exist \\(a_1,...,a_k\\) such thatSo we are assuming \\(\\cal W\\) (but not necessarily \\(\\cal V\\) is finite-dimensional. Using proper math, this could be extended.\n\\[\nz = a_1 u_1 + ..., + a_k u_k\n\\tag{8.4}\\]\nSo we can find \\(z\\) by finding the \\(a_i\\).\n\n\n8.9.2 The virtues of orthogonality\nWe do have a hint to work from: We know that \\(x-z\\) is orthogonal to every vector in \\(\\cal W\\) – including the \\(u_i\\). So\n\\[\n0 = &lt;x-z,u_i&gt; = &lt;x,u_i&gt; - &lt;z,u_i&gt;\n\\]\nThus\n\\[\n&lt;x,u_i&gt; = &lt;z,u_i&gt;\n\\]\nNow, say the \\(u_i\\) are orthogonal to each other, and let’s also say they are length 1. Then \\(&lt;z,u_i&gt; = a_i\\),\nso\n\\[\n&lt;x,u_i&gt; = a_i\n\\]\nSo, we’re done! We want to determine the \\(a_i\\), and now we see that we can easily obtain it by calculating \\(&lt;x,u_i&gt;\\). So, we have:\n\nGiven: a vector space \\(\\cal V\\); a subspace \\(\\cal W\\) with orthonormal basis \\(u_1,...,u_k\\); and a vector \\(x\\) in \\(\\cal V\\). Then the projection of \\(x\\) onto \\(\\cal V\\) is equal to\n\\[\np = &lt;x,u_1&gt; u_1+...+&lt;x,u_k&gt; u_k.\n\\tag{8.5}\\]\nAnd, as a projection, we have that \\(x-p\\) is orthogonal to all the \\(u_i\\).\n\nBut how do we obtain an orthogonal basis, if we only have a nonorthogonal one? That’s next…\n\n\n8.9.3 The Gram-Schmidt Method\nAs seen in the last section, it is desirable to have an orthogonal basis, and it’s even more convenient if its vectors have length 1 (an orthonormal basis). Converting to length 1 is trivial – just divide the vector by its length.\nBut if we start with a basis \\(b_i\\) and generate, say, orthogonal, length 1 vectors \\(u_1,...,u_m\\), how do we get \\(u_{m+1}\\) from \\(b_{m+1}\\)? the answer lies in Equation 8.5: Take the projection \\(q\\) of \\(b_{m+1}\\) onto the subspace generated by \\(u_1,...,u_m\\); \\(b_{m+1}-q\\) will be orthogonal to \\(u_1,...,u_m\\), so we can take \\(u_{m+1}\\) to be \\(b_{m+1}-q\\)! We then normalize it.\nSo, here is the process for conversion:\n\nThe Gram-Schmidt Method\nSay we have a basis \\(b_1,...,b_k\\) for some vector space. Convert it to an orthonormal basis as follows.\n\nSet \\(u_1 = b_1/||b_1||\\).\nFor each \\(i = 2,...,k\\), find the projection \\(q\\) of \\(b_{i}\\) onto the subspace generated by \\(u_1,...,u_{i-1}\\). Set \\(u_i\\) to \\(b_i-q\\), and normalize."
  },
  {
    "objectID": "Ch5b.html#orthogonal-complements-and-direct-sums",
    "href": "Ch5b.html#orthogonal-complements-and-direct-sums",
    "title": "8  Inner Product Spaces",
    "section": "8.10 Orthogonal Complements and Direct Sums",
    "text": "8.10 Orthogonal Complements and Direct Sums\nConsider a subspace \\(W\\) of an inner product space \\(V\\). The set of vectors having inner product 0 with vectors in \\(W\\) is denoted \\(W^{\\perp}\\), known as the orthogonal complement of \\(W\\). It too is a subpace, and jointly \\(W\\) and \\(W^{\\perp}\\) span all of \\(V\\).\nFrom Section 8.6, we know that for any \\(x\\) in \\(V\\), one can uniquely write\n\\[\nx = x_1 + x_2\n\\]\nwhere \\(x_1\\) and \\(x_2\\) are in \\(W\\) and \\(W^{\\perp}\\), respectively.\nSay \\(u_1,...,u_r\\) and \\(v_1,...,v_s\\) are bases for \\(W\\) and \\(W^{\\perp}\\). Then together they form a basis for all of \\(V\\). Typically they are chosen to be orthonormal.\nFinally, we say that \\(V\\) is the direct sum of \\(W\\) and \\(W^{\\perp}\\), denoted \\(V = W \\oplus W^{\\perp}\\)."
  },
  {
    "objectID": "Ch5b.html#sec-fairness",
    "href": "Ch5b.html#sec-fairness",
    "title": "8  Inner Product Spaces",
    "section": "8.11 Application: Racial, Gender Etc. Fairness in Algorithms",
    "text": "8.11 Application: Racial, Gender Etc. Fairness in Algorithms\nCOMPAS is a software tool designed to aid judges in determining sentences in criminal trials, by assessing the probability that the defendant would recidivate. It is a commercial product by Northpointe.\nCOMPAS came under intense scrutiny after an investigation by ProPublica, which asserted evidence of racial bias against black defendants compared to white defendants with similar profiles. Northpointe contested these findings, asserting that their software treated black and white defendants equally.\nIt should be noted the ProPublica did not accuse Northpointe of intentional bias. Instead, the issue largely concerns proxies, variables that are related to race, rather than race (or gender etc.) itself. If for example COMPAS were to use a person’s home location as a predictor, that would be correlated to race, and thus would be unfair to use in prediction. On the other hand, if the prediction tool makes use of the number of prior criminal convictions, that would be reasonable, even though it too might be a proxy for race.\nOn the other hand, even using race indirectly for any reason would likely be unacceptable. While this book does not take a position on the specific dispute, this case highlights the critical importance of addressing fairness in machine learning.\n\n8.11.1 Setting\nWe consider prediction of a variable \\(Y\\) from a feature vector \\(X\\) and a vector of sensitive variables \\(S\\). The target \\(Y\\) may be either numeric (in a regression setting) or dichotomous (in a two-class classification setting where \\(Y\\) = 1 or \\(Y\\) = 0). The \\(m\\)-class case can be handled using \\(m\\) dichotomous variables. We will consider only the numeric case here.\nOur goal is to reduce or eliminate the influence of the sensitive variable \\(S\\). One might ask, why not simply remove \\(S\\) from our model-fitting process? The problem is that \\(X\\) may include variables that are related to \\(S\\). In the COMPAS example, race has some correlation with another variable, number of prior convictions.\n\n\n8.11.2 The method of Scutari et al\nThe basic assumption (BA) amounts to \\((Y,X,S)\\) having a multivariate Gaussian distribution, with \\(Y\\) scalar and \\(X\\) being a vector of length \\(p\\). For convenience, assume here that \\(S\\) is scalar. As in Section 8.5.2, all variables are assumed centered, i.e. mean 0.\nLet’s review the material in Section 4.2: Say we have \\(W\\) with a multivariate normal distribution, and wish to predict one of its components, \\(Y\\), from a vector \\(X\\) consisting of one or more of the other components, or linear combinations of them. Then\n\nthe distribution of \\(Y|X\\) is univeriate normal\nE(Y|X=t) is a linear function of \\(t\\)\nVar(Y|X=t) is independent of \\(t\\)\n\nLet’s add:\n\nthough having 0 correlation does not in general imply independence, it does so in the multivariate normal case\n\nOne first applies a linear model in regressing \\(X\\) on \\(S\\),\n\\[\nE(X | S) = S \\gamma\n\\]\nwhere \\(\\gamma\\) is a length-\\(p\\) coefficient vector. Here we are predicting the predictors (of \\(Y\\)), seemingly odd, but a first step in ridding ourselves from the influence of \\(S\\).\nNow consider the prediction errors (residuals),\n\\[\nU = X - S \\gamma\n\\]\n\\(U\\) can be viewed as the part of \\(X\\) that is unrelated to \\(S\\); think of \\(U\\) as “having no \\(S\\) content.” Note that \\(U\\) is a vector of length \\(p\\).\nNote the following:\n\n\\(E(X|S)\\) is the projection of \\(X\\) onto the subspace of all functions of \\(S\\).\n\\(X - E(X|S)\\) (original vector minus the projection) is orthogonal to \\(S\\).\nThat is,\n\\[\n0 = &lt;S,X - E(X|S)&gt; = E[S (X - E(X|S))] = E(SU)\n\\]\nThus \\(S\\) and \\(U\\) are uncorrelated.\nDue to the BA, that means \\(S\\) and \\(U\\) are independent.\nIn other words, our intution above that \\(U\\) “has no \\(S\\) content” was mathematically correct.\nBottom line: Instead of predicting \\(Y\\) from \\(X\\), use \\(U\\) as the predictor vector. This will enable truly \\(S\\)-free prediction.\n\nGoal achieved."
  },
  {
    "objectID": "Ch5b.html#your-turn",
    "href": "Ch5b.html#your-turn",
    "title": "8  Inner Product Spaces",
    "section": "8.12 Your Turn",
    "text": "8.12 Your Turn\n❄️ Your Turn: Consider the space \\(\\cal RV(\\Omega)\\). In order for the claimed inner product to be valid, we must have that if \\(&lt;X,X&gt; = 0\\), then \\(X\\) must be the 0 vector. Prove this. Make use of the facts that \\(Var(X) = E(X^2) - [E(X)]^2\\) and \\(Var(X) \\geq 0\\) with equality only of \\(X = c\\) for some constant \\(c\\).\n❄️ Your Turn: Derive the Cauchy-Schwarz Inequality, using the following algebraic outline:\n\nThe inequality\n\\[\n0 \\leq &lt;(au+v),(au+v)&gt;\n\\]\nholds for any scalar \\(a\\).\nExpand the right-hand side (RHS), using the bilinear property of inner products.\nMinimize the resulting RHS with respect to \\(a\\).\nCollect terms to yield\n\\[\n&lt;u,v&gt;^2 \\leq ||u||^2 ||v||^2\n\\]\n\n❄️ Your Turn: Consider a set of vectors \\(W = {v_1,...,v_k}\\) in an inner product space \\(V\\). Let \\(U\\) be another vector in \\(V\\). Show that there exist scalars \\(a_1,...,a_k\\) and a vector \\(v\\) such that\n\\[\nu = a_1 v_1 + ... + a_k v_k +v\n\\]\nwith\n\\[\n&lt;v,v_i&gt; = 0 \\textrm{ for all i}\n\\]\n❄️ Your Turn: Say in \\(C(0,1)\\) we want to approximate functions by polynomials. Specifically, for any \\(f\\) in \\(C(0,1)\\), we want to find the closest polynomial of degree \\(m\\). Write functions to do this, with the following call forms:\ngsc01(m)  # performs Gram-Schmidt and returns the result\nbestpoly(f,gsout)  # approx. f by output from gsc01\nHint: Since the vectors here are functions, you’ll need a data structure capable of storing functions. An R list will work well here.\n❄️ Your Turn: Show that the hat matrix is symmetric.\n❄️ Your Turn: Use the Cauchy-Schwarz Inequality to prove the Triangle Inequality, using the following algebraic outline.\n\nStart with\n\\[\n||u+v||^2 = &lt;(u+v,u+v&gt;\n\\]\nExpand the RHS algebraically.\nUsing Cauchy-Schwarz to make the equation an inequality.\nCollect terms to yield the Triangle Inequality.\n\n❄️ Your Turn: Consider the space \\(C(0,1)\\). For function \\(f\\) and \\(g\\) of your own choosing, verify that the Cauchy-Schwarz and Triangle Inequalities hold in that case.\n❄️ Your Turn: In the die rolling example, verify that\n\\[\nE[E(Y|B)] = E(Y)\n\\]\n❄️ Your Turn: Say we roll a die once, producing \\(X\\) dots. If \\(X = 6\\), we get a bonus roll, yielding \\(B\\) additional dots; otherwise, \\(B = 0\\). Let \\(Y = X+B\\)…\n❄️ Your Turn: Say we roll a die once, producing \\(X\\) dots. If \\(X = 6\\), we get a bonus roll, yielding \\(B\\) additional dots; otherwise, \\(B = 0\\). Let \\(Y = X+B\\). Verify that \\(X\\) and \\(B\\) satisfy the Cauchy-Schwarz and Triangle Inequalities, and also find \\(\\rho(X,B)\\).\n❄️ Your Turn: Show that any set of orthogonal vectors is linearly independent.\n❄️ Your Turn: Prove Equation 8.1 by expanding the inner product and simplifying algebraically.\n❄️ Your Turn: Prove Theorem 8.2.\n❄️ Your Turn: Prove that the vector \\(Y - E(Y|X)\\) is uncorrelated with \\(E(Y|X)\\)\n❄️ Your Turn: For \\(X\\) and \\(Y\\) in \\(\\cal RV(\\Omega\\), prove that\n\\[\nVar(Y) = E[Var(Y|X)] + Var[E(Y|X)],\n\\]\nfirst algebraically using Equation 8.2 and the relation \\(Var(R) = E(R^2) - (E(R))^2\\), and then using the Pythagorean Theorem for a much quicker proof. As before, assume \\(X\\) and \\(Y\\) are centered.\n❄️ your turn: show that Equation 8.3 holds. holds. holds.\n❄️ Your Turn: Say \\(\\cal V\\) is \\(R^n\\). Form the matrix \\(A\\) whose columns are the \\(u_i\\), and let \\(P = AA'\\). Show that\n\\[\nPx = &lt;x,u_1&gt; u_1+...+&lt;x,u_k&gt; u_k\n\\]\nso that \\(P\\) thereby implements the projection.\n❄️ Your Turn: Consider the quadratic form \\(x'Px\\). Show that if \\(P\\) is a projection matrix, the form equals \\(||Px||^2\\).\n❄️ Your Turn: Let \\(A\\) be an \\(m \\textrm{ x } n\\) matrix. Consider the possible inner product on \\(\\cal{R}^n\\) defined by \\(&lt;x,y&gt; = x'A'A y\\). State a condition on \\(A\\) that is necessary and sufficient for the claimed inner product to be value, and prove this."
  },
  {
    "objectID": "Ch5c.html#sec-nearly",
    "href": "Ch5c.html#sec-nearly",
    "title": "9  Shrinkage Estimators",
    "section": "9.1 Multicollinearity",
    "text": "9.1 Multicollinearity\nThe term multicollinearity refers to settings in which the following concerns arise:\n\nOne column of the matrix \\(A\\) in Equation 5.4 is nearly equal to some linear combination of the others.\nThus \\(A\\) is nearly not of full rank.\nThus \\(A'A\\) is nearly not of full rank.\nThus \\(\\widehat{\\beta}\\) is unstable, in the form of high variance.\n\n(Technically, the above conditions refer to approximate multicollinearity, with the exact version consisting of the first three bullet points, minus the word “nearly.” However, informally, the term multicollinearity is usually taken to mean the approximate condition.)\nThat latter point is often quantified by the Variance Inflation Factor. To motivate it, consider the “R-squared” value from linear regression analyis, which is the squared correlation between “Y” and predicted “Y”. Let \\(R^2_j\\) denote that measure in the case of predicting column \\(j\\) of \\(A\\) from the other columns. The quantity\n\\[\nVIF_j = \\frac{1}{1-R^2_j}\n\\]\nthen measures the negative impact due to multicollinearity on estimating \\(\\widehat{\\beta}_j\\). The intuition is that, say, column 3 of \\(A\\) can be predicte well using a linear model, then that column is approximately equal to a linear combination of the other columns. This is worrisome in light of the problems described above.\nNeedless to say, the word “nearly” above, e.g. in “nearly not of full rank,” is vague, and leaves open the question of “What can we do about it?” We will present several answers to these questions in this and the succeeding chapters."
  },
  {
    "objectID": "Ch5c.html#sec-millionsong",
    "href": "Ch5c.html#sec-millionsong",
    "title": "9  Shrinkage Estimators",
    "section": "9.2 Example: Million Song Dataset",
    "text": "9.2 Example: Million Song Dataset\nLet’s consider the Million Song Dataset, varous versions of which are on the Web.\nOurs is a 50,000-line subset of the one with 515345 rows and 91 columns. The first column is the year of release, followed by 90 columns of various audio measurements. The goal is to predict the year, V1, from the audio variables V2 through V91.\nThe function regclass::VIF will compute the VIF values for us.\n\nlibrary(WackyData)\ndata(MillSong50K) # loads s50\nlmout &lt;- lm(V1 ~ .,data=s50)\nlibrary(regclass)\nVIF(lmout)\n\n      V2       V3       V4       V5       V6       V7       V8       V9 \n3.215081 2.596474 4.236853 7.414375 1.534492 5.844729 2.794018 3.192805 \n     V10      V11      V12      V13      V14      V15      V16      V17 \n2.072851 3.673590 4.705886 1.708520 2.446279 2.827296 3.672250 7.409782 \n     V18      V19      V20      V21      V22      V23      V24      V25 \n2.634033 9.472261 4.217311 7.147952 5.122114 7.984860 9.675134 3.591586 \n     V26      V27      V28      V29      V30      V31      V32      V33 \n1.818596 1.758043 3.879968 1.663670 2.108174 2.321385 2.056017 1.854913 \n     V34      V35      V36      V37      V38      V39      V40      V41 \n3.011534 2.040587 2.760111 2.879667 1.918229 2.176048 2.074103 1.946859 \n     V42      V43      V44      V45      V46      V47      V48      V49 \n1.704259 2.138794 1.690651 1.556782 1.817380 3.000759 1.547592 2.140282 \n     V50      V51      V52      V53      V54      V55      V56      V57 \n2.496121 1.612253 2.042571 2.208492 1.723669 2.024290 2.016403 2.033654 \n     V58      V59      V60      V61      V62      V63      V64      V65 \n2.004260 2.998469 2.074152 3.410124 2.153116 1.378160 3.270617 1.502543 \n     V66      V67      V68      V69      V70      V71      V72      V73 \n2.581171 1.725809 2.167673 2.379354 2.062862 1.703360 2.036596 1.984427 \n     V74      V75      V76      V77      V78      V79      V80      V81 \n2.557163 1.465020 1.515436 2.260728 1.840509 2.078497 3.604771 1.595064 \n     V82      V83      V84      V85      V86      V87      V88      V89 \n2.528307 2.005876 2.283956 1.448379 1.895053 1.601004 1.581099 2.252362 \n     V90      V91 \n1.332590 1.570891 \n\n\nAs a rough guide, values of VIF about 5.0 are considered concerning by many analysts. Under that criterion, variables V5, V7, V17 and so on look troublesome.\nWhat can be done? One simple approach would be to delete those columns from the dataset. This is indeed is a common solution, but another is ridge regression, which we present next."
  },
  {
    "objectID": "Ch5c.html#ridge-regression",
    "href": "Ch5c.html#ridge-regression",
    "title": "9  Shrinkage Estimators",
    "section": "9.3 Ridge Regression",
    "text": "9.3 Ridge Regression\nIn seminal paper, Hoerl and Kennard discussed the problem of multicollinearity of predictor variables in a linear model.Technometrids, Februaru1970\n\n9.3.1 The ridge solution\nTheir solution is simple; Add some quantity to the diagonal of \\(A'A\\). Specifically, Equation 5.4 now becomes\n\\[\n\\widehat{\\beta} - (A'A + \\lambda I)^{-1} A'S\n\\tag{9.1}\\]\nwhere \\(\\lambda\\) is a positive number chosen by the analyst.\nHere \\(A\\) has dimensions \\(n \\textrm{ x } p\\), and \\(I\\) is the \\(p \\textrm{ x } p\\) identity matrix.\n\n\n9.3.2 Matrix formulation\nUsing partitioned matrices helps understand ridge. Replace \\(A\\) and \\(S\\) by\n\\[\nA_{new} =\n\\left (\n\\begin{array}{r}\nA \\\\\n\\lambda I \\\\\n\\end{array}\n\\right )\n\\]\nand\n\\[\nS_{new} =\n\\left (\n\\begin{array}{r}\nS \\\\\n0 \\\\\n\\end{array}\n\\right )\n\\]\nwhere 0 means \\(p\\) 0s. In essence, we are adding artificial data here, consisting of \\(p\\) new rows to \\(A\\), and \\(p\\) new elements to \\(S\\). So Equation 9.1 is just the result of applying Equation 5.4 to \\(A_{new}\\) and \\(S_{new}\\).\nLoosely speaking, we can think of the addition of \\(\\lambda I\\) to \\(A'A\\) makes the latter “larger”, and thus its inverse smaller. In other words, we are “shrinking” \\(\\widehat{\\beta}\\) towards 0. This effect is made even stronger by the fact that we added 0s data to \\(S\\). This will be made more precise below.\n\n\n9.3.3 Example: Million Song dataset\nWe will use glmnet, one of the most widely-used R package.\n\nlibrary(glmnet)\nx &lt;- s50[,-1]\ny &lt;- s50[,1]\nglmOut &lt;- glmnet(x,y,\n             alpha=0,  # ridge\n             lambda=0.1)\ncoef(glmOut)\n\n91 x 1 sparse Matrix of class \"dgCMatrix\"\n                       s0\n(Intercept)  1.952611e+03\nV2           8.573258e-01\nV3          -5.589618e-02\nV4          -4.521365e-02\nV5           8.939743e-04\nV6          -9.630525e-03\nV7          -2.075296e-01\nV8          -4.753624e-03\nV9          -9.733144e-02\nV10         -6.383153e-02\nV11          2.778348e-02\nV12         -1.486791e-01\nV13         -1.326579e-02\nV14          4.756877e-02\nV15          3.197157e-04\nV16         -4.765018e-04\nV17          4.951670e-04\nV18          5.276124e-04\nV19          1.254372e-03\nV20          1.518736e-03\nV21          2.206416e-03\nV22         -4.304329e-04\nV23          5.792964e-04\nV24          7.717453e-03\nV25          3.205090e-03\nV26         -3.527333e-03\nV27          4.785205e-05\nV28          1.510325e-03\nV29          2.996810e-04\nV30          6.384049e-04\nV31         -2.560997e-04\nV32         -4.861561e-04\nV33         -6.250266e-04\nV34         -3.757523e-03\nV35          3.563733e-04\nV36          1.288622e-03\nV37         -4.417041e-03\nV38         -2.502325e-04\nV39          9.405005e-04\nV40          1.490186e-03\nV41         -1.534097e-03\nV42         -1.510983e-03\nV43         -1.777780e-03\nV44         -1.814201e-03\nV45         -1.865966e-03\nV46         -1.107727e-03\nV47          5.906898e-03\nV48          6.578124e-04\nV49         -2.040170e-03\nV50          4.795372e-04\nV51          1.162657e-03\nV52          6.164815e-04\nV53         -9.613775e-04\nV54          1.571230e-03\nV55         -1.230929e-03\nV56         -1.395143e-03\nV57          2.158453e-04\nV58         -1.975641e-03\nV59          2.039760e-03\nV60         -1.302405e-03\nV61          6.875221e-04\nV62         -3.480975e-03\nV63         -3.285816e-03\nV64         -9.199332e-03\nV65          1.283424e-03\nV66         -1.444237e-03\nV67         -5.957302e-05\nV68          1.139299e-03\nV69         -9.805816e-04\nV70         -3.734949e-03\nV71         -5.127353e-03\nV72         -1.071399e-03\nV73          1.808175e-04\nV74         -1.034781e-05\nV75          4.315147e-03\nV76          3.382526e-03\nV77          1.111779e-02\nV78          3.162072e-04\nV79         -4.565407e-03\nV80          3.729735e-05\nV81          2.362107e-04\nV82         -9.349464e-04\nV83         -2.846584e-04\nV84          1.439897e-03\nV85          1.360484e-03\nV86          2.442744e-02\nV87         -6.838186e-04\nV88          8.555403e-04\nV89         -3.329155e-02\nV90         -1.793489e-03\nV91         -3.614719e-05\n\n\nWe had earlier flagged variable \\(V5\\) as causing multicollinearity. As noted then, we could simply exclude it, but here under ridge, we see that it has been assigned a very small regression coefficient compared to most others."
  },
  {
    "objectID": "Ch5c.html#sec-xval",
    "href": "Ch5c.html#sec-xval",
    "title": "9  Shrinkage Estimators",
    "section": "9.4 Cross-Validation",
    "text": "9.4 Cross-Validation\nSo, how do we choose \\(\\lambda\\)?\nA common way to choose among models is cross-validation: We set aside a subset of the data, known as the holdout or test set, for use in testing predictive accuracy. The remaining data is the training set. For each of our competing models – in this case, competing values of \\(\\lambda\\) – we fit the model on the training set, then use the result to predict the test set. We then use whichever model does best in the test set.\nExample: Million Song data\n&gt; library(glmnet)\n&gt; glmOut &lt;- cv.glmnet(x=as.matrix(s50[,-1]),y=s50$V1,alpha=0)\n&gt; glmOut\n    Lambda Index Measure     SE Nonzero\nmin 0.2474   100   89.44 0.9201      90\n1se 0.9987    85   90.28 0.9823      90\nThe \\(\\lambda\\) value that gave the smallest Mean Squared Prediction Error (MSE) as 0.2474. (Coincidentally, it was also the smallest value that the function tried; see glmOut$lambda.) A more conservative value of \\(\\lambda\\) was 0.9987, the largest \\(\\lambda\\) giving MSE within one standard error of the minimum; it’s conservative in the sense of being less likely to overfit; its MSE value, 90.28, was only slightly larger than the best one. In each case, all 90 predictors had nonzero coefficient estimates."
  },
  {
    "objectID": "Ch5c.html#sec-pgtn",
    "href": "Ch5c.html#sec-pgtn",
    "title": "9  Shrinkage Estimators",
    "section": "9.5 Modern View",
    "text": "9.5 Modern View\nThere are many ways to deal with multicollinearity, which we must remind the reader only concerns approximate linear dependence pf the columns of \\(A\\). These days, many analysts go further, using ridge for situations in which there is exact linear dependence.\nWe saw such a setting in Section 6.1. There we deliberately induced exact linear dependence by inclusion of both male and female dummy variables. Let’s apply ridge:\n\nlibrary(qeML) \ndata(svcensus) \nsvc &lt;- svcensus[,c(1,4:6)]  \nsvc$man &lt;- as.numeric(svc$gender == 'male') \nsvc$woman &lt;- as.numeric(svc$gender == 'female') \nsvc$gender &lt;- NULL \na &lt;- svc[,-2] \nlambda &lt;- 0.1 \na &lt;- as.matrix(a) \ntmp1 &lt;- solve(t(a) %*% a + lambda * diag(4)) \ntmp1 %*% t(a) %*% as.matrix(svc$wageinc) \n\n               [,1]\nage        496.6716\nwkswrkd   1372.7052\nman     -18677.8751\nwoman   -29378.3086\n\n\nThe results essentially are the same as what we obtained by having only one dummy, thus no linear dependence: Men still enjoy about an $11,000 advantage. But ridge allowed us to avoid deleting one of our dummies. Such deletion is easy in this case, but for large \\(p\\), say in the hundreds or even more, some analysts prefer the convenience of ridge."
  },
  {
    "objectID": "Ch5c.html#formalizing-the-notion-of-shrinkage",
    "href": "Ch5c.html#formalizing-the-notion-of-shrinkage",
    "title": "9  Shrinkage Estimators",
    "section": "9.6 Formalizing the Notion of Shrinkage",
    "text": "9.6 Formalizing the Notion of Shrinkage\nIn the early 1980s, the statistical world was shocked by research by James and Stein that found, in short that:\n\nSay \\(W\\) has \\(q\\)-dimensional normal distribution with mean vector \\(\\mu\\) and independent components having variance \\(\\sigma^2\\), each. We have a random sample of size \\(n\\), i.e. \\(n\\) independent observations on \\(W\\). Then if \\(q \\geq 3\\), in terms of Mean Squared Estimation Error, the best estimator of \\(\\mu\\) is NOT the sample mean \\(\\bar{W}\\). Instead, it’s\n\\[\n\\left (\n1 - \\frac{(q-2) \\sigma^2/n}{||\\bar{W}||^2}\n\\right )\n\\bar{W}\n\\]\n\nThe quantity within the parentheses is typically smaller than 1, giving us the shrinkage property. Note, though, that with larger \\(n\\), the amount of shrinkage is minor.\nIn the case of linear regression, shrinkage works there too, with \\(q\\) being the number of columns in the \\(A\\) matrix.\n\n9.6.1 Shrinkage through length penalization\nSay instead of minimizing Equation 5.1, we minimize\n\\[\n(S - Ab)'(S - Ab) + \\lambda ||b||^2\n\\tag{9.2}\\]\nThe larger \\(b\\) is, the harder it is to minimize the overall quantity Equation 9.2. We say that we penalize large values of \\(b\\), an indirect way of pursuing shrinkage. Now take the derivative and set to 0:\n\\[\n0 = A'(S-Ab) + \\lambda b\n\\]\ni.e.\n\\[\n(A'A + \\lambda I) b = A'S\n\\]\nand thus\n\\[\n\\widehat{\\beta} = (A'A+\\lambda I)^{-1} A'S\n\\]\nIt’s ridge! So here is formalization of our “almost singular” etc. language above to justify ridge as a shrinkage estimator..\n\n\n9.6.2 Shrinkage through length limitation\nInstead of penalizing \\(||b||\\), we could simply constrain it, i.e. we could set our optimization problem to:\n\nminimize \\((S-Ab)'(S-Ab)\\), subject to the constraint \\(||b||^2 \\leq \\gamma\\)\n\nWe say that this new formulation is the dual of the first one. One can show that they are typically equivalent."
  },
  {
    "objectID": "Ch5c.html#the-lasso",
    "href": "Ch5c.html#the-lasso",
    "title": "9  Shrinkage Estimators",
    "section": "9.7 The LASSO",
    "text": "9.7 The LASSO\nThe LASSO (Least Absolute Shrinkage and Selection Operator) was developed by Robert Tibshirani in 1996, following earlier work by Leo Breiman. It takes \\(\\widehat{\\beta}\\) to be the value of \\(b\\) that minimizes\n\\[\n(S - Ab)'(S - Ab) + \\lambda ||b||_1\n\\tag{9.3}\\]\nwhere the “l1 norm” is\n\\[\n||b|| = \\sum_{i=1}^p |b_i|\n\\]\nWe will write our original norm as \\(||b||_2\\).\nThis is a seemingly minor change, but with important implications. What Breiman and Tibshirani were trying to do was to obtain a sparse \\(\\widehat{\\beta}\\) , i.e. a solution with lots of 0s, thereby providing a method for predictor variable selection. This is important because so-called “parsimonious” prediction models are desirable.\n\n9.7.1 Properties\nTo that end, first note that it can be shown that, under some technical conditions, that the ridge solution minimizes\n\\[\n(S - Ab)'(S - Ab)\n\\]\nsubject to the constraint\n\\[\n||b||_2 \\leq \\gamma\n\\]\nwhile in the LASSO case the constraint is\n\\[\n||b||_1 \\leq \\gamma\n\\]\nAs with \\(\\lambda\\) in the original formulation, \\(\\gamma\\) is a positive number chosen by the analyst."
  },
  {
    "objectID": "Ch5c.html#ridge-vs.-lasso-for-dimension-reduction",
    "href": "Ch5c.html#ridge-vs.-lasso-for-dimension-reduction",
    "title": "9  Shrinkage Estimators",
    "section": "9.8 Ridge vs. LASSO for Dimension Reduction",
    "text": "9.8 Ridge vs. LASSO for Dimension Reduction\nToday’s large datasets being so common, we need a way to “cut things down to size,” i.e. dimension reduction, aimed at reducing the number of predictor variables. This is done both for the sake of simplicity and to over overfitting, in which fitting an overly complex model can reduce predictive power.\n\n9.8.1 Geometric view\nComparison between the ridge and LASSO concepts is often done via this graph depicting the LASSO setting:\n\n\n\nLASSO sparsity\n\n\nHere \\(p = 2\\), with \\(b = (b_1,b_2)'\\). The horizontal and vertical axes represent \\(b_1\\) and \\(b_2\\).\n\nThe constraint \\(||b||_1 \\leq \\gamma\\) then takes the form of a diamond, with corners at \\((\\gamma,0)\\), \\((0,\\gamma)\\), \\((-\\gamma,0)\\) and \\((0,-\\gamma)\\). The constraint \\(||b||_1 \\leq \\gamma\\) requires us to choose a point \\(b\\) somewhere in the diamond, including the boundary.\nThe concentric ellipses depict the values of \\(c(b) = (S - Ab)'(S - Ab)\\), as follows.\nConsider one particular value of \\(c(b)\\), say 1.68.\nMany different points \\(b\\) in the graph will have \\(c(b) = 1.68\\); in fact, the locus of all such points is an ellipse.\nThere is one ellipse for each possible value of \\(c(b)\\). So, there are infinitely many ellipses, though only two are shown here.\nLarger values of \\(c(b)\\) yield larger ellipses.\nOn the one hand, we want to choose a \\(b\\) for which \\(c(b)\\) – our total squared prediction error – is small, thus a smaller ellipse.\nBut on the other hand, we need at least one point on the ellipse to be in common with the diamond.\nThe solution is then a point \\(b\\) in which the ellipse just barely touches the diamond, respectively.\n\n\n\n9.8.2 Implication for dimension reduction.\nThe key point is that that “barely touching” point will be one of the four corners of the diamond, points at which either \\(b_1 = 0\\) or \\(b_2 = 0\\) – hence a sparse solution, meaning one in which many/most of the coefficients in the fitted model will be 0. This achieves the goal of dimension reduction.\nRidge will not produce a sparse solution. The diamond would now be a circle (not shown). The “barely touching point” will almost certainly will be at a place in which both \\(b_1\\) and \\(b_2\\) are nonzero. Hence no sparsity.\n\n\n9.8.3 Avoidance of overfitting without dimension reduction\nAs we’ve seen, both ridge and LASSO reduce the size of the \\(\\widehat{\\beta}\\) vector of estimated coefficients. Smaller quantities have smaller statistical variances, hence a guard against overfitting. So, even ridge can be employed as an approach to the overfitting problem, even though it does not provide a sparse solution.\nOn the other hand, in some settings, it may be desirable to keep all predictors, as seen in the next section."
  },
  {
    "objectID": "Ch5c.html#sec-useall",
    "href": "Ch5c.html#sec-useall",
    "title": "9  Shrinkage Estimators",
    "section": "9.9 Example: NYC Taxi Data",
    "text": "9.9 Example: NYC Taxi Data\nThe purpose of this data is to predict trip time in the New York City taxi system. The \\(qeML\\) package includes a 10,000-row subset.\n\nlibrary(qeML)\ndata(nyctaxi)\nhead(nyctaxi)\n\n        trip_distance PULocationID DOLocationID tripTime DayOfWeek\n2969561          1.37          236           43      598         1\n7301968          0.71          238          238      224         4\n3556729          2.80          100          263      761         3\n7309631          2.62          161          249      888         4\n3893911          1.20          236          163      648         5\n4108506          2.40          161          164      977         5\n\ndim(nyctaxi)\n\n[1] 10000     5\n\nlength(unique(nyctaxi$PULocationID))\n\n[1] 143\n\nlength(unique(nyctaxi$DOLocationID))\n\n[1] 205\n\n\nAn old rule of thumb says that if we have \\(p\\) predictors and \\(n\\) data points, we should keep \\(p &lt; \\sqrt{n}\\) to avoid overfitting. As we will see in a later chapter, these days that rule is being questioned, but let’s see how it fits in here. In our case here, \\(p = 1+143+205+1 = 350\\), much larger than \\(p = \\sqrt{10000} = 100\\). And if we are fitting a linear model, we may consider pickup/dropoff location interaction variables, basically products of the pickup and dropoff dummy variables. To avoid exact multicollinearity, we will use only 142 and 204 of the pickup and dropoff variables.\nThus we either should delete some of the pickup and dropoff variables, or use all of them but temper the fit using ridge. The latter may be more attractive, as riders would like a time estimate for their particular pickup and dropoff locations.\n\nlibrary(glmnet)\nnycwide &lt;- factorsToDummies(nyctaxi[,-1])\nglmOut &lt;- cv.glmnet(x=nycwide,y=nyctaxi[,1],alpha=0)\nglmOut\n\n\nCall:  cv.glmnet(x = nycwide, y = nyctaxi[, 1], alpha = 0) \n\nMeasure: Mean-Squared Error \n\n    Lambda Index Measure      SE Nonzero\nmin 0.3002   100   2.935 0.06377     356\n1se 0.6320    92   2.984 0.07098     356"
  },
  {
    "objectID": "Ch5c.html#a-warning",
    "href": "Ch5c.html#a-warning",
    "title": "9  Shrinkage Estimators",
    "section": "9.10 A Warning",
    "text": "9.10 A Warning\nMany statistical quantities now have regularized, i.e. shrunken versions. It is also standard practice in neural networks. This may be quite helpful in prediction contexts. However, note the following:\n\n\n\n\n\n\nNo Statistical Inference on Shrinkage Estimators\n\n\n\nShrinkage produces a bias, of unknown size. Thus classical statistical inference (confidence intervals, hypothesis tests), e.g. those based on Equation 5.7 for linear models, is not possible."
  },
  {
    "objectID": "Ch5c.html#your-turn",
    "href": "Ch5c.html#your-turn",
    "title": "9  Shrinkage Estimators",
    "section": "9.11 Your Turn",
    "text": "9.11 Your Turn\n❄️ Your Turn: Show that \\(A_{new}\\) in Section 9.3.2 is of full rank, \\(p\\).\n❄️ Your Turn: Consider a generalization of ridge regression, in which we find\n\\[\n\\textrm{argmin}_b ~\n||{S} - {A} b||^2 + ||{D} b||^2\n\\]\nfor a diagonal matrix \\(D\\). The idea is to allow different shrinkage parameters for different predictor variables. Show that\n\\[\nb =\n{[{A}' {A} + {D}^2]}^{-1}\n{A}' {S}\n\\]\n❄️ Your Turn: In Section 9.9, it was pointed out that in some settings we may prefer to retain all of our predictor variables, rather than do dimension reduction, thus preferring ridge to LASSO. But we might pay a price for that preference, in that the LASSO may actually give us better predictive power. Write an R function to investigate this, with call form\ncompareRidgeLASSO(data,yName)\nwhere data and yName are in the format of the predictive \\(qeML\\) functions. Return the minimum Mean Squared Prediction Error for both algorithms."
  },
  {
    "objectID": "Ch6a.html#example-african-soils-data",
    "href": "Ch6a.html#example-african-soils-data",
    "title": "10  Eigenanalysis",
    "section": "10.1 Example: African Soils Data",
    "text": "10.1 Example: African Soils Data\nTo get things started, let’s consider the African Soils dataset.\nLet’s take a look around:\n\nlibrary(WackyData) \ndata(AfricanSoil)\ndim(AfricanSoil)\n\n[1] 1157 3600\n\nnames(AfricanSoil)[1:25]\n\n [1] \"PIDN\"     \"m7497.96\" \"m7496.04\" \"m7494.11\" \"m7492.18\" \"m7490.25\"\n [7] \"m7488.32\" \"m7486.39\" \"m7484.46\" \"m7482.54\" \"m7480.61\" \"m7478.68\"\n[13] \"m7476.75\" \"m7474.82\" \"m7472.89\" \"m7470.97\" \"m7469.04\" \"m7467.11\"\n[19] \"m7465.18\" \"m7463.25\" \"m7461.32\" \"m7459.39\" \"m7457.47\" \"m7455.54\"\n[25] \"m7453.61\"\n\nnames(AfricanSoil)[3576:3600]\n\n [1] \"m605.545\" \"m603.617\" \"m601.688\" \"m599.76\"  \"BSAN\"     \"BSAS\"    \n [7] \"BSAV\"     \"CTI\"      \"ELEV\"     \"EVI\"      \"LSTD\"     \"LSTN\"    \n[13] \"REF1\"     \"REF2\"     \"REF3\"     \"REF7\"     \"RELI\"     \"TMAP\"    \n[19] \"TMFI\"     \"Depth\"    \"Ca\"       \"P\"        \"pH\"       \"SOC\"     \n[25] \"Sand\"    \n\n\nLet’s try predicting pH, the acidity. But that leaves 3599 possible predictors. There is an old rule of thumb that one should have \\(p &lt; \\sqrt{n}\\), for \\(p\\) predictors and \\(n\\) data points, to avoid overfitting, a rule which in our setting of \\(n = 1157\\) is grossly violated. We need to do dimension reduction. One way to accomplish this is to use PCA.\n(Note, though, that this presumes our goal is prediction, rather than effect estimation. In the latter case, our new variables will be principal components, and their regression coefficients may not be meaningful to us. If we are doing prediction, regression coefficients may not be of interest.)"
  },
  {
    "objectID": "Ch6a.html#overall-idea",
    "href": "Ch6a.html#overall-idea",
    "title": "10  Eigenanalysis",
    "section": "10.2 Overall Idea",
    "text": "10.2 Overall Idea\nThe goal is to find a few important linear combinations of our original predictor variables–important in the sense that they roughly summarize our data. These new variables are called the principal components (PCs) of the data. Let’s see how this is done.\n\n10.2.1 The first PC\nLet X denote a set of variables of interest (not necessarily in a prediction context). In searching for good linear combinations, we want to aim for ones with high variance. We certainly don’t want ones with low variance; after all, a random variable with 0 variance is a constant. So we wish to find linear combinations\n\\[\nXu\n\\]\nwhich maximize\n\\[\nVar(Xu) = u' Cov(X) u\n\\]\nwhere we have invoked Equation 4.4.\nBut that goal is ill-defined, since we could take larger and larger vectors \\(u\\), thus larger and larger vectors \\(Xu\\), no maximum. So, let’s constrain it to vectors \\(u\\) of length 1:\n\\[\nu'u = 1\n\\]\nThe method of Lagrange multipliers is used to solve maximum/minimum problems that have constraints, by adding a new variable corresponding to the constraint. In our case, we maximize\n\\[\nu'Cov(X)u + \\gamma (u'u - 1)\n\\]\nwith respect to \\(u\\) and \\(\\gamma\\).\nSetting derivatives to 0, we have\n\\[\n0 =\n2 Cov(X) u + 2 \\gamma u\n\\]\nand\n\\[\n0 = u'u - 1\n\\]\nIn other words,\n\\[\nCov(X) ~ u = -\\gamma u\n\\tag{10.1}\\]\nA matrix times a vector equals a constant times that same vector. That opens a huge world. We will return to the notion of principal components in the next chapter, after laying the groundwork in the current chapter."
  },
  {
    "objectID": "Ch6a.html#definition",
    "href": "Ch6a.html#definition",
    "title": "10  Eigenanalysis",
    "section": "10.3 Definition",
    "text": "10.3 Definition\nThe concept itself is simple:\n\nConsider an \\(m \\textrm{ x } m\\) matrix \\(M\\). If there is a nonzero vector \\(x\\) and a number \\(\\lambda\\) such that\n\\[\nMx = \\lambda x\n\\tag{10.2}\\]\nwe say that \\(\\lambda\\) is an eigenvalue of \\(M\\), with eigenvector \\(x\\)."
  },
  {
    "objectID": "Ch6a.html#a-first-look",
    "href": "Ch6a.html#a-first-look",
    "title": "10  Eigenanalysis",
    "section": "10.4 A First Look",
    "text": "10.4 A First Look\nHere are a few properties to start with:\n\nThe definition is equivalent to\n\\[\n(M - \\lambda I) x = 0\n\\]\nwhich in turn implies that \\(M - \\lambda I\\) is noninvertible. That then implies that\n\\[\ndet(M - \\lambda I) = 0\n\\]\nThe left-hand side of this equation is a polynomial in \\(\\lambda\\). So for an \\(n \\textrm{ x } n\\) matrix \\(M\\), there are \\(n\\) roots of the equation and thus \\(n\\) eigenvalues. Note that some roots may be repeated; if \\(M\\) is the zero matrix, it will have an eigenvalue \\(0\\) with multiplicity \\(n\\).\nIn principle, that means we can solve the above determinant equation to find the eigenvalues of the matrix. The comments in Section 3.5.1 regarding the “ugly” definition of determinants imply that the equation involves a degree-\\(m\\) polynomial in \\(\\lambda\\).\nThere are much better ways to calculate the eigenvalues than this, but a useful piece information arising from that analysis is that \\(M\\) as \\(m\\) eigenvalues (not necessarily distinct)."
  },
  {
    "objectID": "Ch6a.html#the-special-case-of-symmetric-matrices",
    "href": "Ch6a.html#the-special-case-of-symmetric-matrices",
    "title": "10  Eigenanalysis",
    "section": "10.5 The Special Case of Symmetric Matrices",
    "text": "10.5 The Special Case of Symmetric Matrices\nMany matrices in data science are symmetric, such as covariance matrices and \\(A'A\\) in Equation 5.4.\n\n10.5.1 Eigenvalues are real\nSome eigenvalues may be complex numbers, i.e. of the form \\(a+bi\\), but it can be shown that if \\(M\\) is symmetric, its eigenvalues are guaranteed to be real. This is good news for data science, as many matrices in that field are symmetric, such covariance matrices.\n\n\n10.5.2 Eigenvectors are orthogonal\n\nEigenvectors corresponding to distinct eigenvalues of a symmetric matrix \\(M\\) are othogonal.\nProof: Let \\(u\\) and \\(v\\) be such eigenvectors, corresponding to eigenvalues \\(\\mu\\) and \\(\\nu\\).\n\\[\n\\mu' M \\nu = (\\mu' M \\nu)' = \\nu' M' \\mu = \\nu' M \\mu\n\\]\n(in that first equality, we used the fact that the transpose of a number is that number). Substitute for \\(M\\nu\\) and \\(M\\mu\\), then subtract.\n\n\n\n10.5.3 Symmetric matrices are diagonalizable\nA square matrix \\(R\\) is said to be diagonalizable if there exists an invertible matrix \\(P\\) such that \\(P^{-1}RP\\) is equal to some diagonal matrix \\(D\\). We see that symmetric matrices fall into this category.\nOne application of this is the computation of powers of a diagonal matrix:\n\\[\nR^k = P^{-1}RP ~ P^{-1}RP ~ ... ~ P^{-1}RP = P^{-1}D^kP\n\\]\n\\(D^k\\) is equal to the diagonal matrix with elements \\(d_k^k\\), so the computation of \\(M^k\\) is easy.\n\nTheorem 10.1 Any symmetric matrix \\(M\\) has the following properties\n\n\\(M\\) is diagonalizable.\nIn fact, the matrix \\(P\\) is equal to the matrix whose columns are the eigenvectors of \\(M\\), chosen to have length 1.\nThe associated diagonal matrix has as its diagonal elements the eigenvalues of \\(M\\).\nThe matrix \\(P\\) has the property that \\(P^{-1} = P'\\).\nMoreover, the rank of \\(M\\) is equal to the number of nonzero eigenvalues of \\(M\\).\n\n\n\nProof. Use partitioned matrices.\nLet \\(D = diag(d_1,...,d_m)\\), where the \\(d_i\\) are the eigenvalues of \\(M\\), corresponding to eigenvectors \\(P_i\\). Set the latter to have length 1, by dividing by their lengths.\nRecall that the eigenvectors of \\(M\\), i.e. the columns of \\(P\\), are orthogonal. That means the rows of \\(P'\\) are orthogonal. Again using partitioning, we see that\n\\[\nP'P = I\n\\]\nso that \\(P^{-1} = P'\\).\nBy the way, any square matrix \\(Q\\) such that \\(Q'Q = I\\) is said to be orthogonal.\nNow write\n\\[\nMP = M(P_1 | ... | P_m) = (MP_1 | ... | MP_n) =\n(d_1 P_1 | ... | (d_m P_m) = PD\n\\]\nMultiply on the left by \\(P'\\), and we are done.\nRegarding rank, Theorem 6.1 tells us that pre- or postmultiplying by an invertible matrix does not change rank, and clearly the rank of a diagonal matrix is the number of nonzero elements.\n\\(\\square\\)\n\n\n\n10.5.4 Example: Census dataset\nLet’s illustrate all this with the data in Section 9.5. We will form the matrix \\(A'A\\) in Section 5.3.3, which as mentioned, is symmetric.\nRecall that in that example, \\(A\\) is not of full rank. Thus we should expect to see a 0 eigenvalue.\n\nlibrary(qeML)\ndata(svcensus)\nsvc &lt;- svcensus[,c(1,4:6)]\nsvc$man &lt;- as.numeric(svc$gender == 'male')\nsvc$woman &lt;- as.numeric(svc$gender == 'female')\nsvc$gender &lt;- NULL\na &lt;- as.matrix(svc[,-2])\na &lt;- cbind(1,a)  # add the column of 1s\nm &lt;- t(a) %*% a\neigs &lt;- eigen(m)\neigs\n\neigen() decomposition\n$values\n[1] 7.594762e+07 3.292955e+06 7.466971e+03 1.293594e+03 2.801489e-12\n\n$vectors\n             [,1]         [,2]         [,3]        [,4]          [,5]\n[1,] -0.015881850  0.004389585 -0.035995373  0.81553633  5.773503e-01\n[2,] -0.649660696  0.760031134  0.004461047 -0.01654550  1.561251e-17\n[3,] -0.759952974 -0.649864420  0.004991922 -0.01108122 -4.061024e-17\n[4,] -0.012070494  0.002130704 -0.724401141  0.37650952 -5.773503e-01\n[5,] -0.003811356  0.002258881  0.688405767  0.43902681 -5.773503e-01\n\nm %*% eigs$vectors[,1]\n\n               [,1]\n         -1206188.6\nage     -49340181.0\nwkswrkd -57716616.5\nman       -916725.2\nwoman     -289463.4\n\neigs$values[1] %*% eigs$vectors[,1]\n\n         [,1]      [,2]      [,3]      [,4]      [,5]\n[1,] -1206189 -49340181 -57716617 -916725.2 -289463.4\n\n\nYes, that first column is indeed an eigenvector, with the claimed eigenvalue.\nNote that the expected 0 eigenvalue shows up as 2.801489e-12, quite small but nonzero, due to roundoff error."
  },
  {
    "objectID": "Ch6a.html#application-detecting-multicollinearity",
    "href": "Ch6a.html#application-detecting-multicollinearity",
    "title": "10  Eigenanalysis",
    "section": "10.6 Application: Detecting Multicollinearity",
    "text": "10.6 Application: Detecting Multicollinearity\nConsider the basic eigenanalysis equation,\n\\[\nAx = \\lambda x\n\\]\nfor a square matrix \\(A\\), a conformable vector \\(x\\) and a scalar \\(\\lambda\\). Suppose that, roughly speaking, \\(\\lambda x\\) is small relative to \\(A\\). Then\n\\[\nAx \\approx 0\n\\]\nand since \\(Ax\\) is a linear combination of the columns of \\(A\\), we thus we have found multicollinearity in \\(A\\).\nOne often sees use of the condition number of a matrix, which is the ratio of the largest eigenvalue to the smallest one. This too might be used as a suggestion of multicollinearity, though the main usage is as a signal that matrix operations such finding inverses may have significant problems with roundoff error."
  },
  {
    "objectID": "Ch6a.html#example-currency-data",
    "href": "Ch6a.html#example-currency-data",
    "title": "10  Eigenanalysis",
    "section": "10.7 Example: Currency Data",
    "text": "10.7 Example: Currency Data\nThis dataset tracks five pre-euro European currencies.\n\nlibrary(qeML)\ndata(currency)\nhead(currency)\n\n  Can..dollar Ger..mark Fr..franc UK.pound J..yen\n1          19       580     4.763       29    602\n2          18       609     4.818       44    609\n3          20       618     4.806       66    613\n4          46       635     4.825       79    607\n5          42       631     4.796       77    611\n6          45       635     4.818       74    610\n\ndim(currency)\n\n[1] 762   5\n\ncrc &lt;- currency\ncrc &lt;- as.matrix(crc)\ncrcapa &lt;- t(crc) %*% crc\neigs &lt;- eigen(crcapa)\neigs\n\neigen() decomposition\n$values\n[1] 4.180990e+08 5.356321e+07 8.199388e+06 4.686379e+06 4.764149e+02\n\n$vectors\n             [,1]         [,2]         [,3]          [,4]          [,5]\n[1,] -0.464701846 -0.589664107  0.573442227  0.3277874495 -0.0082362154\n[2,] -0.548123722  0.364936898 -0.424252867  0.6216029591 -0.0008432535\n[3,] -0.008118519 -0.002208129  0.005818183 -0.0005349635  0.9999475368\n[4,] -0.541352395 -0.364398075 -0.476833987 -0.5888747450 -0.0027404807\n[5,] -0.436445017  0.621551662  0.513584476 -0.3992385223 -0.0053728096\n\n# illustrate Ax = lambda x\ncrcapa %*% eigs$vectors[,5]\n\n                   [,1]\nCan..dollar  -3.9238561\nGer..mark    -0.4017386\nFr..franc   476.3899505\nUK.pound     -1.3056059\nJ..yen       -2.5596868\n\neigs$values[5] *  eigs$vectors[,5] \n\n[1]  -3.9238561  -0.4017386 476.3899505  -1.3056060  -2.5596868\n\n# is Ax small?\nhead(crcapa)\n\n            Can..dollar Ger..mark  Fr..franc  UK.pound    J..yen\nCan..dollar   112111475  93929522 1673631.29 113542760  66967756\nGer..mark      93929522 136033582 1795560.29 116882053 109220119\nFr..franc       1673631   1795560   28573.49   1859363   1433430\nUK.pound      113542760 116882053 1859363.20 133130962  85746625\nJ..yen         66967756 109220119 1433430.43  85746625 103243878\n\n\nSo yes, this dataset has some multicollinearity."
  },
  {
    "objectID": "Ch6a.html#computation-the-power-method",
    "href": "Ch6a.html#computation-the-power-method",
    "title": "10  Eigenanalysis",
    "section": "10.8 Computation: the Power Method",
    "text": "10.8 Computation: the Power Method\nOne way to compute eigenvalues and eigenvectors is the power method, a simple iteration. We begin with an initial guess, \\(x_0\\) for an eigenvector. Substituting in Equation 10.2, we have the next guess:\n\\[\nx_1 = M x_0\n\\]\nWe keep iterating until convergence, generating \\(x_2\\) from \\(x_1\\) and so on. However, the \\(x_i\\) may grow, so we normalize to length 1:\n\\[\nx_i \\leftarrow \\frac{x_i}{||x_i||}\n\\]"
  },
  {
    "objectID": "Ch6a.html#application-computation-of-long-run-distribution-in-markov-chains",
    "href": "Ch6a.html#application-computation-of-long-run-distribution-in-markov-chains",
    "title": "10  Eigenanalysis",
    "section": "10.9 Application: Computation of Long-Run Distribution in Markov Chains",
    "text": "10.9 Application: Computation of Long-Run Distribution in Markov Chains\nWe showed in Section 3.1 how matrix inverse can be used to compute the long-run distribution \\(\\nu\\) in a Markov chain. However, this is inefficient for very large transition matrices. For instance, in Google PageRank, there is a Markov state for every page on the Web!\nInstead, we exploit the fact that Equation 3.1 says that the transition matrix has an eigenvector \\(\\nu\\) with eigenvalue 1. Due to the typical huge size of the matrix, the power method or a variant is often used."
  },
  {
    "objectID": "Ch6a.html#your-turn",
    "href": "Ch6a.html#your-turn",
    "title": "10  Eigenanalysis",
    "section": "10.10 Your Turn",
    "text": "10.10 Your Turn\n❄️ Your Turn: Show that the diagonalizing matrix \\(P\\) above must have determinant \\(\\pm 1\\).\n❄️ Your Turn: Show that if \\(x\\) is an eigenvector of \\(M\\) with eigenvalue \\(\\lambda \\neq 0\\), then for any nonzero number \\(c\\), \\(cx\\) will also be an eigenvector with eigenvalue \\(\\lambda\\).\n❄️ Your Turn: Show that if a matrix \\(M\\) has a 0 eigenvalue, \\(M\\) must be singular. Also prove the converse. (Hint: Consider the column rank.)\n❄️ Your Turn: Consider a projection matrix \\(P_W\\). Show that the only possible eigenvalues are 0 and 1. Hint: Recall that projection matrices are idempotent.\n❄️ Your Turn: Say \\(A\\) is an invertible matrix with eigenvalue \\(\\lambda\\) and eigenvector \\(v\\). Show that \\(v\\) is also an eigenvector of \\(A^{-1}\\), with eigenvalue \\(1/\\lambda\\).\n❄️ Your Turn: Show that if \\(A\\) is nonnegative-definite, its eigenvalues must be nonnegative."
  },
  {
    "objectID": "Ch6b.html#properties",
    "href": "Ch6b.html#properties",
    "title": "11  Principal Components",
    "section": "11.1 Properties",
    "text": "11.1 Properties\nThe full set of eigenvectors is then known as the principal components (PCs). They have some further important properties:\n\n\nThe PCs are orthogonal to each other. This follows from our earlier finding that eigenvectors of a symmetric matrix, in this case \\(Cov(X)\\), are orthogonal. We say that a symmetric is orthogonal if its columns are orthogonal as vectors.\nThis in turn implies that the PCs are uncorrelated, and in the multivariate normal case, statistically independent.\nThe eigenvalues are the variances of the PCs. The PCs are in order of decreasing (or at least nonincreasing) variance.\nThis first statement follows from pre-mutiplying Equation 10.1 by \\(u'\\), giving us \\(Var(u)\\) on the left and \\(-\\lambda u'u = -\\lambda\\) on the right. The second statement follows from the fact that as we go from the first PC to the second, third and so on, we are maximizing under more and more constraints, hence smaller maxima.\n\n\nBy the way, though we used the covariance matrix here, it is also common to simply do eigenanalysis on the original data matrix."
  },
  {
    "objectID": "Ch6b.html#eigenanalysis-relation-between-a-and-covx-in-linear-regression",
    "href": "Ch6b.html#eigenanalysis-relation-between-a-and-covx-in-linear-regression",
    "title": "11  Principal Components",
    "section": "11.2 Eigenanalysis Relation between \\(A\\) and \\(Cov(X)\\) in Linear Regression",
    "text": "11.2 Eigenanalysis Relation between \\(A\\) and \\(Cov(X)\\) in Linear Regression\nConsider our usual linear regression setting, in which the matrix \\(A\\) contains our data for our predictor variables \\(X\\). If we center and scale our data, then ’\n\\[\nCov(X) = A'A\n\\]\nNow say the PCA of X has a 0 eigenvalue. That implies that some linear combination \\(u'X\\) has 0 variance. Then\n\\[\n0 = Var(u'X) = u' Cov(X) u = u' A'A u = (Au)'Au\n\\]\nThus\n\\[\nAu = 0\n\\]\nSince \\(Au\\) is a linear combination of the columns of \\(A\\), we see that the columns of \\(A\\) are linearly dependent.\nThe same argument works in reverse. If \\(Au = 0\\), we fine that \\(Var(u'X) = 0\\), and since the eigenvalues of \\(Cov(X)\\) are variances of linear combinations of \\(X\\), we see that one of those variances is 0."
  },
  {
    "objectID": "Ch6b.html#back-to-the-example",
    "href": "Ch6b.html#back-to-the-example",
    "title": "11  Principal Components",
    "section": "11.3 Back to the Example",
    "text": "11.3 Back to the Example\nSo, we remove the ``Y’’ variable, \\(pH\\), number 3598 as seen above, as proceed. We will also remove the nonnumeric columns, PIDN and Depth. We could use R’s prcomp function here, but to better illustrate the concepts, we do things from scratch.\n\nlibrary(WackyData)\ndata(AfricanSoil)\nx &lt;- AfricanSoil[,-c(1,3595,3598)]\ndim(x)\n\n[1] 1157 3597\n\nxcov &lt;- cov(x)\neigs &lt;- eigen(xcov)\neigs$values[1:100]  # don't print out all 3597!\n\n  [1] 7.600042e+01 9.553189e+00 6.979629e+00 4.209555e+00 2.884526e+00\n  [6] 2.175115e+00 1.954537e+00 1.321853e+00 9.375446e-01 6.453537e-01\n [11] 6.360017e-01 5.484400e-01 4.535480e-01 3.981618e-01 3.665122e-01\n [16] 3.496067e-01 2.597679e-01 2.035621e-01 1.498785e-01 1.248452e-01\n [21] 1.119365e-01 9.545515e-02 8.083867e-02 6.415968e-02 6.079547e-02\n [26] 5.737305e-02 4.468220e-02 3.902647e-02 3.641190e-02 3.056959e-02\n [31] 2.541331e-02 2.037622e-02 1.819728e-02 1.532509e-02 1.297067e-02\n [36] 1.242883e-02 1.073280e-02 9.420125e-03 8.687909e-03 7.980043e-03\n [41] 7.178366e-03 5.445802e-03 4.893268e-03 4.027980e-03 3.903172e-03\n [46] 3.526363e-03 3.341049e-03 3.062258e-03 2.847189e-03 2.723814e-03\n [51] 2.586813e-03 2.315871e-03 2.142482e-03 1.977798e-03 1.771873e-03\n [56] 1.506873e-03 1.474780e-03 1.302539e-03 1.173394e-03 1.141518e-03\n [61] 1.004309e-03 9.438253e-04 8.794233e-04 8.217583e-04 8.137159e-04\n [66] 7.269301e-04 7.187134e-04 6.454513e-04 6.261121e-04 5.256722e-04\n [71] 4.603372e-04 4.256952e-04 4.116083e-04 3.873680e-04 3.802244e-04\n [76] 3.421665e-04 3.144433e-04 3.013745e-04 2.650906e-04 2.561422e-04\n [81] 2.553842e-04 2.436702e-04 2.110897e-04 1.937904e-04 1.851444e-04\n [86] 1.766814e-04 1.569518e-04 1.484314e-04 1.437515e-04 1.263429e-04\n [91] 1.242947e-04 1.154861e-04 1.135908e-04 1.113247e-04 1.003160e-04\n [96] 9.908341e-05 9.180042e-05 8.670191e-05 8.415325e-05 7.907993e-05\n\n\nAh, the eigenvalues fall off rapidly after the first few. So, let’s say we decide to use the first 9 \\(u\\) vectors.\n\neigvecs &lt;- eigs$vectors[,1:9]\ndim(eigvecs)\n\n[1] 3597    9\n\n\nSo, recalling the sequence of equations leading to Equation 10.1, we are ready to replace our old variables by the new:\n\nx &lt;- as.matrix(x)\ndim(x)\n\n[1] 1157 3597\n\nxnew &lt;- x %*% eigvecs\ndim(xnew)  \n\n[1] 1157    9\n\nph &lt;- AfricanSoil[,3595]\nlmout &lt;- lm(AfricanSoil$pH ~ xnew)\n# and so on"
  },
  {
    "objectID": "Ch6b.html#pca-in-detection-of-multicollinearity",
    "href": "Ch6b.html#pca-in-detection-of-multicollinearity",
    "title": "11  Principal Components",
    "section": "11.4 PCA in Detection of Multicollinearity",
    "text": "11.4 PCA in Detection of Multicollinearity\nRecall the itemized list in Section 9.1 of various conditions under which we have multicollinearity. We can add the following item to that list:\n\n\\(A\\) has an eigenvalue that is nearly 0.\n\nThe reasons are straightforward. If \\(Ax = 0\\) for some nonzero $x, then the material in ?sec-partitioned on partitioning says that the coefficients in \\(x\\) form a linear combination of the columns of \\(A\\) that results in the 0 vector, i.e. \\(A\\) is not of full rank.\nIn fact, PCA can warn us of specific linear combinations of \\(Cov(A'A)\\) that are nearly 0, as follows.\n\nSay a PC corresponds to a small eigenvalue.\nThen from Section 10.2.1 that linear combination has small variance.\nRandom variables with small variance are nearly constant.\nThus with high probability, the linear combination is near to \\(c\\) for some number \\(c\\).\nIf our design matrix \\(A\\) includes a 1s column, then the above linear combination, together with this 1s column, gives us a linear combination that is usually close to the 0 vector, thus multicollinear.\n\nIn other words, PCA can point out to us specific linear combinations of our variables that may be problematic. If we are considering solving the problem by removing one or more predictor variables, this analysis could suggest which ones to remove."
  },
  {
    "objectID": "Ch6b.html#how-many-principal-components-should-we-use",
    "href": "Ch6b.html#how-many-principal-components-should-we-use",
    "title": "11  Principal Components",
    "section": "11.5 How Many Principal Components Should We Use?",
    "text": "11.5 How Many Principal Components Should We Use?\nWe chose to use the first 9 principal components in our example here, but just for illustration purposes. There are no formal rules for how many to use. Many “rules of thumb” exist.\nIf we are doing prediction, there is a very natural way to choose our number of PCs–do cross-validation (?sec-lassoxval), and use whichever number gives the most accurate prediction.\n\n11.5.1 Example: New York City taxi trips\nThis is a well-known dataset, a version of which is included in the qeML package. The object is to predict trip time, given pickup and dropoff locations, trip distance and day of the week.\nIn the raw form of the data, there are just 5 columns, thus 4 predictors. But the pickup and dropoff locations are R factors, coding numerous locations. These must be decoded to dummy variables (values 1 or 0, coding whether or not, say, a given trip began at pickup location 121. If for instance one calls the R linear regression function lm, that function will do the conversion internally, but in our case we will perform the conversion ourselves, using regtools::factorsToDummies. (The regtools package is included by qeML.)\n\nlibrary(qeML)\ndata(nyctaxi)\ndim(nyctaxi)\n\n[1] 10000     5\n\nnyc &lt;- factorsToDummies(nyctaxi,dfOut=T)\ndim(nyc)\n\n[1] 10000   357\n\n\nWow! That’s quite a lot of predictors, and well in excess of the common rule of thumb that one should have no more than \\(\\sqrt{n}\\) predictors, 100 in this case.\nSo, we might try dimension reduction via PCA, then use the PCs as predictors. The function qeML::qePCA combines these two operations. Let’s see how it works.\n\nargs(qePCA)\n\nfunction (data, yName, qeName, opts = NULL, pcaProp, holdout = floor(min(1000, \n    0.1 * nrow(data)))) \nNULL\n\n\nHere qeName indicates which function is desired for prediction, e.g. qeLin for a linear model. (This function wraps lm.) But a key argument here is pcaProp. Recall that:\n\nThe PCs come in order of decreasing variance.\nWe are mainly interested in the first few PCs. The later ones have small variance, which makes them approximately constant and thus of no use to us.\n\nThe name ‘pcaProp’ stands for “proportion of total variance.” If we set this to, say, 0.25, we are saying “Give us whatever number of the first few PCs that have a total variance of at least 25% of the total.” Let’s give that a try:\nqePCA(nyc,'tripTime','qeLin',pcaProp=0.25)$testAcc\n[1] 311.9159\nWe asked to predict the column ‘tripTime’ in the dataset nyc using the qeLin function, based on as many of the PCs that will give us 25% of the total variance. As seen above, most of the qeML prediction functions split the data into a training set and a holdout set. The model is fit to the training set, and then applied to prediction of the holdout set. The output value is the mean absolute prediction error (MAPE).\nHowever, since the holdout set is randomly generated, the MAPE value is random, so we should do multiple runs. Some experimentation showed that MAPE here is highly variable, so we decided to perform 500 runs, e.g.\nmean(sapply(1:500,function(i) qePCA(nyc,'tripTime','qeLin',pcaProp=0.1)$testAcc)) \n[1] 359.663\n\nMean Absolute Predictive Error\n\n\npcaProb\nMAPE\n\n\n\n\n0.1\n359.6630\n\n\n0.2\n359.3068\n\n\n0.3\n403.9878\n\n\n0.4\n397.3783\n\n\n0.5\n430.8958\n\n\n0.6\n423.7299\n\n\n0.7\n517.7917\n\n\n0.8\n969.1304\n\n\n0.9\n2275.091\n\n\n\nAmong other things, this shows the dangers of overfitting, in this case using too many PCs in our linear regression model. It seems best here to use only the first 10 or 20% of the PCs."
  },
  {
    "objectID": "Ch6b.html#your-turn",
    "href": "Ch6b.html#your-turn",
    "title": "11  Principal Components",
    "section": "11.6 Your Turn",
    "text": "11.6 Your Turn\n❄️ Your Turn: Modify the code for qePCA for the case of linear regression by addig a component xCoeffs to its return value. This will give the regression coefficients in terms of the original X predictors.\n❄️ Your Turn: Say the symmetric matrix \\(A\\) has block diagonal form\n\\[\nA =\n\\left (\n\\begin{array}{rrr}\nA_1 & 0 & 0 ... \\\\\n0 & A_2 & 0 ...  \\\\\n...\n\\end{array}\n\\right )\n\\]\nwhere \\(A_i\\) (\\(i=1,...,r)\\)\n\nis symmetric\nis of size \\(k_i \\textrm{ x } k_i\\)\nhas eigenvalues \\(\\gamma_{1},...,\\gamma_{k}\\)\nhas eigenvectors \\(u{1,j},...,u{k,j}\\), where \\(j=1,...,k_i\\)\n\nState the form of the eigenvalues and eigenvectors of \\(A\\)."
  },
  {
    "objectID": "Ch6c.html#basic-idea",
    "href": "Ch6c.html#basic-idea",
    "title": "12  Singular Value Decomposition",
    "section": "12.1 Basic Idea",
    "text": "12.1 Basic Idea\nHere is what we are aiming for.\n\n\n\n\n\n\nOur target relation:\n\n\n\nGiven an \\(m \\times n\\) matrix \\(A\\), we wish to find orthogonal matrices \\(U\\) and \\(V\\), and a matrix \\(\\Sigma = diag(\\sigma_1,...,\\sigma_n)\\) whose nonzero elements are positive, such that\n\\[\nA = U \\Sigma V'\n\\tag{12.1}\\]\nBy convention the ordering of columns is set so that the \\(\\sigma_i\\) occur in nonincreasing order.\nNote that, by matrix partitioning, Equation 12.1 also says that\n\\[\nA = \\sum_{i=1}^n \\sigma_i u_i v_i'\n\\tag{12.2}\\]\n\n\nNote the dimensions:\n\n\\(U\\) is \\(m \\times n\\)\n\\(V\\) is \\(n \\times n\\)\n\\(\\Sigma\\) is \\(n \\times n\\)\n\nLet \\(r\\) denote the rank of \\(A\\). Recall that \\(r \\leq \\min(m,n)\\). It will turn out that \\(\\sigma_i = 0\\) for \\(i=r+1,...,n\\)."
  },
  {
    "objectID": "Ch6c.html#solution",
    "href": "Ch6c.html#solution",
    "title": "12  Singular Value Decomposition",
    "section": "12.2 Solution",
    "text": "12.2 Solution\nThere are various derivations, e.g. along the lines of Section 10.2.1 (one maximizes the quantity \\(U A V'\\)), but let’s go quickly to the answer:\n\nCompute the eigenvalues \\(\\sigma_1,...,\\sigma_n\\) and eigenvectors \\(q_1,...,q_n\\) of \\(A'A\\).\nSince \\(A'A\\) is symmetric and positive-semidefinite, its eigenvalues will be nonnegative and its length-1 eigenvectors will be orthogonal.\nSet \\(\\Sigma\\) to \\(diag(\\sqrt{\\sigma_1},...,\\sqrt{\\sigma_n})\\), the nonincreasing list of eigenvalues. Set the columns of \\(V\\) to the corresponding eigenvectors (recall Section 10.5.3).\nSet\n\n\\[\nu_i = \\frac{1}{\\sigma_i} Av_i, ~ i=1,...,r\n\\]\nUsing the Gram-Schmidt Method (Section 8.9.3), we can compute (if \\(r &lt; m\\) necessitates it) vectors \\(u_{r+1},...u_m\\) so that \\(u_1,...,u_m\\) is an orthonormal basis for \\(\\mathcal{R}^m\\). Set \\(U\\), decribed in partitioning terms: to\n\\[\nU = (u_1|...|u_m)\n\\]"
  },
  {
    "objectID": "Ch6c.html#checking-the-formula",
    "href": "Ch6c.html#checking-the-formula",
    "title": "12  Singular Value Decomposition",
    "section": "12.3 Checking the Formula",
    "text": "12.3 Checking the Formula\nAre \\(U\\) and \\(V\\) really orthogonal, and does \\(U \\Sigma V'\\) really work out to be \\(A\\)?\n\n\\(U\\) is orthogonal.\nThis follows immediately from the fact that the columns of \\(U\\) are an orthonormal basis from \\(\\mathcal{R}^m\\).\n\\(V\\) is orthogonal, by Section 10.5.2.\nThe product evaluates to \\(A\\), as claimed.\nUsing matrix partitioning, we have\n\\[\n  U \\Sigma = (\\sigma_1 u_1 | ... | \\sigma_n u_n)\n  \\]\nSo, again using partitioning,\n\\[\n  U \\Sigma V' = (\\sigma_1 u_1 | ... | \\sigma_n u_n)\n   \\left (\n   \\begin{array}{r}\n   v_1' \\\\\n   ... \\\\\n   v_n'  \\\\\n   \\end{array}\n   \\right )\n  = \\sum_{i=1}^n \\sigma_i u_i v_i'  \n  \\]\nThat last expression is equal to \\(A\\), by Equation 12.2."
  },
  {
    "objectID": "Ch6c.html#uniqueness",
    "href": "Ch6c.html#uniqueness",
    "title": "12  Singular Value Decomposition",
    "section": "12.4 Uniqueness",
    "text": "12.4 Uniqueness\nThe SVD can be shown to be unique, if there are no repeated eigenvalues. If the latter exist, one can permute the corresponding columns of \\(U\\) and \\(V\\), but otherwise uniqueness holds."
  },
  {
    "objectID": "Ch6c.html#the-fundamental-theorem-of-linear-algebra",
    "href": "Ch6c.html#the-fundamental-theorem-of-linear-algebra",
    "title": "12  Singular Value Decomposition",
    "section": "12.5 The Fundamental Theorem of Linear Algebra",
    "text": "12.5 The Fundamental Theorem of Linear Algebra\nOne can define four fundamental subspaces for any matrix \\(A\\). We will need the following:\n\n\\(\\textrm{row space}(A), \\mathcal{R}(A)\\): \\(\\{x'A\\}~~\\) (all linear combinations of rows of \\(A\\))\n\\(\\textrm{column space}(A), \\mathcal{C}(A)\\): \\(\\{Ax\\}~~\\) (all linear combinations of columns of \\(A\\))\n\\(\\textrm{null space}(A), \\mathcal{N}(A)\\): \\(\\{x: Ax = 0\\}\\)\n\\(\\textrm{left null space}(A) = \\mathcal{N}(A')\\): \\(\\{x: x'A = 0\\}\\)\n\nThese subspaces have various properties, which may be verified via SVD. Equation 12.2 will come in very handy.\nConsider the column space first. From Equation 12.2 write\n\\[\nAx = \\sum_{i=1}^n \\sigma_i u_i v_i' x\n\\tag{12.3}\\]\nNote that \\(v_i'x\\) is a scalar, so we have\n\\[\n\\begin{aligned}\nAx &= \\sum_{i=1}^n [\\sigma_i (v_i' x)] u_i \\\\\n&= \\sum_{i=1}^r [\\sigma_i (v_i' x)] u_i\n\\end{aligned}\n\\tag{12.4}\\]\nSo\n\\[\n\\mathcal{C}(A) \\subset \\textrm{ span}(u_1,...,u_r)\n\\tag{12.5}\\]\nNow, is the left side equal to all of the right side? Say a member of the right side is \\(c_1 u_1+...+c_r u_r\\). Then, using the fact that \\(\\{v_1....,v_r\\}\\) is an orthonormal set, take \\(x\\) to be\n\\[\nx = \\sum_{j=1}^r \\frac{c_j}{\\sigma_j} v_j\n\\]\nThen for each \\(i\\), we have\n\\[\n\\sigma_i v_i'x =c_i\n\\]\nand\n\\[\nAx = \\sum_{i=1}^r c_i u_i\n\\]\nas desired, so Equation 12.5 is actually an equality.\nWe have thus characterized one of the four fundamental subspaces:\n\n\\(\\mathcal{C}(A)\\) has dimension \\(r\\), with basis \\(u_1,...,u_r\\).\n\nNow look at the null space. \\(Ax = 0\\) means that the vector \\(x\\) is orthogonal to each row of \\(A\\). Thus\n\\[\n\\begin{aligned}\n\\mathcal{N}(A) &= \\mathcal{R}(A)^{\\perp} \\\\\n&= \\mathcal{C}(A')^{\\perp}\n\\end{aligned}\n\\]\nWe already found above a characterization of the column space of a matrix, which we now apply to \\(A'\\): \\(\\mathcal{C}(A')\\) has dimension \\(r\\), with basis \\(v_1,...,v_r\\). Applying \\(\\perp\\), and noting that orthogonality of \\(v_1,...,v_n\\), we can now characterize a second, and even a third, of the four subspaces:\n\n\\(\\mathcal{N}(A)\\) is of dimension \\(n-r\\), with basis \\(v_{r+1},...,v_n\\).\n\\(\\mathcal{R}(A)\\) is of dimension \\(r\\), with basis \\(v_1,...,v_r\\).\n\nWe leave the fourth subspace as a Your Turn problem."
  },
  {
    "objectID": "Ch6c.html#the-data-scientists-swiss-army-knife",
    "href": "Ch6c.html#the-data-scientists-swiss-army-knife",
    "title": "12  Singular Value Decomposition",
    "section": "12.6 The Data Scientist’s Swiss Army Knife",
    "text": "12.6 The Data Scientist’s Swiss Army Knife\nThere are a great many pieces of information and sources of tools packed into this innocuous-looking factorization. Let’s take a look at some:\n\n12.6.1 Rank of \\(A\\)\nThis is the number of nonzero values in \\(\\Sigma\\). This follows from Theorem 6.1, since \\(U\\) and \\(V\\), as orthogonal matrices, are invertible.\nIt should be noted, though, that if there are some really tiny elements there, one might also entertain the thought that the true rank is less than this size, as these tiny values may be due to roundoff error from 0.\n\n\n12.6.2 SVD as matrix pseudoinverse\nRecall the example in Section 6.1. We could not have dummy-variable columns for both male and female, as their sum would be a column of all 1s, in addition to a column the X data matrix already had. The three columns would then have a nonzero linear combination that evaluates to the 0 vector. Then in Equation 5.4, \\(A\\) (i.e. X) would not be of full rank, and \\((A'A)^{-1}\\) would not exist.\nAnd yet the equation from which that comes,\n\\[\nA'A b = A'S\n\\tag{12.6}\\]\nis still valid. We could, as in that example, remove one of the gender columns, thus solving the problem of less than full rank, but the use of pseudoinverses (also known as generalized inverses) solves the problem directly. If \\(A\\) has hundreds or thousands of columns, say, the use of pseudoinverses may be more convenient.\nWe omit the formal definition of pseudoinverses and their properties. We will note the use of the latter as the need arises. For now, we state without proof that:\n\nOne of the most famous forms of pseudoinverse, Moore-Penrose, is based on SVD. Given the SVD of a matrix \\(M\\),\n\\[\nM = U_M \\Sigma_M V_{M}'\n\\]\nits Moore-Penrose inverse, denoted by \\(M^{-1}\\) is\n\\[\nM^{-} = V_M \\Sigma_M^{-} U_{M}'\n\\]\nwhere \\(\\Sigma_M^{-}\\) is the diagonal matrix obtained form \\(\\Sigma_M\\) by replacing each nonzero element by its reciprocal.\nThe Moore-Penrose solution of \\(Mz = w\\) for vectors \\(z\\) and \\(w\\), is \\(M^{-} w\\).\n\nBy the way, the R function MASS::ginv performs the necessary computation for us; we need not call svd().\n\n\n12.6.3 SVD in linear models\nNow apply this to our linear model problem Equation 13.1. The claim is that\n\\[\nb = A^{-} S = V \\Sigma^{-} U' S\n\\]\nsolves the equation.  Let’s check (warning: this will be messy):As in Section 3.2, where we found that the inverse of a product is the reverse product of the inverses, the same holds for pseudoinverses.\n\\[\n\\begin{aligned}\nA'A b &= (U \\Sigma V')' (U \\Sigma V') (U \\Sigma V')^{-} S \\\\\n&= (V \\Sigma U') (U \\Sigma V') (V \\Sigma^{-} U') S \\\\\n&= V \\Sigma^2 V'V \\Sigma^{-} U' S \\\\\n&= V \\Sigma^{-} U' S\n\\end{aligned}\n\\]\nBut that is exactly the expression we found for \\(A^{-1}S\\) above.\n\\(\\square\\)"
  },
  {
    "objectID": "Ch6c.html#example-census-data",
    "href": "Ch6c.html#example-census-data",
    "title": "12  Singular Value Decomposition",
    "section": "12.7 Example: Census Data",
    "text": "12.7 Example: Census Data\n\nlibrary(qeML)\ndata(svcensus)\n# have only 1 categorical/dichotomous variable, for simple example\nhead(svcensus)\n\n       age     educ occ wageinc wkswrkd gender\n1 50.30082 zzzOther 102   75000      52 female\n2 41.10139 zzzOther 101   12300      20   male\n3 24.67374 zzzOther 102   15400      52 female\n4 50.19951 zzzOther 100       0      52   male\n5 51.18112 zzzOther 100     160       1 female\n6 57.70413 zzzOther 100       0       0   male\n\nsvc &lt;- svcensus[,-c(2,3)]\nsvc &lt;- factorsToDummies(svc)\nhead(svc)\n\n          age wageinc wkswrkd gender.female gender.male\n[1,] 50.30082   75000      52             1           0\n[2,] 41.10139   12300      20             0           1\n[3,] 24.67374   15400      52             1           0\n[4,] 50.19951       0      52             0           1\n[5,] 51.18112     160       1             1           0\n[6,] 57.70413       0       0             0           1\n\nx &lt;- cbind(1,svc[,-2])\nhead(x)\n\n            age wkswrkd gender.female gender.male\n[1,] 1 50.30082      52             1           0\n[2,] 1 41.10139      20             0           1\n[3,] 1 24.67374      52             1           0\n[4,] 1 50.19951      52             0           1\n[5,] 1 51.18112       1             1           0\n[6,] 1 57.70413       0             0           1\n\nxminus &lt;- MASS::ginv(x)\nbhat &lt;- xminus %*% svc[,2]\nbhat\n\n           [,1]\n[1,] -16022.454\n[2,]    496.747\n[3,]   1372.756\n[4,] -13361.634\n[5,]  -2660.821\n\nlm(wageinc ~ .,svcensus[,-c(2,3)])$coef\n\n(Intercept)         age     wkswrkd  gendermale \n -29384.088     496.747    1372.756   10700.813 \n\n\nThe two approaches are consistent with each other (though internally they are solving slightly different problems). Note that\n\\[\n-13361.634-(-2660.821) = -10700.81\n\\]"
  },
  {
    "objectID": "Ch6c.html#application-svd-as-the-best-low-rank-approximation",
    "href": "Ch6c.html#application-svd-as-the-best-low-rank-approximation",
    "title": "12  Singular Value Decomposition",
    "section": "12.8 Application: SVD as the Best Low-Rank Approximation",
    "text": "12.8 Application: SVD as the Best Low-Rank Approximation"
  },
  {
    "objectID": "Ch6c.html#your-turn",
    "href": "Ch6c.html#your-turn",
    "title": "12  Singular Value Decomposition",
    "section": "12.9 Your Turn",
    "text": "12.9 Your Turn\n❄️ Your Turn: Show that in the SVD factorization, \\(U\\) consists of the eigenvectors of \\(AA'\\).\n❄️ Your Turn: Find a characterization of the left null space of a matrix.\n❄️ Your Turn: Write an R function with a matrix as its sole argument, with return value consisting of an R list containing four matrices, representing the four fundamental subspaces of the input argument. Each matrix will consist of columns equal to a basis for the given subspace."
  },
  {
    "objectID": "Ch6d.html#example-census-data",
    "href": "Ch6d.html#example-census-data",
    "title": "13  Pseudoinverse and Double Descent",
    "section": "13.1 Example: Census Data",
    "text": "13.1 Example: Census Data\n\nlibrary(qeML)\ndata(svcensus)\n# have only 1 categorical/dichotomous variable, for simple example\nhead(svcensus)\n\n       age     educ occ wageinc wkswrkd gender\n1 50.30082 zzzOther 102   75000      52 female\n2 41.10139 zzzOther 101   12300      20   male\n3 24.67374 zzzOther 102   15400      52 female\n4 50.19951 zzzOther 100       0      52   male\n5 51.18112 zzzOther 100     160       1 female\n6 57.70413 zzzOther 100       0       0   male\n\nsvc &lt;- svcensus[,-c(2,3)]\nsvc &lt;- factorsToDummies(svc)\nhead(svc)\n\n          age wageinc wkswrkd gender.female gender.male\n[1,] 50.30082   75000      52             1           0\n[2,] 41.10139   12300      20             0           1\n[3,] 24.67374   15400      52             1           0\n[4,] 50.19951       0      52             0           1\n[5,] 51.18112     160       1             1           0\n[6,] 57.70413       0       0             0           1\n\nx &lt;- cbind(1,svc[,-2])\nhead(x)\n\n            age wkswrkd gender.female gender.male\n[1,] 1 50.30082      52             1           0\n[2,] 1 41.10139      20             0           1\n[3,] 1 24.67374      52             1           0\n[4,] 1 50.19951      52             0           1\n[5,] 1 51.18112       1             1           0\n[6,] 1 57.70413       0             0           1\n\nxminus &lt;- MASS::ginv(x)\nbhat &lt;- xminus %*% svc[,2]\nbhat\n\n           [,1]\n[1,] -16022.454\n[2,]    496.747\n[3,]   1372.756\n[4,] -13361.634\n[5,]  -2660.821\n\nlm(wageinc ~ .,svcensus[,-c(2,3)])$coef\n\n(Intercept)         age     wkswrkd  gendermale \n -29384.088     496.747    1372.756   10700.813 \n\n\nThe two approaches are consistent with each other (though internally they are solving slightly different problems). Note that\n\\[\n-13361.634-(-2660.821) = -10700.81\n\\]"
  },
  {
    "objectID": "Ch6d.html#svd-as-the-minimum-norm-solution",
    "href": "Ch6d.html#svd-as-the-minimum-norm-solution",
    "title": "13  Pseudoinverse and Double Descent",
    "section": "13.2 SVD as the Minimum-Norm Solution",
    "text": "13.2 SVD as the Minimum-Norm Solution\nIn an overdetermined linear system such as Equation 13.1, there are many solutions. However, an advantage of Moore-Penrose is that it gives us the minimum norm solution. We’ll discuss the significance of this shortly, but let’s prove it first.\nWe will need this:\n\nTheorem 13.1 (Multiplication by an Orthogonal Matrix Preserves Norm) For an orthogonal matrix \\(M\\) and a vector \\(w\\), \\(||Mw|| = ||w||\\).\n\n\nProof. See the Your Turn problem below.\n\n\nTheorem 13.2 (The Moore-Penrose Solution Is Min-Norm) Consider a matrix \\(B\\) and vector \\(q\\). Of all solutions \\(x\\) to\n\\[\nBx = q\n\\tag{13.2}\\]\nthe Moore-Penrose solution minimizes \\(||x||\\).\n\n\nProof. Again, writeAdapted from a derivation by Carlo Tomasi\n\\[\nB = U \\Sigma V',\n\\]\nand consider the residual sum of squares\n\\[\n||Bx - q||^2 = ||U (\\Sigma V' x - U'q)||^2\n\\tag{13.3}\\]\nsince \\(UU'=I\\).\nThus we can remove the factor \\(U\\) in Equation 13.3, yielding\n\\[\n||Bx - q||^2 = ||\\Sigma V' x - U'q||^2\n\\tag{13.4}\\]\nRename \\[V'x\\] to \\(y\\):\n\\[\n||Bx - q||^2 = ||\\Sigma y - U'q||^2\n\\tag{13.5}\\]\nNow bring in the fact that \\(\\sigma_i = 0\\) for \\(i &gt; r\\), by writing everything in partitioned fashion. Break \\(U\\) into \\(r\\) and \\(n-r\\) rows,\n\\[\nU =\n\\left (\n\\begin{array}{r}\nU_1 \\\\\nU_2 \\\\\n\\end{array}\n\\right )\n\\]\nand partition other objects similarly:\n\\[\nV = (V_1 | V_2),\n\\]\n\\[\n\\Sigma =\n\\left (\n\\begin{array}{rr}\n\\Sigma_1 & 0 \\\\\n0 & 0  \\\\\n\\end{array}\n\\right ),\n\\]\nand\n\\[\ny =\n\\left (\n\\begin{array}{r}\ny_1 \\\\\ny_2 \\\\\n\\end{array}\n\\right )\n\\]\nSubstituting into Equation 13.5, we have\n\\[\n||Bx - q||^2\n= ||\n\\left (\n\\begin{array}{r}\n\\Sigma_1 y_1 \\\\\n0 \\\\\n\\end{array}\n\\right ) -\n\\left (\n\\begin{array}{r}\nU_1q \\\\\nU_2q \\\\\n\\end{array}\n\\right )\n||^2\n\\tag{13.6}\\]\nThis means that \\(y_2\\) can be anything at all, without changing the residual sum of squares, confirming that there are infinitely many equally-effective solutions!\nBut if we want \\(x\\) to be of minimum length, we need \\(y\\) to be of minimum length (the shortest \\(y\\) gives us the shortest \\(x = Vy\\), again by the Your Turn problem). And,\n\\[\n||y||^2 = ||y_2||^2 + ||y_2||^2\n\\]\nThus we take \\(y_2 = 0\\).\nNow, reviewing Equation 13.7, note that the equation\n\\[\n\\Sigma_1 y_1 = U_1 q\n\\]\nhas the solution\n\\[\ny_1 = \\Sigma_1^{-1} U_1 q\n\\]\nAn interesting sidelight of this is that it means that\n\\[\n||Bx - q||^2\n= ||\n\\left (\n\\begin{array}{r}\n0  \\\\\nU_2q \\\\\n\\end{array}\n\\right )\n||^2,\n\\tag{13.7}\\]\ni.e. the residual sum of squares depends only on \\(U_2\\), not \\(U_1\\). But returning to our quest for the minimum-norm solution, we map back to \\(x = Vy\\), and have\n\\[\nx_1 = V_1 \\Sigma_1^{-1} U_1'q\n\\]\nwhich is the SVD solution! Thus the SVD solution does have minimum norm.\n\\(\\square\\)\n\nto add:\npossible reasons:\nat interp, min norm suddenly gives more than 1 choice; min norm is like min Var; maybe show unbiased by arguing A’A b = A’S for all A, thus A’E(estreg) = A’beta’Y for all A etc.; this of course holds only for estimable x’beta, meaning x is in row space of A (or equiv, A’A)\ncondition number is max at interpolation\n(size of?) second U depends on min nonzero eigenvalue\nmy empirical example\novrf &lt;- function(nreps,n,maxP) { load(‘YearData.save’) # yr &lt;- yr[,1:2] nas &lt;- rep(NA,nreps*(maxP-1)) outdf &lt;- data.frame(p=nas,mape=nas) rownum &lt;- 0 for (i in 1:nreps) { idxs &lt;- sample(1:nrow(yr),n) trn &lt;- yr[idxs,] tst &lt;- yr[-idxs,] for (p in 2:maxP) { rownum &lt;- rownum + 1 # out&lt;-qePolyLin(trn[,1:(p+1)], # ‘V1’,2,holdout=NULL) out &lt;- qeLin(trn[,1:(p+1)],‘V1’,2,holdout=NULL) preds &lt;- predict(out,tst[,2:(p+1)]) mape &lt;- mean(abs(preds - tst[,1])) outdf[rownum,1] &lt;- p outdf[rownum,2] &lt;- mape print(outdf[rownum,]) } } outdf #run through tapply() for the graph }"
  },
  {
    "objectID": "Ch6d.html#application-explaining-the-mystery-of-double-descent",
    "href": "Ch6d.html#application-explaining-the-mystery-of-double-descent",
    "title": "13  Pseudoinverse and Double Descent",
    "section": "13.3 Application: Explaining the Mystery of ``Double Descent”",
    "text": "13.3 Application: Explaining the Mystery of ``Double Descent”\nAround 2018-2019, one of the statistics field’s most deeply-held notions was thrown off its pedestal, largely by some researchers in machine learning. (This is arguably when the idea first became widespread, but for earlier instances, see Marco Loog et al, A brief prehistory of double descent, PNAS, 2020.) And many of the explanations given have been related to SVD.\n\n13.3.1 Motivating example\nLet’s consider the Million Song Dataset from Section 9.2.\n\n\n13.3.2 Overfitting and BEYOND\nRecall that a well-known rule of thumb is that the number \\(p\\) of predictors should be less than \\(\\sqrt{n}\\), where \\(n\\) is the number of data points. We will abandon this and see what happens.\nWe will add predictors one at a time, to form models of increasing complexity. As usual, let \\(p\\) denote the number of predictors in a given model. We will also use only a small number of rows, say \\(n = 30\\). We start with \\(p=1\\), then \\(p=2\\), eventually reaching \\(p = n-1\\) (accounting for the 1s column in the \\(A\\) matrix in Equation 5.6), and not stopping even there. We go to \\(p=n\\), \\(p=n+1\\) and so on.\nWhen we reach \\(p=n+2\\), the matrix \\(A\\) will have more columns than rows, and \\((A'A)^{-1}\\) will not exist. (See Your Turn problem below.) We can still use SVD to solve for \\(b\\), but intuitively some kind of major change may happen at that point.\nHere is the code and output\novrf &lt;- function(nreps,n,maxP)\n {\n    load('YearData.save')\n    # yr &lt;- yr[,1:2]\n    nas &lt;- rep(NA,nreps*(maxP-1))\n    outdf &lt;- data.frame(p=nas,mape=nas)\n    rownum &lt;- 0\n    for (i in 1:nreps) {\n       idxs &lt;- sample(1:nrow(yr),n)\n       trn &lt;- yr[idxs,]\n       tst &lt;- yr[-idxs,]\n       for (p in 2:maxP) {\n          rownum &lt;- rownum + 1\n          # don't use lm or qeLin, since no SVD; instead, fit polynomial\n          # of degree 1, i.e. linear model\n          out &lt;- qePolyLin(trn[,1:(p+1)],'V1',1,holdout=NULL)\n          preds &lt;- predict(out,tst[,2:(p+1)])\n          mape &lt;- mean(abs(preds - tst[,1]))\n          outdf[rownum,1] &lt;- p\n          outdf[rownum,2] &lt;- mape\n          print(outdf[rownum,])\n       }\n    }\n    outdf  #run through tapply() for the graph\n }\n&gt; set.seed(111111); o1 &lt;- ovrf(1,30,40)\n  p     mape\n1 2 7.865117\n  p     mape\n2 3 7.876853\n  p     mape\n3 4 7.984233\n  p     mape\n4 5 8.631492\n  p     mape\n5 6 8.608561\n  p     mape\n6 7 8.734537\n  p    mape\n7 8 8.74647\n  p     mape\n8 9 8.746996\n   p     mape\n9 10 9.042233\n    p     mape\n10 11 8.795864\n    p    mape\n11 12 8.88431\n    p     mape\n12 13 10.06747\n    p     mape\n13 14 10.17324\n    p     mape\n14 15 10.68381\n    p     mape\n15 16 10.41553\n    p     mape\n16 17 9.892437\n    p    mape\n17 18 9.71409\n    p     mape\n18 19 11.25314\n    p     mape\n19 20 11.88647\n    p     mape\n20 21 17.94607\n    p     mape\n21 22 18.63847\n    p     mape\n22 23 15.26327\n    p     mape\n23 24 19.50255\n    p     mape\n24 25 24.53056\n    p     mape\n25 26 23.60238\n    p     mape\n26 27 25.35561\n    p    mape\n27 28 36.1927\n    p     mape\n28 29 57.97567\n    p     mape\n29 30 754.9635\nP &gt; N. With polynomial terms and interactions, P is 31.\n\n\n    p    mape\n30 31 741.573\nP &gt; N. With polynomial terms and interactions, P is 32.\n\n\n    p  mape\n31 32 477.3\nP &gt; N. With polynomial terms and interactions, P is 33.\n\n\n    p     mape\n32 33 525.8362\nP &gt; N. With polynomial terms and interactions, P is 34.\n\n\n    p     mape\n33 34 465.0518\nP &gt; N. With polynomial terms and interactions, P is 35.\n\n\n    p     mape\n34 35 761.3692\nP &gt; N. With polynomial terms and interactions, P is 36.\n\n\n    p    mape\n35 36 689.829\nP &gt; N. With polynomial terms and interactions, P is 37.\n\n\n    p     mape\n36 37 673.8049\nP &gt; N. With polynomial terms and interactions, P is 38.\n\n\n    p     mape\n37 38 728.8918\nP &gt; N. With polynomial terms and interactions, P is 39.\n\n\n    p     mape\n38 39 666.2301\nP &gt; N. With polynomial terms and interactions, P is 40.\n\n\n    p     mape\n39 40 621.9375\nMAPE here is mean absolute prediction error (calculated on the holdout set).\nHere \\(n = 30\\), so \\(p = 30, 31,...\\) uses SVD, and is overfitting. Indeed, much smaller values of \\(p\\) seem to have overfit as well. But the point is that once we got to the point at which there is no unique solution, MAPE actually improved, a huge shock to the field. It was always assumed that there is no point going beyond the values of \\(p\\) that gives us 0 for the sum of squares.\nThe graph of MAPE is typically a U-shape. In this case, we seem to have only the right half of a U, but still a U. What was shocking was that sometimes there is a second U-shape after we reach 0 sum of squares. Even more shocking, in some case the low point of the second U is lower than that of the first–it pays to radically ovefit.\n\n\n13.3.3 The classic and modern views\nIn other words, here is the sea change that occurred in the field around 2018-2019.\nLet \\(\\kappa\\) denote the complexity of a model. In the case of a linear model, \\(\\kappa\\) would be the number of predictor columns in our dataset, and there are ways of defining it for other methods.\nWe might try several values of \\(\\kappa\\), as we did in the table in ?eq-nyc, and then choose the one with smallest MAPE or other accuracy measure.\nBefore 2018-2019, the view was:\n\nIn plotting MAPE against \\(\\kappa\\), the curve will generally be roughly U-shaped, up to the point at which \\(\\kappa\\) gives us a 0 value of RSS in the training set. That value of \\(\\kappa\\), called the interpolation point, will give us “perfect” prediction in the training set, but very poor prediction in the test set, which is what counts.\nWe choose the value of \\(\\kappa\\) at which the curve is lowest. There is no point in trying values of \\(\\kappa\\) past the interpolation point.\n\nThis was taken for granted throughout the statistics and machine learning fields. But around 2018-2019, machine learning engineers were routinely analyzing dataset of extraordinarily large sizes, using unprecedently large values of \\(\\kappa\\). And they discovered that, bizarrely, as \\(\\kappa\\) moved past the interpolation point, the curve often went down–the second “descent”–often tracing its own second U-shape. And most significantly, the low point of the second U was sometimes below that of the first U! Overfitting–grossly so–may pay off!\nSo the modern view is:\n\nThe first U-shaped curve is as in the classic view. But the curve should be plotted past \\(\\kappa\\), and the overall minimum may be in that second U.\n\nThis is a rare sea change in classic quantitative analysis.\n\n\n13.3.4 How can this bizarre effect occur?\nLet’s look further. If at least some our predictors are numeric (with Million Song, they all are), then for \\(p = n-1\\), the (square) matrix \\(A\\) itself will very likely be invertible, and setting\n\\[\nb = A^{-1} S\n\\]\nwill minimize Equation 5.1–with the value of that expression being 0–a perfect fit to the data!\nNow again consider \\(p=n\\). Let \\(A_{new}\\) denote our new \\(A\\) (it will be equal to the old one with a new column added on the right), and \\(b_{new}\\) denote a new version of \\(b\\). We say “a new version” here, because there are now infinitely many versions; recall that the SVD version has minimum norm among them, a point we will return to shortly. Let’s use \\(A_{old}\\) and \\(b_{ol}\\) to denote the quantities we got for \\(p=n-1\\).\nWe can write\n\\[\nA_{new} =\n(A_{old} | c_{n+1})\n\\]\nand\n\\[\nb_{new} = (b_{old} | b_{n+1})'\n\\]\nThus\n\\[\nA_{new}' A_{new} b_{new} =\n(A_{old}' A_{old} + c_{n+1} c_{n+1}')\n\\left (\n\\begin{array}{r}\nb_{old} \\\\\nb_{n+1} \\\\\n\\end{array}\n\\right )\n\\]\nAs noted, we now must use SVD. We will still get a perfect fit. To see this, set \\(b_{n+1} = 0\\) above, which gives us the same fit we got for \\(n = p-1\\). In fact, there will be infinitely many values of \\(b\\) that achieve that perfect fit. But remember, the SVD solution has minimum norm among them all, a point we will return to shortly.\nIn considering the effectiveness of an estimator in statistics, we often look at bias and variance. Let’s consider bias first.\nIn our setting here, the quantities of interest are of the form \\(w'\\beta\\) the predicted value for an individual who has the characterics \\(w\\). We don’t know \\(\\beta\\), so we use \\(w'b\\) instead. , estimated by \\(w'b\\). In the non-full rank setting, not all such quantities are physically estimable, but the theory says that any \\(w\\) in the row space of \\(A\\) will be fine. Moreover, it says the least squares estimate of \\(w'\\beta\\) will have 0 bias.\nNow concerning variance, we have\n\\[\nVar(w'b) = w' Cov(b) w\n\\]\nIn general, shorter random vectors will have less variability, which though not the same as variance/covariance at least gives us the feeling that such an estimator has low variance. Recall that this is the rationale for shrinkage estimators – accept a small amount of bias in return for a reduction in variance.\nThus it is at least plausible that we might get a second U-shape, as the impact of the minimum-norm nature of SVD kicks in. On the other hand, as \\(p\\) grows further, \\(b\\) has more and more components, and likely that minmum-norm \\(b\\) will grow in length at some point, hence the typical later turn back upward of the second U."
  },
  {
    "objectID": "Ch6d.html#your-turn",
    "href": "Ch6d.html#your-turn",
    "title": "13  Pseudoinverse and Double Descent",
    "section": "13.4 Your Turn",
    "text": "13.4 Your Turn\n❄️ Your Turn: Show that for an orthogonal matrix \\(M\\) and a vector \\(w\\), \\(||Mw|| = ||w||\\).\n❄️ Your Turn: Using properties of matrix rank, show that if \\(p+1 &gt; n\\) in Equation 5.6, the inverse will not exist."
  },
  {
    "objectID": "Ch6d.html#your-turn-1",
    "href": "Ch6d.html#your-turn-1",
    "title": "13  Pseudoinverse and Double Descent",
    "section": "13.5 Your Turn",
    "text": "13.5 Your Turn"
  },
  {
    "objectID": "Ch7a.html",
    "href": "Ch7a.html",
    "title": "14  Recommender Systems",
    "section": "",
    "text": "Goals of this chapter:"
  },
  {
    "objectID": "Ch8a.html#tensors",
    "href": "Ch8a.html#tensors",
    "title": "15  Neural Networks",
    "section": "15.1 Tensors",
    "text": "15.1 Tensors"
  },
  {
    "objectID": "Ch8a.html#sgd",
    "href": "Ch8a.html#sgd",
    "title": "15  Neural Networks",
    "section": "15.2 SGD",
    "text": "15.2 SGD"
  },
  {
    "objectID": "Ch8a.html#sgd-and-double-descent",
    "href": "Ch8a.html#sgd-and-double-descent",
    "title": "15  Neural Networks",
    "section": "15.3 SGD and Double Descent",
    "text": "15.3 SGD and Double Descent"
  },
  {
    "objectID": "Ch8a.html#low-rank-approximation-of-weight-matrices",
    "href": "Ch8a.html#low-rank-approximation-of-weight-matrices",
    "title": "15  Neural Networks",
    "section": "15.4 Low Rank Approximation of Weight Matrices",
    "text": "15.4 Low Rank Approximation of Weight Matrices"
  },
  {
    "objectID": "Ch8a.html#role-of-matrix-condition-number",
    "href": "Ch8a.html#role-of-matrix-condition-number",
    "title": "15  Neural Networks",
    "section": "15.5 Role of Matrix Condition Number",
    "text": "15.5 Role of Matrix Condition Number\nhttps://math.stackexchange.com/questions/4362666/relationship-between-condition-number-of-a-matrix-and-eigenvalues"
  }
]