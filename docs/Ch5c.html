<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>10&nbsp; Shrinkage Estimators – Powered by Linear Algebra</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./Ch6a.html" rel="next">
<link href="./Ch5bb.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-5e2915e4d4df5928b2b0a61215b328b6.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./Ch5c.html"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Shrinkage Estimators</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Powered by Linear Algebra</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">index.html</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./preface.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Matrices and Vectors</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Matrix Inverse</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch2a.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Covariance Matrices, MV Normal Distribution, Delta Method</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Linear Statistical Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Matrix Rank</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch5a.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Vector Spaces</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch5b.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Inner Product Spaces</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch5bb.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Four Fundamental Spaces</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch5c.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Shrinkage Estimators</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch6a.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Eigenanalysis</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch6b.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Principal Components</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch6c.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Singular Value Decomposition</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch6d.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">A Deeper Look at Overfitting</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch8a.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Selected Issues in Machine Learning</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Contents</h2>
   
  <ul>
  <li><a href="#sec-nearly" id="toc-sec-nearly" class="nav-link active" data-scroll-target="#sec-nearly"><span class="header-section-number">10.1</span> Classic View of Shrinkage Estimators: Multicollinearity</a></li>
  <li><a href="#sec-millionsong" id="toc-sec-millionsong" class="nav-link" data-scroll-target="#sec-millionsong"><span class="header-section-number">10.2</span> Example: Million Song Dataset</a></li>
  <li><a href="#ridge-regression" id="toc-ridge-regression" class="nav-link" data-scroll-target="#ridge-regression"><span class="header-section-number">10.3</span> Ridge Regression</a>
  <ul class="collapse">
  <li><a href="#the-ridge-solution" id="toc-the-ridge-solution" class="nav-link" data-scroll-target="#the-ridge-solution"><span class="header-section-number">10.3.1</span> The ridge solution</a></li>
  <li><a href="#sec-matform" id="toc-sec-matform" class="nav-link" data-scroll-target="#sec-matform"><span class="header-section-number">10.3.2</span> Matrix formulation</a></li>
  <li><a href="#example-million-song-dataset" id="toc-example-million-song-dataset" class="nav-link" data-scroll-target="#example-million-song-dataset"><span class="header-section-number">10.3.3</span> Example: Million Song dataset</a></li>
  </ul></li>
  <li><a href="#choosing-the-value-of-lambda" id="toc-choosing-the-value-of-lambda" class="nav-link" data-scroll-target="#choosing-the-value-of-lambda"><span class="header-section-number">10.4</span> Choosing the Value of <span class="math inline">\(\lambda\)</span></a></li>
  <li><a href="#sec-nte-monkeys" id="toc-sec-nte-monkeys" class="nav-link" data-scroll-target="#sec-nte-monkeys"><span class="header-section-number">10.5</span> P-hacking, Both in Hypothesis Testing and Generally</a></li>
  <li><a href="#sec-xval" id="toc-sec-xval" class="nav-link" data-scroll-target="#sec-xval"><span class="header-section-number">10.6</span> Cross-validation</a></li>
  <li><a href="#example-million-song-data" id="toc-example-million-song-data" class="nav-link" data-scroll-target="#example-million-song-data"><span class="header-section-number">10.7</span> Example: Million Song Data</a></li>
  <li><a href="#sec-pgtn" id="toc-sec-pgtn" class="nav-link" data-scroll-target="#sec-pgtn"><span class="header-section-number">10.8</span> Modern View of Shrinkage Estimators</a></li>
  <li><a href="#formalizing-the-notion-of-shrinkage" id="toc-formalizing-the-notion-of-shrinkage" class="nav-link" data-scroll-target="#formalizing-the-notion-of-shrinkage"><span class="header-section-number">10.9</span> Formalizing the Notion of Shrinkage</a>
  <ul class="collapse">
  <li><a href="#shrinkage-through-length-penalization" id="toc-shrinkage-through-length-penalization" class="nav-link" data-scroll-target="#shrinkage-through-length-penalization"><span class="header-section-number">10.9.1</span> Shrinkage through length penalization</a></li>
  <li><a href="#shrinkage-through-length-limitation" id="toc-shrinkage-through-length-limitation" class="nav-link" data-scroll-target="#shrinkage-through-length-limitation"><span class="header-section-number">10.9.2</span> Shrinkage through length limitation</a></li>
  </ul></li>
  <li><a href="#the-lasso" id="toc-the-lasso" class="nav-link" data-scroll-target="#the-lasso"><span class="header-section-number">10.10</span> The LASSO</a>
  <ul class="collapse">
  <li><a href="#properties" id="toc-properties" class="nav-link" data-scroll-target="#properties"><span class="header-section-number">10.10.1</span> Properties</a></li>
  </ul></li>
  <li><a href="#sec-dimred" id="toc-sec-dimred" class="nav-link" data-scroll-target="#sec-dimred"><span class="header-section-number">10.11</span> Ridge vs.&nbsp;LASSO for Dimension Reduction</a>
  <ul class="collapse">
  <li><a href="#geometric-view" id="toc-geometric-view" class="nav-link" data-scroll-target="#geometric-view"><span class="header-section-number">10.11.1</span> Geometric view</a></li>
  <li><a href="#implication-for-dimension-reduction." id="toc-implication-for-dimension-reduction." class="nav-link" data-scroll-target="#implication-for-dimension-reduction."><span class="header-section-number">10.11.2</span> Implication for dimension reduction.</a></li>
  <li><a href="#avoidance-of-overfitting-without-dimension-reduction" id="toc-avoidance-of-overfitting-without-dimension-reduction" class="nav-link" data-scroll-target="#avoidance-of-overfitting-without-dimension-reduction"><span class="header-section-number">10.11.3</span> Avoidance of overfitting <em>without</em> dimension reduction</a></li>
  </ul></li>
  <li><a href="#sec-useall" id="toc-sec-useall" class="nav-link" data-scroll-target="#sec-useall"><span class="header-section-number">10.12</span> Example: NYC Taxi Data</a></li>
  <li><a href="#sec-iter" id="toc-sec-iter" class="nav-link" data-scroll-target="#sec-iter"><span class="header-section-number">10.13</span> Iterative Calculation</a></li>
  <li><a href="#the-kernel-trick-and-kernel-ridge-regression" id="toc-the-kernel-trick-and-kernel-ridge-regression" class="nav-link" data-scroll-target="#the-kernel-trick-and-kernel-ridge-regression"><span class="header-section-number">10.14</span> The Kernel Trick and Kernel Ridge Regression</a>
  <ul class="collapse">
  <li><a href="#polynomial-regression" id="toc-polynomial-regression" class="nav-link" data-scroll-target="#polynomial-regression"><span class="header-section-number">10.14.1</span> Polynomial Regression</a></li>
  <li><a href="#the-kernel-trick" id="toc-the-kernel-trick" class="nav-link" data-scroll-target="#the-kernel-trick"><span class="header-section-number">10.14.2</span> The Kernel Trick</a></li>
  <li><a href="#kernel-ridge-regression-the-code" id="toc-kernel-ridge-regression-the-code" class="nav-link" data-scroll-target="#kernel-ridge-regression-the-code"><span class="header-section-number">10.14.3</span> Kernel Ridge Regression: the Code</a></li>
  </ul></li>
  <li><a href="#a-warning" id="toc-a-warning" class="nav-link" data-scroll-target="#a-warning"><span class="header-section-number">10.15</span> A Warning</a></li>
  <li><a href="#your-turn" id="toc-your-turn" class="nav-link" data-scroll-target="#your-turn"><span class="header-section-number">10.16</span> Your Turn</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="sec-shrinkage" class="quarto-section-identifier"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Shrinkage Estimators</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<div style="page-break-after: always;"></div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Goals of this chapter:
</div>
</div>
<div class="callout-body-container callout-body">
<p>In the previous chapter, we introduced the norm of a vector. In many data science applications, solutions with smaller norms may be more accurate. This point is explored here.</p>
</div>
</div>
<section id="sec-nearly" class="level2" data-number="10.1">
<h2 data-number="10.1" class="anchored" data-anchor-id="sec-nearly"><span class="header-section-number">10.1</span> Classic View of Shrinkage Estimators: Multicollinearity</h2>
<p>The notion of <em>multicollinearity</em> was the original motivation for shrinkage estimators. It refers to settings in which the following concerns arise:</p>
<ul>
<li><p>One column of the matrix <span class="math inline">\(A\)</span> in <a href="Ch3.html#eq-linregformula" class="quarto-xref">Equation&nbsp;<span>5.6</span></a> is nearly equal to some linear combination of the others.</p></li>
<li><p>Thus <span class="math inline">\(A\)</span> is nearly not of full rank.</p></li>
<li><p>Thus <span class="math inline">\(A'A\)</span> is nearly not of full rank.</p></li>
<li><p>Thus <span class="math inline">\(\widehat{\beta}\)</span> is unstable, in the form of high variance.</p></li>
</ul>
<p>That latter point is often quantified by the <em>Variance Inflation Factor</em>. To motivate it, consider the “R-squared” value from linear regression analyis, which is the squared correlation between true “Y” and predicted “Y”. Let <span class="math inline">\(R^2_j\)</span> denote that measure in the case of predicting column <span class="math inline">\(j\)</span> of <span class="math inline">\(A\)</span> from the other columns. The quantity</p>
<p><span class="math display">\[
VIF_j = \frac{1}{1-R^2_j}
\]</span></p>
<p>then measures negative impact due to multicollinearity on estimating <span class="math inline">\(\widehat{\beta}\)</span>. The intuition is that if, say, column 3 of <span class="math inline">\(A\)</span> can be predicted well using a linear model, then that column is approximately equal to a linear combination of the other “X” columns. This is worrisome in light of the problems described above.</p>
<p>Needless to say, the word “nearly” above, e.g.&nbsp;in “nearly not of full rank,” is vague, and leaves open the question of “What can we do about it?” We will present several answers to these questions in this and the succeeding chapters.</p>
</section>
<section id="sec-millionsong" class="level2" data-number="10.2">
<h2 data-number="10.2" class="anchored" data-anchor-id="sec-millionsong"><span class="header-section-number">10.2</span> Example: Million Song Dataset</h2>
<p>Let’s consider the Million Song Dataset, varous versions of which are on the Web.</p>
<p>Ours is a 50,000-line subset of the one with 515345 rows and 91 columns. The first column is the year of release, followed by 90 columns of various audio measurements. The goal is to predict the year, <strong>V1</strong>, from the audio variables <strong>V2</strong> through <strong>V91</strong>.</p>
<p>The function <strong>regclass::VIF</strong> will compute the VIF values for us.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(WackyData)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(MillSong50K) <span class="co"># loads s50</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>lmout <span class="ot">&lt;-</span> <span class="fu">lm</span>(V1 <span class="sc">~</span> .,<span class="at">data=</span>s50)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(regclass)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="fu">VIF</span>(lmout)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>      V2       V3       V4       V5       V6       V7       V8       V9 
3.215081 2.596474 4.236853 7.414375 1.534492 5.844729 2.794018 3.192805 
     V10      V11      V12      V13      V14      V15      V16      V17 
2.072851 3.673590 4.705886 1.708520 2.446279 2.827296 3.672250 7.409782 
     V18      V19      V20      V21      V22      V23      V24      V25 
2.634033 9.472261 4.217311 7.147952 5.122114 7.984860 9.675134 3.591586 
     V26      V27      V28      V29      V30      V31      V32      V33 
1.818596 1.758043 3.879968 1.663670 2.108174 2.321385 2.056017 1.854913 
     V34      V35      V36      V37      V38      V39      V40      V41 
3.011534 2.040587 2.760111 2.879667 1.918229 2.176048 2.074103 1.946859 
     V42      V43      V44      V45      V46      V47      V48      V49 
1.704259 2.138794 1.690651 1.556782 1.817380 3.000759 1.547592 2.140282 
     V50      V51      V52      V53      V54      V55      V56      V57 
2.496121 1.612253 2.042571 2.208492 1.723669 2.024290 2.016403 2.033654 
     V58      V59      V60      V61      V62      V63      V64      V65 
2.004260 2.998469 2.074152 3.410124 2.153116 1.378160 3.270617 1.502543 
     V66      V67      V68      V69      V70      V71      V72      V73 
2.581171 1.725809 2.167673 2.379354 2.062862 1.703360 2.036596 1.984427 
     V74      V75      V76      V77      V78      V79      V80      V81 
2.557163 1.465020 1.515436 2.260728 1.840509 2.078497 3.604771 1.595064 
     V82      V83      V84      V85      V86      V87      V88      V89 
2.528307 2.005876 2.283956 1.448379 1.895053 1.601004 1.581099 2.252362 
     V90      V91 
1.332590 1.570891 </code></pre>
</div>
</div>
<p>As a rough guide, values of VIF about 5.0 are considered concerning by many analysts. Under that criterion, variables <strong>V5</strong>, <strong>V7</strong>, <strong>V17</strong> and so on look troublesome.</p>
<p>What can be done? One simple approach would be to delete those columns from the dataset. This is indeed is a common solution, but another is <em>ridge regression</em>, which we present next.</p>
</section>
<section id="ridge-regression" class="level2 page-columns page-full" data-number="10.3">
<h2 data-number="10.3" class="anchored" data-anchor-id="ridge-regression"><span class="header-section-number">10.3</span> Ridge Regression</h2>
<div class="page-columns page-full"><p>In seminal paper, Hoerl and Kennard presented a new approach to the problem of multicollinearity of predictor variables in a linear model.</p><div class="no-row-height column-margin column-container"><span class=""><em>Technometrics</em>, February 1970</span></div></div>
<section id="the-ridge-solution" class="level3" data-number="10.3.1">
<h3 data-number="10.3.1" class="anchored" data-anchor-id="the-ridge-solution"><span class="header-section-number">10.3.1</span> The ridge solution</h3>
<p>Their solution is simple; Add some quantity to the diagonal of <span class="math inline">\(A'A\)</span>. Specifically, <a href="Ch3.html#eq-linregformula" class="quarto-xref">Equation&nbsp;<span>5.6</span></a> now becomes</p>
<p><span id="eq-ridge"><span class="math display">\[
\widehat{\beta} = (A'A + \lambda I)^{-1} A'S
\tag{10.1}\]</span></span></p>
<p>where <span class="math inline">\(\lambda\)</span> is a positive number chosen by the analyst.</p>
<p>Here <span class="math inline">\(A\)</span> has dimensions <span class="math inline">\(n \times p\)</span>, and <span class="math inline">\(I\)</span> is the <span class="math inline">\(p \times p\)</span> identity matrix.</p>
</section>
<section id="sec-matform" class="level3" data-number="10.3.2">
<h3 data-number="10.3.2" class="anchored" data-anchor-id="sec-matform"><span class="header-section-number">10.3.2</span> Matrix formulation</h3>
<p>Using partitioned matrices helps understand ridge. Replace <span class="math inline">\(A\)</span> and <span class="math inline">\(S\)</span> by</p>
<p><span class="math display">\[
A_{new} =
\left (
\begin{array}{r}
A \\
\lambda I \\
\end{array}
\right )
\]</span></p>
<p>and</p>
<p><span class="math display">\[
S_{new} =
\left (
\begin{array}{r}
S \\
0 \\
\end{array}
\right )
\]</span></p>
<p>where 0 means <span class="math inline">\(p\)</span> 0s. In essence, we are adding artificial data here, consisting of <span class="math inline">\(p\)</span> new rows to <span class="math inline">\(A\)</span>, and <span class="math inline">\(p\)</span> new elements to <span class="math inline">\(S\)</span>. So <a href="#eq-ridge" class="quarto-xref">Equation&nbsp;<span>10.1</span></a> is just the result of applying <a href="Ch3.html#eq-linregformula" class="quarto-xref">Equation&nbsp;<span>5.6</span></a> to <span class="math inline">\(A_{new}\)</span> and <span class="math inline">\(S_{new}\)</span>.</p>
<p>Loosely speaking, we can think of the addition of <span class="math inline">\(\lambda I\)</span> to <span class="math inline">\(A'A\)</span> as making the latter “larger”, and thus making its inverse smaller. In other words, we are “shrinking” <span class="math inline">\(\widehat{\beta}\)</span> towards 0. This effect is made even stronger by the fact that we added 0s data to <span class="math inline">\(S\)</span>. This will be made more precise below.</p>
</section>
<section id="example-million-song-dataset" class="level3" data-number="10.3.3">
<h3 data-number="10.3.3" class="anchored" data-anchor-id="example-million-song-dataset"><span class="header-section-number">10.3.3</span> Example: Million Song dataset</h3>
<p>We will use <strong>glmnet</strong>, one of the most widely-used R packages.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(glmnet)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> s50[,<span class="sc">-</span><span class="dv">1</span>]</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> s50[,<span class="dv">1</span>]</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>glmOut <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(x,y,</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>             <span class="at">alpha=</span><span class="dv">0</span>,  <span class="co"># ridge</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>             <span class="at">lambda=</span><span class="fl">0.1</span>)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(glmOut)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>91 x 1 sparse Matrix of class "dgCMatrix"
                       s0
(Intercept)  1.952611e+03
V2           8.573258e-01
V3          -5.589618e-02
V4          -4.521365e-02
V5           8.939743e-04
V6          -9.630525e-03
V7          -2.075296e-01
V8          -4.753624e-03
V9          -9.733144e-02
V10         -6.383153e-02
V11          2.778348e-02
V12         -1.486791e-01
V13         -1.326579e-02
V14          4.756877e-02
V15          3.197157e-04
V16         -4.765018e-04
V17          4.951670e-04
V18          5.276124e-04
V19          1.254372e-03
V20          1.518736e-03
V21          2.206416e-03
V22         -4.304329e-04
V23          5.792964e-04
V24          7.717453e-03
V25          3.205090e-03
V26         -3.527333e-03
V27          4.785205e-05
V28          1.510325e-03
V29          2.996810e-04
V30          6.384049e-04
V31         -2.560997e-04
V32         -4.861561e-04
V33         -6.250266e-04
V34         -3.757523e-03
V35          3.563733e-04
V36          1.288622e-03
V37         -4.417041e-03
V38         -2.502325e-04
V39          9.405005e-04
V40          1.490186e-03
V41         -1.534097e-03
V42         -1.510983e-03
V43         -1.777780e-03
V44         -1.814201e-03
V45         -1.865966e-03
V46         -1.107727e-03
V47          5.906898e-03
V48          6.578124e-04
V49         -2.040170e-03
V50          4.795372e-04
V51          1.162657e-03
V52          6.164815e-04
V53         -9.613775e-04
V54          1.571230e-03
V55         -1.230929e-03
V56         -1.395143e-03
V57          2.158453e-04
V58         -1.975641e-03
V59          2.039760e-03
V60         -1.302405e-03
V61          6.875221e-04
V62         -3.480975e-03
V63         -3.285816e-03
V64         -9.199332e-03
V65          1.283424e-03
V66         -1.444237e-03
V67         -5.957302e-05
V68          1.139299e-03
V69         -9.805816e-04
V70         -3.734949e-03
V71         -5.127353e-03
V72         -1.071399e-03
V73          1.808175e-04
V74         -1.034781e-05
V75          4.315147e-03
V76          3.382526e-03
V77          1.111779e-02
V78          3.162072e-04
V79         -4.565407e-03
V80          3.729735e-05
V81          2.362107e-04
V82         -9.349464e-04
V83         -2.846584e-04
V84          1.439897e-03
V85          1.360484e-03
V86          2.442744e-02
V87         -6.838186e-04
V88          8.555403e-04
V89         -3.329155e-02
V90         -1.793489e-03
V91         -3.614719e-05</code></pre>
</div>
</div>
<p>We had earlier flagged variable <span class="math inline">\(V5\)</span> as causing multicollinearity. As noted then, we could simply exclude it, but here under ridge, we see that it has been assigned a very small regression coefficient compared to most others.</p>
<p>So, did the estimated coefficient vector shrink?</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>l2norm <span class="ot">&lt;-</span> <span class="cf">function</span>(x) <span class="fu">sqrt</span>(<span class="fu">sum</span>(x<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>bh01<span class="ot">&lt;-</span> <span class="fu">coef</span>(<span class="fu">glmnet</span>(x,y,<span class="at">alpha=</span><span class="dv">0</span>,<span class="at">lambda=</span><span class="fl">0.1</span>))</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="fu">l2norm</span>(bh01)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 1952.612</code></pre>
</div>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>bhOLS <span class="ot">&lt;-</span> <span class="fu">coef</span>(<span class="fu">lm</span>(V1 <span class="sc">~</span> .,s50)) <span class="co"># OLS means Ordinary Least Squares</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="fu">l2norm</span>(bhOLS)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 1951.028</code></pre>
</div>
</div>
<p>No, the vector got larger!</p>
<p>The culprit is the intercept term, <span class="math inline">\(\widehat{\beta}_0\)</span>:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>bh01[<span class="dv">1</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 1952.611</code></pre>
</div>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>bhOLS[<span class="dv">1</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(Intercept) 
   1951.028 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="fu">l2norm</span>(bh01[<span class="sc">-</span><span class="dv">1</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.9080075</code></pre>
</div>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="fu">l2norm</span>(bhOLS[<span class="sc">-</span><span class="dv">1</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.9451025</code></pre>
</div>
</div>
<p>The rest of the vector did shrink (though not necessily element-by-element).</p>
<p>Actually, we should have centered and scaled “X” before applying ridge, since the predictors are of such different magnitudes. This also makes the intercept 0.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>s50a <span class="ot">&lt;-</span> s50</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>s50a[,<span class="sc">-</span><span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="fu">scale</span>(s50[,<span class="sc">-</span><span class="dv">1</span>])</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>s50a[,<span class="dv">1</span>] <span class="ot">&lt;-</span> s50a[,<span class="dv">1</span>] <span class="sc">-</span> <span class="fu">mean</span>(s50a[,<span class="dv">1</span>])</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>bh01<span class="ot">&lt;-</span> <span class="fu">coef</span>(<span class="fu">glmnet</span>(s50a[,<span class="sc">-</span><span class="dv">1</span>],s50a[,<span class="dv">1</span>],<span class="at">alpha=</span><span class="dv">0</span>,<span class="at">lambda=</span><span class="fl">0.1</span>))</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a><span class="fu">l2norm</span>(bh01)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 7.570367</code></pre>
</div>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>bhOLS <span class="ot">&lt;-</span> <span class="fu">coef</span>(<span class="fu">lm</span>(V1 <span class="sc">~</span> .,s50a))</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="fu">l2norm</span>(bhOLS)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 7.886902</code></pre>
</div>
</div>
</section>
</section>
<section id="choosing-the-value-of-lambda" class="level2" data-number="10.4">
<h2 data-number="10.4" class="anchored" data-anchor-id="choosing-the-value-of-lambda"><span class="header-section-number">10.4</span> Choosing the Value of <span class="math inline">\(\lambda\)</span></h2>
<p>So, how do we choose <span class="math inline">\(\lambda\)</span>? We need to note a very important principle first.</p>
</section>
<section id="sec-nte-monkeys" class="level2 page-columns page-full" data-number="10.5">
<h2 data-number="10.5" class="anchored" data-anchor-id="sec-nte-monkeys"><span class="header-section-number">10.5</span> P-hacking, Both in Hypothesis Testing and Generally</h2>
<p>“In a set of 10,000 randomly typing monkeys, one of them will accidentally type a Shakespearian sonnet.”</p>
<div class="page-columns page-full"><p>Some readers have heard this before in the context of <em>p-hacking</em>, in which an analyst poses a large collection of research questions, and runs a statistical hypothesis test on each of them. Even if the null hypothesis is true for all of them, each will have a 5% chance of being  rejected, and since there are many, the chance that at least one will be rejected and declared “significant,” even if <span class="math inline">\(H_0\)</span> is true.</p><div class="no-row-height column-margin column-container"><span class="">One should not be doing hypothesis tests in the first place. See <a href="https://matloff.github.io/No-P-Values/NPV.html">these notes</a>.</span></div></div>
<p>This problem arises in many, many Data Science contexts, and a good analyst must be vigilant to recognize the potential to mislead. In our context here, if we look at many values of <span class="math inline">\(\lambda\)</span>, one of them may by accident look very promising, e.g.&nbsp;result in very good prediction accuracy on our training set yet actually overfit.</p>
</section>
<section id="sec-xval" class="level2" data-number="10.6">
<h2 data-number="10.6" class="anchored" data-anchor-id="sec-xval"><span class="header-section-number">10.6</span> Cross-validation</h2>
<p>A common way to choose among models is <em>cross-validation</em>: We set aside a subset of the data, known as the <em>holdout</em> or <em>test</em> set, for use in testing predictive accuracy. The remaining data is the <em>training set</em>. For each of our competing models – in this case, competing values of <span class="math inline">\(\lambda\)</span> – we fit the model on the training set, then use the result to predict the test set. We then use whichever model – again, in this case whichever value of <span class="math inline">\(\lambda\)</span> – does best in the test set.</p>
<p>The test set serves as “fresh data,” simulating how our fitted model might do in the real world (assuming our data is representative of the real world). Predicting on the training set is not as good, since our fit was by design tailored to that data.</p>
<p>We still run the risk of p-hacking, but cross-validation works well as long as we keep the problem in mind. This topic will come up in future chapters as well.</p>
</section>
<section id="example-million-song-data" class="level2" data-number="10.7">
<h2 data-number="10.7" class="anchored" data-anchor-id="example-million-song-data"><span class="header-section-number">10.7</span> Example: Million Song Data</h2>
<div class="sourceCode" id="cb21"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span> <span class="fu">library</span>(glmnet)</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span> glmOut <span class="ot">&lt;-</span> <span class="fu">cv.glmnet</span>(<span class="at">x=</span><span class="fu">as.matrix</span>(s50[,<span class="sc">-</span><span class="dv">1</span>]),<span class="at">y=</span>s50<span class="sc">$</span>V1,<span class="at">alpha=</span><span class="dv">0</span>)</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span> glmOut</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>    Lambda Index Measure     SE Nonzero</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>min <span class="fl">0.2474</span>   <span class="dv">100</span>   <span class="fl">89.44</span> <span class="fl">0.9201</span>      <span class="dv">90</span></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a><span class="dv">1</span>se <span class="fl">0.9987</span>    <span class="dv">85</span>   <span class="fl">90.28</span> <span class="fl">0.9823</span>      <span class="dv">90</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The <span class="math inline">\(\lambda\)</span> value that gave the smallest Mean Squared Prediction Error (MSE) was 0.2474. (Coincidentally, it was also the smallest value that the function tried; see <span class="math inline">\(\textbf{glmOut\$lambda}\)</span>.)</p>
<p>A more conservative value of <span class="math inline">\(\lambda\)</span> was 0.9987, the largest <span class="math inline">\(\lambda\)</span> giving MSE within one standard error of the minimum; it’s conservative in the sense of being less likely to overfit (p-hacking); its MSE value, 90.28, was only slightly larger than the best one. In each case, all 90 predictors had nonzero coefficient estimates.</p>
<p>We can then predict as usual. Say we have a song similar to that in <strong>s50[1,]</strong>, but with <strong>V2</strong> equal to 25.0.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>z <span class="ot">&lt;-</span> s50[<span class="dv">1</span>,<span class="sc">-</span><span class="dv">1</span>]  <span class="co"># exclude Y</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>z[<span class="dv">1</span>,<span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="fl">25.0</span></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a><span class="fu">predict</span>(glmOut,z)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>           s0
[1,] 2003.235</code></pre>
</div>
</div>
<p>The year of release is predicted to be 2003.</p>
</section>
<section id="sec-pgtn" class="level2" data-number="10.8">
<h2 data-number="10.8" class="anchored" data-anchor-id="sec-pgtn"><span class="header-section-number">10.8</span> Modern View of Shrinkage Estimators</h2>
<p>There are many ways to deal with multicollinearity other than shrinkage, and indeed, these days one seldom hears mention of multicollinearity in discussions of shrinkage. Instead, the goal is <em>dimension reduction</em>, meaning to reduce the complexity of a model in order to avoid overfitting; the LASSO, introduced below, does this explicitly, while ridge accomplishes it via pure size reduction.</p>
<p>We will discuss this further in <a href="#sec-dimred" class="quarto-xref"><span>Section 10.11</span></a>, but one more point about the discussion no longer being motivated explicitly by multicollinearity: One can apply ridge to situations of <strong>exact</strong> dependence among the columns of X, as opposed to the original motivation of dealing with <em>approximate</em> linear dependence. in which there is <em>exact</em> linear dependence.</p>
<p>We saw such a setting in <a href="Ch4.html#sec-censusrref" class="quarto-xref"><span>Section 6.1</span></a>. There we deliberately induced exact linear dependence by inclusion of both male and female dummy variables. Let’s apply ridge:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(qeML) </span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(svcensus) </span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>svc <span class="ot">&lt;-</span> svcensus[,<span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">4</span><span class="sc">:</span><span class="dv">6</span>)]  </span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>svc<span class="sc">$</span>man <span class="ot">&lt;-</span> <span class="fu">as.numeric</span>(svc<span class="sc">$</span>gender <span class="sc">==</span> <span class="st">'male'</span>) </span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>svc<span class="sc">$</span>woman <span class="ot">&lt;-</span> <span class="fu">as.numeric</span>(svc<span class="sc">$</span>gender <span class="sc">==</span> <span class="st">'female'</span>) </span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>svc<span class="sc">$</span>gender <span class="ot">&lt;-</span> <span class="cn">NULL</span> </span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>a <span class="ot">&lt;-</span> svc[,<span class="sc">-</span><span class="dv">2</span>] </span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>lambda <span class="ot">&lt;-</span> <span class="fl">0.1</span> </span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>a <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(a) </span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a>tmp1 <span class="ot">&lt;-</span> <span class="fu">solve</span>(<span class="fu">t</span>(a) <span class="sc">%*%</span> a <span class="sc">+</span> lambda <span class="sc">*</span> <span class="fu">diag</span>(<span class="dv">4</span>)) </span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a>tmp1 <span class="sc">%*%</span> <span class="fu">t</span>(a) <span class="sc">%*%</span> <span class="fu">as.matrix</span>(svc<span class="sc">$</span>wageinc) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>               [,1]
age        496.6716
wkswrkd   1372.7052
man     -18677.8751
woman   -29378.3086</code></pre>
</div>
</div>
<p>The results essentially are the same as what we obtained by having only one dummy, thus no linear dependence: Men still enjoy about an $11,000 advantage. But ridge allowed us to avoid deleting one of our dummies. Such deletion is easy in this case, but for large <span class="math inline">\(p\)</span>, say in the hundreds or even more, some analysts prefer the convenience of ridge.</p>
</section>
<section id="formalizing-the-notion-of-shrinkage" class="level2" data-number="10.9">
<h2 data-number="10.9" class="anchored" data-anchor-id="formalizing-the-notion-of-shrinkage"><span class="header-section-number">10.9</span> Formalizing the Notion of Shrinkage</h2>
<p>How in the world did statisticians develop an interest in shrinking estimators? A watershed event occurred in the early 1980s, when the statistical world was shocked by research by James and Stein that found, in short, that:</p>
<blockquote class="blockquote">
<p>Say <span class="math inline">\(W\)</span> has <span class="math inline">\(q\)</span>-dimensional normal distribution with mean vector <span class="math inline">\(\mu\)</span> and independent components having variance <span class="math inline">\(\sigma^2\)</span>, each. We have a random sample of size <span class="math inline">\(n\)</span>, i.e.&nbsp;<span class="math inline">\(n\)</span> independent observations on <span class="math inline">\(W\)</span>. Then if <span class="math inline">\(q \geq 3\)</span>, in terms of Mean Squared Estimation Error, the best estimator of <span class="math inline">\(\mu\)</span> is NOT the sample mean <span class="math inline">\(\bar{W}\)</span>. Instead, it’s a shrunken version of <span class="math inline">\(\bar{W}\)</span>,</p>
<p><span id="eq-jamesstein"><span class="math display">\[
\left (
1 - \frac{(q-2) \sigma^2/n}{||\bar{W}||^2}
\right )
\bar{W}
\tag{10.2}\]</span></span></p>
</blockquote>
<p>The quantity within the parentheses is typically smaller than 1, giving us the shrinkage property. Note, though, that with larger <span class="math inline">\(n\)</span>, the amount of shrinkage is minor.</p>
<p>In the case of linear regression, a version of shrinkage works there too, with <span class="math inline">\(q\)</span> being the number of columns in the <span class="math inline">\(A\)</span> matrix.</p>
<p>So, let’s view the issue of shrinkage more formally, first for ridge and later for the LASSO.</p>
<section id="shrinkage-through-length-penalization" class="level3" data-number="10.9.1">
<h3 data-number="10.9.1" class="anchored" data-anchor-id="shrinkage-through-length-penalization"><span class="header-section-number">10.9.1</span> Shrinkage through length penalization</h3>
<p>Say instead of minimizing <a href="Ch3.html#eq-matrixss" class="quarto-xref">Equation&nbsp;<span>5.3</span></a>, we minimize</p>
<p><span id="eq-matrixssridge"><span class="math display">\[
(S - Ab)'(S - Ab) + \lambda ||b||^2
\tag{10.3}\]</span></span></p>
<p>We say that we <em>penalize</em> large values of <span class="math inline">\(b\)</span>, an indirect way of pursuing shrinkage. Now take the derivative and set to 0:</p>
<p><span id="eq-ridgederiv"><span class="math display">\[
0 = A'(S-Ab) + \lambda b
\tag{10.4}\]</span></span></p>
<p>i.e.</p>
<p><span class="math display">\[
(A'A + \lambda I) b = A'S
\]</span></p>
<p>and thus</p>
<p><span class="math display">\[
\widehat{\beta} = (A'A+\lambda I)^{-1} A'S
\]</span></p>
<p>It’s ridge! Exactly what we had in <a href="#eq-ridge" class="quarto-xref">Equation&nbsp;<span>10.1</span></a>. So ridge, originally motivated by “almost singular” settings, also turns out to be justified as a shrinkage estimator..</p>
</section>
<section id="shrinkage-through-length-limitation" class="level3" data-number="10.9.2">
<h3 data-number="10.9.2" class="anchored" data-anchor-id="shrinkage-through-length-limitation"><span class="header-section-number">10.9.2</span> Shrinkage through length limitation</h3>
<p>Instead of penalizing <span class="math inline">\(||b||\)</span>, we could simply constrain it, i.e.&nbsp;we could set our optimization problem to:</p>
<blockquote class="blockquote">
<p>minimize <span class="math inline">\((S-Ab)'(S-Ab)\)</span>, subject to the constraint <span class="math inline">\(||b||^2 \leq \gamma\)</span></p>
</blockquote>
<p>We say that this new formulation is the <em>dual</em> of the first one. One can show that they are typically equivalent.</p>
</section>
</section>
<section id="the-lasso" class="level2" data-number="10.10">
<h2 data-number="10.10" class="anchored" data-anchor-id="the-lasso"><span class="header-section-number">10.10</span> The LASSO</h2>
<p>The LASSO (Least Absolute Shrinkage and Selection Operator) was developed by Robert Tibshirani in 1996, following earlier work by Leo Breiman. It takes <span class="math inline">\(\widehat{\beta}\)</span> to be the value of <span class="math inline">\(b\)</span> that minimizes</p>
<p><span id="eq-matrixsslasso"><span class="math display">\[
(S - Ab)'(S - Ab) + \lambda ||b||_1
\tag{10.5}\]</span></span></p>
<p>where the “l1 norm” is</p>
<p><span class="math display">\[
||b|| = \sum_{i=1}^p |b_i|
\]</span></p>
<p>We will write our original norm as <span class="math inline">\(||b||_2\)</span>.</p>
<p>This is a seemingly minor change, but with important implications. What Breiman and Tibshirani were trying to do was to obtain a <em>sparse</em> <span class="math inline">\(\widehat{\beta}\)</span> , i.e.&nbsp;a solution with lots of 0s, thereby providing a method for predictor variable selection. This is important because so-called “parsimonious” prediction models are desirable.</p>
<section id="properties" class="level3" data-number="10.10.1">
<h3 data-number="10.10.1" class="anchored" data-anchor-id="properties"><span class="header-section-number">10.10.1</span> Properties</h3>
<p>To that end, as noted above, it can be shown that, under some technical conditions, that the ridge solution minimizes</p>
<p><span class="math display">\[
(S - Ab)'(S - Ab)
\]</span></p>
<p>subject to the constraint</p>
<p><span class="math display">\[
||b||_2 \leq \gamma
\]</span></p>
<p>while in the LASSO case the constraint is</p>
<p><span class="math display">\[
||b||_1 \leq \gamma
\]</span></p>
<p>As with <span class="math inline">\(\lambda\)</span> in the original formulation, <span class="math inline">\(\gamma\)</span> is a positive number chosen by the analyst.</p>
</section>
</section>
<section id="sec-dimred" class="level2 page-columns page-full" data-number="10.11">
<h2 data-number="10.11" class="anchored" data-anchor-id="sec-dimred"><span class="header-section-number">10.11</span> Ridge vs.&nbsp;LASSO for Dimension Reduction</h2>
<p>Today’s large datasets being so common, we need a way to “cut things down to size,” i.e.&nbsp;<em>dimension reduction</em>, aimed at reducing the number of predictor variables. This is done both for the sake of simplicity and to avoid <em>overfitting</em>, in which fitting an overly complex model can reduce predictive power.</p>
<section id="geometric-view" class="level3 page-columns page-full" data-number="10.11.1">
<h3 data-number="10.11.1" class="anchored" data-anchor-id="geometric-view"><span class="header-section-number">10.11.1</span> Geometric view</h3>
<p>Comparison between the ridge and LASSO concepts is often done via this graph depicting the LASSO setting:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="LASSO_gray.png" class="img-fluid figure-img"></p>
<figcaption>LASSO sparsity</figcaption>
</figure>
</div>
<p>Here <span class="math inline">\(p = 2\)</span>, with <span class="math inline">\(b = (b_1,b_2)'\)</span>. The horizontal and vertical axes represent <span class="math inline">\(b_1\)</span> and <span class="math inline">\(b_2\)</span>. Which point in the graph will be the LASSO solution? Reason as follows.</p>
<ul>
<li><p>The constraint <span class="math inline">\(||b||_1 \leq \gamma\)</span> then takes the form of a diamond, with corners at <span class="math inline">\((\gamma,0)\)</span>, <span class="math inline">\((0,\gamma)\)</span>, <span class="math inline">\((-\gamma,0)\)</span> and <span class="math inline">\((0,-\gamma)\)</span>. The constraint <span class="math inline">\(||b||_1 \leq \gamma\)</span> requires us to choose a point <span class="math inline">\(b\)</span> somewhere in the diamond, including the boundary.</p></li>
<li><p>The concentric ellipses depict the values of <span class="math inline">\(c(b) = (S - Ab)'(S -
Ab)\)</span>, as follows.</p>
<ul>
<li><p>Consider one particular value of <span class="math inline">\(c(b)\)</span>, say 1.68.</p></li>
<li><p>Many different points <span class="math inline">\(b\)</span> in the graph will have <span class="math inline">\(c(b) = 1.68\)</span>; in fact, the locus of all such points is an ellipse.</p></li>
<li><p>There is one ellipse for each possible value of <span class="math inline">\(c(b)\)</span>. So, there are infinitely many ellipses, though only two are shown here.</p></li>
<li><p>Larger values of <span class="math inline">\(c(b)\)</span> yield larger ellipses.</p></li>
<li><p>By the way, the common center of these ellipses is the ordinary (i.e.&nbsp;non-shrunken) least squares (OLS) solution <span class="math inline">\(b_{OLS}\)</span>, and the smallest ellipse has c(b) equal to the OLS sum of squares.</p></li>
</ul></li>
<li><p>On the one hand, we want to choose a <span class="math inline">\(b\)</span> for which <span class="math inline">\(c(b)\)</span> – our total squared prediction error – is small, thus a smaller ellipse.</p></li>
<li><p>But on the other hand, we need at least one point on the ellipse to be in common with the diamond.</p></li>
<li><p>The solution is then a point <span class="math inline">\(b\)</span> in which the ellipse just barely touches the diamond.</p></li>
<li><p>Picture in your mind an ellipse, say the inner one in the graph, growing larger and larger, while retaining the same center and orientation, until it hits the diamond. That is the outer ellipse, which indeed hits the diamond at one of the corners.</p></li>
<li><p>Note that each of the four corners of the diamond represents a sparse solution. For instance, the point <span class="math inline">\((0,\gamma)\)</span> has <span class="math inline">\(b_2 = 0\)</span>.</p></li>
<li><p>Then picture other ellipses, at other centers with other orientations, and go through the same process in your mind’s eye. You will see that  typically the solution turns out to be one of the corners. Again, this is important because it gives us a sparse solution.</p></li>
</ul>
<div class="no-row-height column-margin column-container"><span class="">Of course, we are in just two dimensions here. With <span class="math inline">\(p = 10\)</span> predictor variables, the graph would be in 10 dimensions, beyond our human ability to picture. But we still would have a diamond in that space, with <span class="math inline">\(2p\)</span> corners etc.</span></div></section>
<section id="implication-for-dimension-reduction." class="level3" data-number="10.11.2">
<h3 data-number="10.11.2" class="anchored" data-anchor-id="implication-for-dimension-reduction."><span class="header-section-number">10.11.2</span> Implication for dimension reduction.</h3>
<p>The key point is that that “barely touching” point will be one of the four corners of the diamond, points at which either <span class="math inline">\(b_1 = 0\)</span> or <span class="math inline">\(b_2 = 0\)</span> – <em>hence a sparse solution</em>, meaning one in which many/most of the coefficients in the fitted model will be 0. This achieves the goal of dimension reduction.</p>
<p>Ridge will not produce a sparse solution. The diamond would now be a circle (not shown). The “barely touching point” will almost certainly will be at a place in which both <span class="math inline">\(b_1\)</span> and <span class="math inline">\(b_2\)</span> are nonzero. Hence no sparsity.</p>
</section>
<section id="avoidance-of-overfitting-without-dimension-reduction" class="level3" data-number="10.11.3">
<h3 data-number="10.11.3" class="anchored" data-anchor-id="avoidance-of-overfitting-without-dimension-reduction"><span class="header-section-number">10.11.3</span> Avoidance of overfitting <em>without</em> dimension reduction</h3>
<p>As we’ve seen, both ridge and LASSO reduce the size of the <span class="math inline">\(\widehat{\beta}\)</span> vector of estimated coefficients. Smaller quantities have smaller statistical variances, hence a guard against overfitting. So, ridge can be employed as an approach to the overfitting problem, even though it does not provide a sparse solution.</p>
<p>Moreover, in some settings, it may be desirable to keep all predictors, as seen in the next section.</p>
</section>
</section>
<section id="sec-useall" class="level2 page-columns page-full" data-number="10.12">
<h2 data-number="10.12" class="anchored" data-anchor-id="sec-useall"><span class="header-section-number">10.12</span> Example: NYC Taxi Data</h2>
<p>The purpose of this data is to predict trip time in the New York City taxi system. The <span class="math inline">\(\textbf{qeML}\)</span> package includes a 10,000-row subset.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(qeML)</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(nyctaxi)</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(nyctaxi)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>        trip_distance PULocationID DOLocationID tripTime DayOfWeek
2969561          1.37          236           43      598         1
7301968          0.71          238          238      224         4
3556729          2.80          100          263      761         3
7309631          2.62          161          249      888         4
3893911          1.20          236          163      648         5
4108506          2.40          161          164      977         5</code></pre>
</div>
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="fu">dim</span>(nyctaxi)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 10000     5</code></pre>
</div>
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="fu">length</span>(<span class="fu">unique</span>(nyctaxi<span class="sc">$</span>PULocationID))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 143</code></pre>
</div>
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="fu">length</span>(<span class="fu">unique</span>(nyctaxi<span class="sc">$</span>DOLocationID))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 205</code></pre>
</div>
</div>
<div class="page-columns page-full"><p>If we fit, say, a linear model, <span class="math inline">\(\textbf{lm}\)</span> will form a dummy variable for each of the pickup and dropoff locations. Thus we will have <span class="math inline">\(p =
1+143+205+1 = 350\)</span>. An old rule of thumb says that if we have <span class="math inline">\(p\)</span> predictors and <span class="math inline">\(n\)</span> data points, we should keep <span class="math inline">\(p &lt; \sqrt{n}\)</span> to avoid overfitting. As we will see in a later chapter, these days that rule is being questioned, but  it is still useful. Since here we have <span class="math inline">\(\sqrt{n} = 100\)</span>, there is a strong suggestion that we do some dimension reduction.</p><div class="no-row-height column-margin column-container"><span class="">The problem would be even worse if we add pickup/dropoff location interaction variables, basically products of the pickup and dropoff dummy variables.</span></div></div>
<p>Thus we either should delete some of the pickup and dropoff variables, or use all of them but temper the fit using ridge. The latter may be more attractive, as riders would like a time estimate for their particular pickup and dropoff locations.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(glmnet)</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>nycwide <span class="ot">&lt;-</span> <span class="fu">factorsToDummies</span>(nyctaxi[,<span class="sc">-</span><span class="dv">1</span>])</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>glmOut <span class="ot">&lt;-</span> <span class="fu">cv.glmnet</span>(<span class="at">x=</span>nycwide,<span class="at">y=</span>nyctaxi[,<span class="dv">1</span>],<span class="at">alpha=</span><span class="dv">0</span>)</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>glmOut</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:  cv.glmnet(x = nycwide, y = nyctaxi[, 1], alpha = 0) 

Measure: Mean-Squared Error 

    Lambda Index Measure    SE Nonzero
min 0.3002   100   2.941 0.162     356
1se 1.0063    87   3.101 0.190     356</code></pre>
</div>
</div>
<p>The best <span class="math inline">\(\lambda\)</span> value was found to be 0.3002. But as noted above, we might use the more conservative value, 1.0063, to try to avoid p-hacking.</p>
</section>
<section id="sec-iter" class="level2 page-columns page-full" data-number="10.13">
<h2 data-number="10.13" class="anchored" data-anchor-id="sec-iter"><span class="header-section-number">10.13</span> Iterative Calculation</h2>
<p>An advantage of ridge over LASSO and other l1 shrinkage estimators is that the former has an explicit (we say <em>closed-form</em>) solution, which is not the case for the LASSO. In fact, as will be seen often in this book, many algorithms in statistics/machine learning lack closed-form solutions, in which case one must resort to <em>iterative</em> computation.</p>
<p>These means we make a series of guesses as the to value of the desired quantity, hopefully each more accurate than the last, eventually settling on a final guess.</p>
<p>This may or may not work well. Here are some of the major issues/perils:</p>
<ul>
<li><p>initial guess</p></li>
<li><p>updating method</p></li>
<li><p>learning rate</p></li>
<li><p>convergence</p></li>
<li><p>presence or lack of calculus derivatives</p></li>
</ul>
<p>The basic idea is to first (somehow) make some guess as to the value of the desired quantity, say <span class="math inline">\(\widehat{\beta}\)</span> in the LASSO. The algorithm crunches this to make a new, updated guess, hopefully one that is more accurate than the first. One then updates the new guess, continuing this process until, hopefully, it <em>converges</em>, meaning that it no longer changes much from one iteration to the next. The current guess is then deemed to be the correct value.</p>
<p>The case of computation of the LASSO is further complicated by its being based on the <span class="math inline">\(l_1\)</span> norm, which in turn uses absolute values, i.e.&nbsp;<span class="math inline">\(|x|\)</span>. These have no derivative in the calculus sense, say as used in <a href="#eq-ridgederiv" class="quarto-xref">Equation&nbsp;<span>10.4</span></a> for ridge. This is a problem because many iterative methods are based on derivatives, as follows.</p>
<div class="page-columns page-full"><p>Say we have a function <span class="math inline">\(f\)</span> whose root <span class="math inline">\(r\)</span> is of interest to us. We might make a series of guesses for <span class="math inline">\(r\)</span> by considering the derivative <span class="math inline">\(f'\)</span>. This is illustrated in the figure below. Unknown to us, <span class="math inline">\(r = 2\)</span>. Our current guess is <span class="math inline">\(x = 3\)</span>. We draw <span class="math inline">\(f'\)</span>, i.e.&nbsp;the tangent line to the curve at our current guess, and temporarily pretend that the line is the curve. We thus compute the root for the line, which is seen here to be near 2.0, and then take this tangent root as our updated guess for the root of the curve.</p><div class="no-row-height column-margin column-container"><span class="">Our quest for a root may arise for instance in a minimization problem, where we set a derivative to 0 and solve for that root.</span></div></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="NewtonRaph.png" class="img-fluid figure-img"></p>
<figcaption>root hunting</figcaption>
</figure>
</div>
<div class="page-columns page-full"><p>Some machine learning algorithm have a parameter called the <em>learning rate</em>, which is motivated by a concern that the process may overshoot the root, or converge to the wrong root. . A smaller learning rate value directs the algorithm to take smaller steps in generating new guesses. In this case, we might go only partway to the tangent root. On the one hand, this can slow the computation but on the other, we may be less likely to overshoot the true value. On the other hand, if we are concerned about settling on the wrong root, we might set a large value for the learning rate.</p><div class="no-row-height column-margin column-container"><span class="">Again, the desired root may arise in a minimum/maximum problem. There could be many local mimima/maxima, which we use to avoid.</span></div></div>
<p>At any rate, if the quantity <span class="math inline">\(f\)</span> that we are working with does not have a derivative, our work is extra difficult.</p>
<p>At first one may think that such internal details of computation need not concern the end user of the software. But the fact is that often an algorithm will fail to converge, and the user will need to get more directly involved, say in trying a different value for the initial guess.</p>
<p>So, if say <strong>glmnet</strong> fails to converge, what can be done? For example, in <strong>glmnet</strong>, the argument <strong>thresh</strong> defines what we meant by our phrasing “no longer changes much”; we can decrease or even increase that value. One can make sure to center and scale the X data. Tweaking other parameters may help as well, such as changing the updating method. (In fitting neural network models, there are actually many different methods to choose from.) But in the end, there are no magic solutions. It may well be that one’s basic model is flawed.</p>
</section>
<section id="the-kernel-trick-and-kernel-ridge-regression" class="level2 page-columns page-full" data-number="10.14">
<h2 data-number="10.14" class="anchored" data-anchor-id="the-kernel-trick-and-kernel-ridge-regression"><span class="header-section-number">10.14</span> The Kernel Trick and Kernel Ridge Regression</h2>
<p>How about do you see a named trick in a math book? Well, there is indeed one here, one that is of great practical value.</p>
<section id="polynomial-regression" class="level3 page-columns page-full" data-number="10.14.1">
<h3 data-number="10.14.1" class="anchored" data-anchor-id="polynomial-regression"><span class="header-section-number">10.14.1</span> Polynomial Regression</h3>
<p>The world is not linear, not even approximately so. Thus early in the development of statistics, analysts started using polynomial models. To predict human weight from height, for instance, one might fit the model</p>
<p><span class="math display">\[
E(\textrm{weight} | \textrm{height}) =
\beta_0 + \beta_1 \textrm{height} + \beta_2 \textrm{height}^2
\]</span></p>
<p>The key point is that <em>this is still a linear model</em>. Though it is a nonlinear function of height, it is linear in the <span class="math inline">\(\beta_i\)</span>. If say, we multiple all the <span class="math inline">\(\beta_i\)</span> by 2, the value of the above expression is doubled. In <a href="Ch3.html#eq-Aex" class="quarto-xref">Equation&nbsp;<span>5.2</span></a>, we now would tack on a column consisting of the <span class="math inline">\(V_i^2\)</span>, but <span class="math inline">\(E(C|V)\)</span> would still be a linear expression in <span class="math inline">\(\beta\)</span>:</p>
<p><span class="math display">\[
E(C | V) = A \beta =
\beta_0 + \beta_1 \textrm{V} + \beta_2 \textrm{V}^2
\]</span></p>
<div class="page-columns page-full"><p>Nice, but the size of our model grows rapidly. Say we predict weight from not only height but also age. Then our matrix <span class="math inline">\(A\)</span> will have columns not only for height and height-squared, but also age and age-squared, as well as height times age. In my machine learning book , there is an example in which <span class="math inline">\(A\)</span> originally had 54 columns, but with a polynomial model of degree 2, the number of columns grew to 1,564!</p><div class="no-row-height column-margin column-container"><span class="">N. Matloff, <em>The Art of Machine Learning</em>, NSP, 2023</span></div></div>
<p>This is especially problematic in the case <span class="math inline">\(p &gt; n\)</span>, which is increasingly common these days, and motivates the following material.</p>
</section>
<section id="the-kernel-trick" class="level3" data-number="10.14.2">
<h3 data-number="10.14.2" class="anchored" data-anchor-id="the-kernel-trick"><span class="header-section-number">10.14.2</span> The Kernel Trick</h3>
<p>We will first introduce a computational shortcut, and then the Kernel Trick itself. Our context will be ridge regression.</p>
<p>Using some algebraic manipulation, <a href="#eq-ridge" class="quarto-xref">Equation&nbsp;<span>10.1</span></a> can be shown to be equivalent to</p>
<p><span class="math display">\[
\widehat{\beta} = A' (AA' + \lambda I)^{-1} S
\]</span></p>
<p>Note that the product <span class="math inline">\(AA'\)</span> will be <span class="math inline">\(n \times n\)</span>, as opposed to our original <span class="math inline">\(p \times p\)</span> product <span class="math inline">\(A'A\)</span>. With <span class="math inline">\(p &gt; n\)</span>, this change means a large saving in memory storage space and computation time.</p>
<p>But we reap even larger benefit by looking at fitted or predicted values. Let <span class="math inline">\(x_{new}\)</span> be a matrix of “X” values at which we wish to predict “Y,” in the same format as <span class="math inline">\(X\)</span>. Then the fitted/predicted values are</p>
<p><span class="math display">\[
x_{new} \widehat{\beta}
= x_{new} A' (AA' + \lambda I)^{-1}
= [x_{new} A'] [(AA' + \lambda I)^{-1}]
\]</span></p>
<p>What is so special about this expression, or more specifically, the two bracketed expressions? The answer is that every major computation here is a “dot product:</p>
<ul>
<li><p>In <span class="math inline">\(x_{new} A'\)</span>, we are taking dot products of rows of <span class="math inline">\(x_{new}\)</span> and columns of <span class="math inline">\(A'\)</span>.</p></li>
<li><p>In <span class="math inline">\(AA'\)</span>, we are taking dot products of rows of <span class="math inline">\(A\)</span> and columns of <span class="math inline">\(A'\)</span>.</p></li>
</ul>
<p>The idea of the Kernel Trick is to replace dot products by kernel computations. In computing <span class="math inline">\(AA'\)</span>, for example, denote row <span class="math inline">\(i\)</span> of <span class="math inline">\(A\)</span> by <span class="math inline">\(a_i\)</span>. Then we replace the expression for the row <span class="math inline">\(i\)</span>, column <span class="math inline">\(j\)</span> element of <span class="math inline">\(AA'\)</span>,</p>
<p><span id="eq-aiaj"><span class="math display">\[
a_i' a_j
\tag{10.6}\]</span></span></p>
<p>by</p>
<p><span class="math display">\[
k(a_i,a_j)
\]</span></p>
<p>Here <span class="math inline">\(k\)</span> is a <em>kernel function</em> satisfying</p>
<ul>
<li><p><span class="math inline">\(k\)</span> is a symmetric function in its arguments, and</p></li>
<li><p><span class="math inline">\(k\)</span> is <em>nonnegative definite</em>.</p>
<p>The latter is a generalization of nonnegative definite matrices. If we have a set of vectors <span class="math inline">\(u_1,...,u_r\)</span> and form the matrix having its row <span class="math inline">\(i\)</span>, column <span class="math inline">\(j\)</span> value equal to <span class="math inline">\(k(u_i,u_j)\)</span>, then the matrix is required to nonnegative positive definite.</p></li>
</ul>
<p>What does that give us? Suppose we wish to do polynomial regression of degree 2. As noted above, we could add the appropriate columns to the <span class="math inline">\(A\)</span> matrix, thus applying the transformation (e.g.&nbsp;for the case <span class="math inline">\(p = 2\)</span>),</p>
<p><span id="eq-origquadphi"><span class="math display">\[
x \rightarrow \phi(x) = (x_1,x_1^2,x_2,x_2^2,x_1 x_2)
\tag{10.7}\]</span></span></p>
<p>We say that <span class="math inline">\(\phi\)</span> <em>lifts</em> us from our <em>ambient</em> space (i.e.&nbsp;original space, dimension <span class="math inline">\(p\)</span>) to a desired higher-dimensional space, the <em>latent</em> space.</p>
<p>But again, this would increase the dimensionality, going from 2 to 5 in this <span class="math inline">\(p = 2\)</span> case, but the main point for now is that we still would be computing dot products. For instance, the new value of <a href="#eq-aiaj" class="quarto-xref">Equation&nbsp;<span>10.6</span></a> would now be</p>
<p><span class="math display">\[
\phi(a_i)' \phi(a_j)
\]</span></p>
<p>The Kernel Trick involves finding a kernel <span class="math inline">\(k\)</span> for which</p>
<p><span class="math display">\[
k(u,v) = \phi(u)' \phi(v)
\]</span></p>
<p>It turns out that such a kernel is</p>
<p><span id="eq-polykern"><span class="math display">\[
k(u,v) = (c + u'v)^d
\tag{10.8}\]</span></span></p>
<p>for polynomials of degree <span class="math inline">\(d\)</span>.</p>
<p>In other words:</p>
<blockquote class="blockquote">
<p>The Kernel Trick enables us to stay in the ambient space yet still achieve the nonlinearity of the lifted space.</p>
</blockquote>
<p>In fact, although it turns out that <a href="#eq-polykern" class="quarto-xref">Equation&nbsp;<span>10.8</span></a> corresponds to</p>
<p><span class="math display">\[
\phi(t) = (t_1^2,  t_2^2, \sqrt{2} ~ t_1 t_2,
\sqrt{2c} ~ t_1,
\sqrt{2c} ~ t_2, c)
\]</span></p>
<p>not only do we avoid computing <span class="math inline">\(\phi(t)\)</span>, we don’t even need to know its form.</p>
<p>Note again that all this worked only because our quantities of interest here consist of dot products.</p>
</section>
<section id="kernel-ridge-regression-the-code" class="level3" data-number="10.14.3">
<h3 data-number="10.14.3" class="anchored" data-anchor-id="kernel-ridge-regression-the-code"><span class="header-section-number">10.14.3</span> Kernel Ridge Regression: the Code</h3>
<div class="sourceCode" id="cb36"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a><span class="co"># kernel ridge regression</span></span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a><span class="co"># fits 'data' with KRR, ridge parameter 'lamb' and kernel 'kern', and</span></span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a><span class="co"># then predicts "Y" from 'newX'</span></span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a>krr <span class="ot">&lt;-</span> <span class="cf">function</span>(data,yName,lamb,kern,newX)</span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a>   <span class="cf">if</span> (<span class="sc">!</span><span class="fu">allNumeric</span>(data)) <span class="fu">stop</span>(<span class="st">"'data' must be all numeric"</span>)</span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-11"><a href="#cb36-11" aria-hidden="true" tabindex="-1"></a>   ycol <span class="ot">&lt;-</span> <span class="fu">which</span>(<span class="fu">names</span>(data) <span class="sc">==</span> yName)</span>
<span id="cb36-12"><a href="#cb36-12" aria-hidden="true" tabindex="-1"></a>   data <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(data)</span>
<span id="cb36-13"><a href="#cb36-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-14"><a href="#cb36-14" aria-hidden="true" tabindex="-1"></a>   x <span class="ot">&lt;-</span> data[,<span class="sc">-</span>ycol,drop<span class="ot">=</span><span class="cn">FALSE</span>]</span>
<span id="cb36-15"><a href="#cb36-15" aria-hidden="true" tabindex="-1"></a>   x <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="dv">1</span>,x)</span>
<span id="cb36-16"><a href="#cb36-16" aria-hidden="true" tabindex="-1"></a>   n <span class="ot">&lt;-</span> <span class="fu">nrow</span>(x)</span>
<span id="cb36-17"><a href="#cb36-17" aria-hidden="true" tabindex="-1"></a>   p <span class="ot">&lt;-</span> <span class="fu">ncol</span>(x)</span>
<span id="cb36-18"><a href="#cb36-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-19"><a href="#cb36-19" aria-hidden="true" tabindex="-1"></a>   newX <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(newX)</span>
<span id="cb36-20"><a href="#cb36-20" aria-hidden="true" tabindex="-1"></a>   <span class="cf">if</span> (<span class="fu">ncol</span>(newX) <span class="sc">==</span> <span class="dv">1</span>) newX <span class="ot">&lt;-</span> <span class="fu">t</span>(newX)</span>
<span id="cb36-21"><a href="#cb36-21" aria-hidden="true" tabindex="-1"></a>   newX <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="dv">1</span>,newX)</span>
<span id="cb36-22"><a href="#cb36-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-23"><a href="#cb36-23" aria-hidden="true" tabindex="-1"></a>   part1 <span class="ot">&lt;-</span> <span class="fu">kernAB</span>(newX,<span class="fu">t</span>(x),kern)</span>
<span id="cb36-24"><a href="#cb36-24" aria-hidden="true" tabindex="-1"></a>   part2 <span class="ot">&lt;-</span> <span class="fu">kernAB</span>(x,<span class="fu">t</span>(x),kern)</span>
<span id="cb36-25"><a href="#cb36-25" aria-hidden="true" tabindex="-1"></a>   part2 <span class="ot">&lt;-</span> <span class="fu">solve</span>(part2 <span class="sc">+</span> lamb <span class="sc">*</span> <span class="fu">diag</span>(n)) <span class="sc">%*%</span> y</span>
<span id="cb36-26"><a href="#cb36-26" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb36-27"><a href="#cb36-27" aria-hidden="true" tabindex="-1"></a>   predsKRR <span class="ot">&lt;-</span> part1 <span class="sc">%*%</span> part2</span>
<span id="cb36-28"><a href="#cb36-28" aria-hidden="true" tabindex="-1"></a>   <span class="fu">return</span>(<span class="fu">cbind</span>(predsKRR))</span>
<span id="cb36-29"><a href="#cb36-29" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb36-30"><a href="#cb36-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-31"><a href="#cb36-31" aria-hidden="true" tabindex="-1"></a><span class="co"># finds the matrix product ab, but with the kernel k evaluated at each</span></span>
<span id="cb36-32"><a href="#cb36-32" aria-hidden="true" tabindex="-1"></a><span class="co"># entry</span></span>
<span id="cb36-33"><a href="#cb36-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-34"><a href="#cb36-34" aria-hidden="true" tabindex="-1"></a>kernAB <span class="ot">&lt;-</span> <span class="cf">function</span>(a,b,k)</span>
<span id="cb36-35"><a href="#cb36-35" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb36-36"><a href="#cb36-36" aria-hidden="true" tabindex="-1"></a>   ab <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="at">nrow=</span><span class="fu">nrow</span>(a),<span class="at">ncol=</span><span class="fu">ncol</span>(b))</span>
<span id="cb36-37"><a href="#cb36-37" aria-hidden="true" tabindex="-1"></a>   <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(a)) {</span>
<span id="cb36-38"><a href="#cb36-38" aria-hidden="true" tabindex="-1"></a>      arow <span class="ot">&lt;-</span> a[i,]</span>
<span id="cb36-39"><a href="#cb36-39" aria-hidden="true" tabindex="-1"></a>      <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">ncol</span>(b)) {</span>
<span id="cb36-40"><a href="#cb36-40" aria-hidden="true" tabindex="-1"></a>         ab[i,j] <span class="ot">&lt;-</span> <span class="fu">k</span>(arow,b[,j])</span>
<span id="cb36-41"><a href="#cb36-41" aria-hidden="true" tabindex="-1"></a>      }</span>
<span id="cb36-42"><a href="#cb36-42" aria-hidden="true" tabindex="-1"></a>   }</span>
<span id="cb36-43"><a href="#cb36-43" aria-hidden="true" tabindex="-1"></a>   ab</span>
<span id="cb36-44"><a href="#cb36-44" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="a-warning" class="level2" data-number="10.15">
<h2 data-number="10.15" class="anchored" data-anchor-id="a-warning"><span class="header-section-number">10.15</span> A Warning</h2>
<p>Many statistical quantities now have <em>regularized</em>, i.e.&nbsp;shrunken versions. It is also standard practice in neural networks. This may be quite helpful in prediction contexts. However, note the following:</p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
No Statistical Inference on Shrinkage Estimators
</div>
</div>
<div class="callout-body-container callout-body">
<p>Shrinkage produces a bias, of unknown size. Thus classical statistical inference (confidence intervals, hypothesis tests), e.g.&nbsp;those based on <a href="Ch3.html#eq-varcovar" class="quarto-xref">Equation&nbsp;<span>5.9</span></a> for linear models, is not possible.</p>
</div>
</div>
</section>
<section id="your-turn" class="level2" data-number="10.16">
<h2 data-number="10.16" class="anchored" data-anchor-id="your-turn"><span class="header-section-number">10.16</span> Your Turn</h2>
<p>❄️ <strong>Your Turn:</strong> Show that <span class="math inline">\(A_{new}\)</span> in <a href="#sec-matform" class="quarto-xref"><span>Section 10.3.2</span></a> is of full rank, <span class="math inline">\(p\)</span>.</p>
<p>❄️ <strong>Your Turn:</strong> In <a href="#sec-useall" class="quarto-xref"><span>Section 10.12</span></a>, it was pointed out that in some settings we may prefer to retain all of our predictor variables, rather than do dimension reduction, thus preferring ridge to LASSO. But we might pay a price for that preference, in that the LASSO may actually give us better predictive power. Write an R function to investigate this, with call form</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="fu">compareRidgeLASSO</span>(data,yName)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>where <strong>data</strong> and <strong>yName</strong> are in the format of the predictive <span class="math inline">\(qeML\)</span> functions, and the minimum Mean Squared Prediction Error is returned for both algorithms. Try your function on some of our datasets, or others.</p>
<p>❄️ <strong>Your Turn:</strong> The LASSO will tend to produce solutions with lesser sparsity if the dataset is large. Write an R function to illustrate this, with call form</p>
<div class="sourceCode" id="cb38"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="fu">dependN</span>(data,yName,<span class="at">n=</span><span class="fu">seq</span>(<span class="dv">1</span>,<span class="fu">nrow</span>(data),<span class="dv">100</span>,<span class="at">nReps=</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>where: where <strong>data</strong> and <strong>yName</strong> are in the format of the predictive <span class="math inline">\(qeML\)</span> functions; the LASSO is applied to <strong>n</strong> randomly chosen rows of <strong>data</strong>; and <strong>nReps</strong> is the number of replicates to run at each value of <strong>n</strong>. The function will compute the number of nonzero elements in the <span class="math inline">\(\widehat{\beta}\)</span> chosen by cross-validation in <strong>cv.glmnet</strong>. Try your code on a few datasets.</p>
<p>❄️ <strong>Your Turn:</strong> Consider a generalization of ridge regression, in which we find</p>
<p><span class="math display">\[
\textrm{argmin}_b ~
||{S} - {A} b||^2 + ||{D} b||^2
\]</span></p>
<p>for a diagonal matrix <span class="math inline">\(D\)</span>. The idea is to allow different shrinkage parameters for different predictor variables. Show that</p>
<p><span class="math display">\[
b =
{[{A}' {A} + {D}^2]}^{-1}
{A}' {S}
\]</span></p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./Ch5bb.html" class="pagination-link" aria-label="Four Fundamental Spaces">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Four Fundamental Spaces</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./Ch6a.html" class="pagination-link" aria-label="Eigenanalysis">
        <span class="nav-page-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Eigenanalysis</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>