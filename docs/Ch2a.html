<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.433">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Powered by Linear Algebra - 4&nbsp; Covariance Matrices, MV Normal Distribution, Delta Method</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./Ch3.html" rel="next">
<link href="./Ch2.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./Ch2a.html"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Covariance Matrices, MV Normal Distribution, Delta Method</span></a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Powered by Linear Algebra</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">index.html</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./preface.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Matrices and Vectors</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Matrix Inverse</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch2a.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Covariance Matrices, MV Normal Distribution, Delta Method</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Linear Statistical Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Matrix Rank</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch5a.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Vector Spaces</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch5b.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Inner Product Spaces</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch5bb.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Four Fundamental Spaces</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch5c.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Shrinkage Estimators</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch6a.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Eigenanalysis</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch6b.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Principal Components</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch6c.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Singular Value Decomposition</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch6d.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Pseudoinverse and Double Descent</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch8a.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Neural Networks</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Contents</h2>
   
  <ul>
  <li><a href="#sec-cov" id="toc-sec-cov" class="nav-link active" data-scroll-target="#sec-cov"><span class="header-section-number">4.1</span> Covariance</a>
  <ul class="collapse">
  <li><a href="#scalar-covariance" id="toc-scalar-covariance" class="nav-link" data-scroll-target="#scalar-covariance"><span class="header-section-number">4.1.1</span> Scalar covariance</a></li>
  <li><a href="#covariance-matrices" id="toc-covariance-matrices" class="nav-link" data-scroll-target="#covariance-matrices"><span class="header-section-number">4.1.2</span> Covariance matrices</a></li>
  <li><a href="#cross-covariance" id="toc-cross-covariance" class="nav-link" data-scroll-target="#cross-covariance"><span class="header-section-number">4.1.3</span> Cross-covariance</a></li>
  </ul></li>
  <li><a href="#sec-mvn" id="toc-sec-mvn" class="nav-link" data-scroll-target="#sec-mvn"><span class="header-section-number">4.2</span> The Multivariate Normal Distribution Family</a>
  <ul class="collapse">
  <li><a href="#example-k-2" id="toc-example-k-2" class="nav-link" data-scroll-target="#example-k-2"><span class="header-section-number">4.2.1</span> Example: k = 2</a></li>
  <li><a href="#general-form" id="toc-general-form" class="nav-link" data-scroll-target="#general-form"><span class="header-section-number">4.2.2</span> General form</a></li>
  <li><a href="#properties" id="toc-properties" class="nav-link" data-scroll-target="#properties"><span class="header-section-number">4.2.3</span> Properties</a></li>
  </ul></li>
  <li><a href="#multinomial-random-vectors-have-approximate-multivariate-normal-distributions" id="toc-multinomial-random-vectors-have-approximate-multivariate-normal-distributions" class="nav-link" data-scroll-target="#multinomial-random-vectors-have-approximate-multivariate-normal-distributions"><span class="header-section-number">4.3</span> Multinomial Random Vectors Have Approximate Multivariate Normal Distributions</a></li>
  <li><a href="#the-delta-method" id="toc-the-delta-method" class="nav-link" data-scroll-target="#the-delta-method"><span class="header-section-number">4.4</span> The Delta Method</a>
  <ul class="collapse">
  <li><a href="#review-confidence-intervals-standard-errors" id="toc-review-confidence-intervals-standard-errors" class="nav-link" data-scroll-target="#review-confidence-intervals-standard-errors"><span class="header-section-number">4.4.1</span> Review: confidence intervals, standard errors</a></li>
  <li><a href="#delta-method-motivating-example" id="toc-delta-method-motivating-example" class="nav-link" data-scroll-target="#delta-method-motivating-example"><span class="header-section-number">4.4.2</span> Delta method: motivating example</a></li>
  <li><a href="#use-of-the-central-limit-theorem" id="toc-use-of-the-central-limit-theorem" class="nav-link" data-scroll-target="#use-of-the-central-limit-theorem"><span class="header-section-number">4.4.3</span> Use of the Central Limit Theorem</a></li>
  <li><a href="#use-of-the-multivariate-central-limit-theorem" id="toc-use-of-the-multivariate-central-limit-theorem" class="nav-link" data-scroll-target="#use-of-the-multivariate-central-limit-theorem"><span class="header-section-number">4.4.4</span> Use of the Multivariate Central Limit Theorem</a></li>
  <li><a href="#example-ratio-of-two-means" id="toc-example-ratio-of-two-means" class="nav-link" data-scroll-target="#example-ratio-of-two-means"><span class="header-section-number">4.4.5</span> Example: ratio of two means</a></li>
  </ul></li>
  <li><a href="#example-iranian-churn-data" id="toc-example-iranian-churn-data" class="nav-link" data-scroll-target="#example-iranian-churn-data"><span class="header-section-number">4.5</span> Example: Iranian Churn Data</a></li>
  <li><a href="#example-motherdaughter-height-data" id="toc-example-motherdaughter-height-data" class="nav-link" data-scroll-target="#example-motherdaughter-height-data"><span class="header-section-number">4.6</span> Example: Mother/Daughter Height Data</a></li>
  <li><a href="#regarding-those-pesky-derivatives" id="toc-regarding-those-pesky-derivatives" class="nav-link" data-scroll-target="#regarding-those-pesky-derivatives"><span class="header-section-number">4.7</span> Regarding Those Pesky Derivatives</a></li>
  <li><a href="#your-turn" id="toc-your-turn" class="nav-link" data-scroll-target="#your-turn"><span class="header-section-number">4.8</span> Your Turn</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="sec-covar" class="quarto-section-identifier"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Covariance Matrices, MV Normal Distribution, Delta Method</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<div style="page-break-after: always;"></div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Goals of this chapter:
</div>
</div>
<div class="callout-body-container callout-body">
<p>A central entity in multivariate analysis is that of the <em>covariance matrix</em>. In this chapter we define the term, list its many properties, and show its role in the multivariate analog of the normal distribution family. We close the chapter with an application to the <em>delta method</em>, one of the most useful simple tools in statistics.</p>
</div>
</div>
<section id="sec-cov" class="level2 page-columns page-full" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="sec-cov"><span class="header-section-number">4.1</span> Covariance</h2>
<section id="scalar-covariance" class="level3 page-columns page-full" data-number="4.1.1">
<h3 data-number="4.1.1" class="anchored" data-anchor-id="scalar-covariance"><span class="header-section-number">4.1.1</span> Scalar covariance</h3>
<p>Recall first the notion in statistics of <em>covariance</em>: Given a pair of random variables <span class="math inline">\(U\)</span> and <span class="math inline">\(V\)</span>, their covariance is defined by</p>
<p><span class="math display">\[
Cov(U,V) = E[(U - EU)(V - EV)]
\]</span></p>
<p>Loosely speaking, this measures the degree to which the two random variables vary together. Consider for instance human height <span class="math inline">\(H\)</span> and <span class="math inline">\(W\)</span>. Taller people tend to also be heavier. Say we sample many people from a population. Most of those who are taller than average, i.e.&nbsp;<span class="math inline">\(H &gt; EH\)</span>, will also be heavier than average, <span class="math inline">\(W &gt; EW\)</span>, making <span class="math inline">\((H - EH)(W - EW) &gt; 0\)</span>. Similarly, shorter people tend to be lighter, i.e.&nbsp;we often have <span class="math inline">\(H &lt; EH\)</span> and <span class="math inline">\(W &lt; EW\)</span>, but then we still have <span class="math inline">\((H - EH)(W - EW) &gt; 0\)</span>. So, one way or the other, usually <span class="math inline">\((H - EH)(W - EW) &gt; 0\)</span>, and though there will be a number of exceptions, they will be rare enough so that</p>
<div class="page-columns page-full"><p><span class="math inline">\(E[(H - EH)(W - EW)] &gt; 0\)</span>.</p><div class="no-row-height column-margin column-container"><span class="">Of course, the <em>magnitude</em> of <span class="math inline">\((H - EH)(W - EW)\)</span> plays a role too.</span></div></div>
<p>In other words, <span class="math inline">\(Cov(H,W) &gt; 0\)</span>. Similarly if <span class="math inline">\(U\)</span> is often large when <span class="math inline">\(V\)</span> is small, and vice versa, we will likely have <span class="math inline">\(Cov(H,W) &lt; 0\)</span>.</p>
<p>If this sounds like correlation to you, then your hunch is correct. Covariance will indeed later lead to the concept of correlation, but that intuition will serve us now.</p>
<p>Note some properties of scalar covariance.</p>
<ul>
<li>Symmetry:</li>
</ul>
<p><span class="math display">\[
Cov(U,V) = Cov(V,U)
\]</span></p>
<ul>
<li><span class="math inline">\(Cov\)</span> is bilinear:</li>
</ul>
<p><span class="math display">\[
Cov(aU,bV) = ab Cov(U,V)
\]</span></p>
<ul>
<li>Variance as a special case:</li>
</ul>
<p><span class="math display">\[
Cov(U,U) = Var(U)
\]</span></p>
<ul>
<li>Cross-product term:</li>
</ul>
<p><span class="math display">\[
Var(U+V) = Var(U) + Var(V) + 2 Cov(U,V)
\]</span></p>
<ul>
<li>“Short cut” formula:</li>
</ul>
<p><span id="eq-covuv"><span class="math display">\[
Cov(U,V) = E(UV) - (EU) (EV)
\tag{4.1}\]</span></span></p>
</section>
<section id="covariance-matrices" class="level3 page-columns page-full" data-number="4.1.2">
<h3 data-number="4.1.2" class="anchored" data-anchor-id="covariance-matrices"><span class="header-section-number">4.1.2</span> Covariance matrices</h3>
<div class="page-columns page-full"><p>The relations between the various components of <span class="math inline">\(X\)</span> are often characterized by the <em>covariance matrix</em> of <span class="math inline">\(X\)</span>, whose entries consist of scalar covariances between pairs of components of a random vector.  It is defined as follows for a <span class="math inline">\(k\)</span>-component random vector <span class="math inline">\(X\)</span> . The covariance matrix, denoted by <span class="math inline">\(Cov(X)\)</span>, is a <span class="math inline">\(k \textrm{ x } k\)</span> matrix, and for <span class="math inline">\(1 \leq i,j \leq k\)</span>, its row <span class="math inline">\(i\)</span>, column <span class="math inline">\(j\)</span> element is</p><div class="no-row-height column-margin column-container"><span class="">The notation is somewhat overloaded. “Cov” refers both to the covariance between two random variables, say height and weight, and to the covariance matrix of a random vector. But it will always be clear from context which one is being discussed.</span></div></div>
<p><span class="math display">\[
Cov(X)_{ij} = Cov(X_i,X_j)
\]</span></p>
<p>As an example, here is data on major league baseball players:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(qeML) </span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(mlb1) </span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(mlb1) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>        Position Height Weight   Age
1        Catcher     74    180 22.99
2        Catcher     74    215 34.69
3        Catcher     72    210 30.78
4  First_Baseman     72    210 35.43
5  First_Baseman     73    188 35.71
6 Second_Baseman     69    176 29.39</code></pre>
</div>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>hwa <span class="ot">&lt;-</span> mlb1[,<span class="sc">-</span><span class="dv">1</span>] </span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="fu">cov</span>(hwa) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>           Height    Weight        Age
Height  5.3542814  25.61130 -0.8239233
Weight 25.6113038 433.60211 12.9110576
Age    -0.8239233  12.91106 18.6145019</code></pre>
</div>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(hwa)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>            Height    Weight         Age
Height  1.00000000 0.5315393 -0.08252974
Weight  0.53153932 1.0000000  0.14371113
Age    -0.08252974 0.1437111  1.00000000</code></pre>
</div>
</div>
<p>Again, we’ll be discussing more of this later, but what about that negative correlation between height and age? It’s near 0, and this could be a sampling artifact, but another possibility is that in this sport, shorter players do not survive as well.</p>
<p>Properties of the matrix version of covariance:</p>
<ul>
<li><p>Matrix form of definition:</p>
<p><span id="eq-matrixdef"><span class="math display">\[ Cov(X) = E[(X - X) (X - EX)']
\tag{4.2}\]</span></span></p>
<p>(Note the dimensions: <span class="math inline">\(X\)</span> is a column vector, say <span class="math inline">\(k \textrm{ x } 1\)</span>, so <span class="math inline">\((X - X) (X - EX)'\)</span> is <span class="math inline">\(k \textrm{ x } k\)</span>. The expected value is then of that size as well.)</p></li>
<li><p>For statistically independent random vectors <span class="math inline">\(Q\)</span> and <span class="math inline">\(W\)</span> of the same length,</p></li>
</ul>
<p><span id="eq-indepcov"><span class="math display">\[
Cov(Q+W) = Cov(Q) + Cov(W)
\tag{4.3}\]</span></span></p>
<ul>
<li>For any nonrandom scalar <span class="math inline">\(c\)</span>, and <span class="math inline">\(Q\)</span> a random vector, we have</li>
</ul>
<p><span class="math display">\[
Cov(cQ) = c^2 Cov(Q)
\]</span></p>
<ul>
<li><p>Say we have a random vector <span class="math inline">\(X\)</span>, of length <span class="math inline">\(k\)</span>, and a nonrandom matrix <span class="math inline">\(A\)</span> of size <span class="math inline">\(m \textrm{ x } k\)</span>. Then <span class="math inline">\(A X\)</span> is a new random vector <span class="math inline">\(Y\)</span> of <span class="math inline">\(m\)</span> components. It turns out that</p>
<p><span id="eq-acova"><span class="math display">\[
Cov(Y) = A Cov(X) A'
\tag{4.4}\]</span></span></p>
<p>The proof is straightforward but tedious, and will be omittted.</p></li>
<li><p><span class="math inline">\(Cov(X)\)</span> is a symmetric matrix. This follows from the symmetry of the definition.</p></li>
<li><p>The diagonal elements of <span class="math inline">\(Cov(X)\)</span> are the variances of the random variables <span class="math inline">\(X_i\)</span>. This follows from <span class="math inline">\(Cov(U,U) = Var(U)\)</span> for scalar <span class="math inline">\(U\)</span>.</p></li>
<li><p>If <span class="math inline">\(X\)</span> is a vector of length 1, i.e.&nbsp;a number, then</p></li>
</ul>
<p><span class="math display">\[
Cov(X) = Var(X)
\]</span></p>
<ul>
<li>For any length-<span class="math inline">\(k\)</span> column vector <span class="math inline">\(a\)</span>,</li>
</ul>
<p><span id="eq-quadform"><span class="math display">\[
Var(a'X) = a' ~ Cov(X) ~ a
\tag{4.5}\]</span></span></p>
<ul>
<li>Thus <span class="math inline">\(Cov(X)\)</span> is <em>nonnegative definite</em>, meaning that for any length-<span class="math inline">\(k\)</span> column vector <span class="math inline">\(a\)</span></li>
</ul>
<p><span class="math display">\[
a' Cov(X) a \geq 0
\]</span></p>
</section>
<section id="cross-covariance" class="level3" data-number="4.1.3">
<h3 data-number="4.1.3" class="anchored" data-anchor-id="cross-covariance"><span class="header-section-number">4.1.3</span> Cross-covariance</h3>
<p>The matrix <span class="math inline">\(Cov(X)\)</span> represents the covariance of a vector <span class="math inline">\(X\)</span> <em>with itself</em>. But we can also speak of the covariance of one vector <span class="math inline">\(X\)</span> with another vector <span class="math inline">\(Y\)</span>, termed the <em>cross-covariance</em> between them. Its definition is a natural extension of covariance matrices:</p>
<p><span class="math display">\[
Cov(X,Y) = E[(X - EX) (Y- EY)]
\]</span></p>
<p>Say <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are of lengths <span class="math inline">\(m\)</span> and <span class="math inline">\(n\)</span>. Then <span class="math inline">\(Cov(X,Y)\)</span> will be of size <span class="math inline">\(m \textrm{ x } n\)</span>, with</p>
<p><span class="math display">\[
Cov(X,Y)_{ij} = Cov(X_i,Y_j)
\]</span></p>
<p>where the latter covariance is scalar.</p>
</section>
</section>
<section id="sec-mvn" class="level2 page-columns page-full" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="sec-mvn"><span class="header-section-number">4.2</span> The Multivariate Normal Distribution Family</h2>
<p>The familiar “bell-shaped curve” refers to the <em>normal</em> (or <em>Gaussian</em>) family, whose densities have the form</p>
<p><span class="math display">\[
\frac{1}{\sigma \sqrt{2 \pi}}
e^{-0.5 (\frac{t-\mu}{\sigma})^2}
\]</span></p>
<p>The values of <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> are the mean and standard deviation. But what if we have a random vector, say of length <span class="math inline">\(k\)</span>? Is there a generalized normal family?</p>
<section id="example-k-2" class="level3" data-number="4.2.1">
<h3 data-number="4.2.1" class="anchored" data-anchor-id="example-k-2"><span class="header-section-number">4.2.1</span> Example: k = 2</h3>
<p>The answer is yes. Here is an example for <span class="math inline">\(k = 2\)</span>:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Bell.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">3D bell density</figcaption>
</figure>
</div>
</section>
<section id="general-form" class="level3" data-number="4.2.2">
<h3 data-number="4.2.2" class="anchored" data-anchor-id="general-form"><span class="header-section-number">4.2.2</span> General form</h3>
<p>Well, then, what is the form of the <span class="math inline">\(k\)</span>-dimensional density function? Just as the univariate normal family is parameterized by mean and variance, the multivariate one is parameterized via mean vector <span class="math inline">\(\mu\)</span> and covariance matrix <span class="math inline">\(\Sigma\)</span>. The form is</p>
<p><span class="math display">\[
(2\pi)^{-k/2} \det(\Sigma)^{-k/2}
e^{-0.5 (t-\mu)'(\Sigma)^{-1}(t-\mu)}
\]</span></p>
<p>Note the intuition:</p>
<ul>
<li><p>Instead of <span class="math inline">\(1/\sigma^2\)</span>, i.e.&nbsp;instead of dividing by variance, we “divide by <span class="math inline">\(\Sigma\)</span>,” intuitively viewing matrix inverse as a “reciprocal” of a matrix.</p></li>
<li><p>In other words, covariance matrices operate roughly like generalized variances.</p></li>
<li><p>Instead of squaring the scalar <span class="math inline">\(t - \mu\)</span>, we “square” it in the vector case by peforming a <span class="math inline">\(w'w\)</span> operation, albeit with <span class="math inline">\(\Sigma^{-1}\)</span> in the middle.</p></li>
</ul>
<p>Clearly, we should not stretch these analogies very far, but they do help our intuition here.</p>
</section>
<section id="properties" class="level3 page-columns page-full" data-number="4.2.3">
<h3 data-number="4.2.3" class="anchored" data-anchor-id="properties"><span class="header-section-number">4.2.3</span> Properties</h3>
<div id="thm-conditMVnormal" class="theorem">
<p><span class="theorem-title"><strong>Theorem 4.1 </strong></span>If a random vector is MV normally distributed, then the conditional distribution of any one of its components <span class="math inline">\(Y\)</span>, given the others <span class="math inline">\(X_{others} = t\)</span> (note that <span class="math inline">\(t\)</span> is a vector if <span class="math inline">\(k &gt; 2\)</span>) has the following properties:</p>
<ul>
<li><p>It has a (univariate) normal distribution.</p></li>
<li><p>Its mean <span class="math inline">\(E(Y | X_{others}) = t\)</span> is linear in <span class="math inline">\(t\)</span>.</p></li>
<li><p>Its variance <span class="math inline">\(Var(Y | X_{others} = t)\)</span> does not involve <span class="math inline">\(t\)</span>.</p></li>
</ul>
<p>These of course are the classical assumptions of linear regression models. They actually come from the MV normal model.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>This comes out of writing down the conditional density (overall density divided by marginal), and then doing some algebra.</p>
<p><span class="math inline">\(\square\)</span></p>
</div>
<div id="thm-xnormalthenaxnormal" class="theorem">
<p><span class="theorem-title"><strong>Theorem 4.2 </strong></span>If <span class="math inline">\(X\)</span> is a multivariate-normal random vector, then so is <span class="math inline">\(AX\)</span> for any conformable nonrandom matrix <span class="math inline">\(A\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Again, perform direct evaluation of the density.</p>
<p><span class="math inline">\(\square\)</span></p>
</div>
<div id="thm-cw" class="theorem">
<p><span class="theorem-title"><strong>Theorem 4.3 (Cramer-Wold Theorem) </strong></span>A random vector <span class="math inline">\(X\)</span> has a multivariate normal distribution if and only if <span class="math inline">\(w'X\)</span> has a univariate normal distribution for all conformable nonrandom vectors <span class="math inline">\(w\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>“Only if” follows from above. “If” part too complex to present here.</p>
<p>% NM <span class="math inline">\(\square\)</span></p>
</div>
<div id="thm-mvclt" class="theorem page-columns page-full">
<p><span class="theorem-title"><strong>Theorem 4.4 (The Multivariate Central Limit Theorem) </strong></span>Let <span class="math inline">\(X_1, X_2,...\)</span> be a sequence on statistically independent random vectors, with common distribution multivariate normal with mean vector <span class="math inline">\(\mu\)</span> and covariance matrix <span class="math inline">\(\Sigma\)</span>. Write</p>
<p><span class="math display">\[
\bar{X} = \frac{X_1+...+X_n}{n}
\]</span></p>
<p>Then the distribution of the random vector</p>
<p><span class="math display">\[
W_n = \sqrt{n} (\bar{X} - \mu)
\]</span></p>
<div class="page-columns page-full"><p>goes to multivariate normal with the 0 vector as mean and covariance matrix <span class="math inline">\(\Sigma\)</span>.</p><div class="no-row-height column-margin column-container"><span class="">The usual form would involve the “square root” of a matrix, but we will not discuss that concept until our chapter on inner product spaces.</span></div></div>
</div>
<p>` ::: {.proof}</p>
<p><a href="Ch2.html#thm-mvuvnormal">Theorem&nbsp;<span>3.3</span></a> reduces the problem to the univariate case, where we know the Central Limit Theorem holds.</p>
<p><span class="math inline">\(\square\)</span></p>
<p>:::</p>
</section>
</section>
<section id="multinomial-random-vectors-have-approximate-multivariate-normal-distributions" class="level2" data-number="4.3">
<h2 data-number="4.3" class="anchored" data-anchor-id="multinomial-random-vectors-have-approximate-multivariate-normal-distributions"><span class="header-section-number">4.3</span> Multinomial Random Vectors Have Approximate Multivariate Normal Distributions</h2>
<p>Recall that a <em>multinomial</em> random vector is the mathematial analog of an R factor variable – a categorical variable with <span class="math inline">\(k\)</span> levels/categories. Just as a binomial random variable represents the number of “successes” in <span class="math inline">\(n\)</span> “trials,” a multinomial random vector represents the numbers of successes in each of the <span class="math inline">\(k\)</span> categories.</p>
<p>Let’s write such a random vector as</p>
<p><span class="math display">\[
X =
\left (
\begin{array}{r}
N_1 \\
... \\
N_k \\
\end{array}
\right )
\]</span></p>
<p>Let <span class="math inline">\(p_i\)</span> be the probability of a trial having outcome <span class="math inline">\(i-1,...,k\)</span>. Note the following:</p>
<ul>
<li><p><span class="math inline">\(N_1+...+N_k = n\)</span></p></li>
<li><p>The marginal distribution of <span class="math inline">\(N_i\)</span> is binomial, with success probability <span class="math inline">\(p_i\)</span> and <span class="math inline">\(n\)</span> trials.</p></li>
</ul>
<p>So for instance if we roll a fair die 10 times, then <span class="math inline">\(N_i\)</span> is the number of trials in which the roll’s outcome was <span class="math inline">\(i\)</span> dots and <span class="math inline">\(p_i = 1/6\)</span> , <span class="math inline">\(i=1,2,3,4,5,6\)</span>.</p>
<p>Let’s find <span class="math inline">\(Cov(X)\)</span>. Define the <em>indicator</em> vector</p>
<p><span class="math display">\[
I_i =
\left (
\begin{array}{r}
I_{i1} \\
... \\
I_{ik}  \\
\end{array}
\right )
\]</span></p>
<p>Here <span class="math inline">\(I_{ij}\)</span> is 1 or 0, depending on whether trial <span class="math inline">\(i\)</span> resulted in category <span class="math inline">\(j\)</span>. (A 1 “indicates” that caregory <span class="math inline">\(j\)</span> occurred.)</p>
<p>The key is that</p>
<p><span id="eq-sumindic"><span class="math display">\[
X = \sum_{i=1}^n I_i
\tag{4.6}\]</span></span></p>
<p>For instance, in the die-rolling example, the first component on the right-hand side is the number of rolls in which we got 1 dot, and that is by definition the same as <span class="math inline">\(N_1\)</span>, the first component of <span class="math inline">\(X\)</span>.</p>
<p>So there we have it – <span class="math inline">\(X\)</span> is a sum of independent, identically distributed random vectors, so by the Multivariate Central Limit Theorem, <span class="math inline">\(X\)</span> has an approximate multivariate normal distribution. Now, what are the mean vector and covariance matrix in that distribution?</p>
<p>From our discussion above, we know that</p>
<p><span class="math display">\[
EX =
\left (
\begin{array}{r}
np_1 \\
... \\
np_k \\
\end{array}
\right )
\]</span></p>
<p>What about <span class="math inline">\(Cov(X)\)</span>? Again, recognizing that <a href="#eq-sumindic">Equation&nbsp;<span>4.6</span></a> is a sum of independent terms, <a href="#eq-indepcov">Equation&nbsp;<span>4.3</span></a> tells us that</p>
<p><span class="math display">\[
Cov(X) = Cov(\sum_{i=1}^n I_i) = \sum_{i=1}^n Cov(I_i) = n Cov(I_1),
\]</span></p>
<p>that last equality reflecting that the <span class="math inline">\(I_i\)</span> are identically distributed (the trials all have the same probabilistic behavior).</p>
<p>Now to evaluate that covariance matrix, consider two specific elements <span class="math inline">\(I_{1j}\)</span> and <span class="math inline">\(I_{im}\)</span> of <span class="math inline">\(I_1\)</span>. Recall, those elements are equal to 1 or 0, depending on whether the first trial results in Categories j and m, respectively. Then what is <span class="math inline">\(Cov(I_{1j},I_{1m})\)</span>? From <a href="#eq-covuv">Equation&nbsp;<span>4.1</span></a>, we have</p>
<p><span id="eq-covjm"><span class="math display">\[
Cov(I_{1j},I_{1m}) = E(I_{1j} I_{1m}) - (EI_{1j}) (EI_{1m})
\tag{4.7}\]</span></span></p>
<p>Consider the two cases:</p>
<ul>
<li><span class="math inline">\(i=j\)</span>: Here <span class="math inline">\(E(I_{1j}^2) = E(I_{1j}) = p_j\)</span>. Thus</li>
</ul>
<p><span class="math display">\[
Cov(I_{1j},I_{1m}) = p_j (1-p_j)
\]</span></p>
<ul>
<li><span class="math inline">\(i \neq j\)</span>: Each <span class="math inline">\(I_s\)</span> consists of one 1 and <span class="math inline">\(k-1\)</span> 0s. Thus <span class="math inline">\(E(I_{1j} I_{1m}) = 0\)</span> and</li>
</ul>
<p><span class="math display">\[
Cov(I_{1j},I_{1m}) = -p_j p_m
\]</span></p>
</section>
<section id="the-delta-method" class="level2 page-columns page-full" data-number="4.4">
<h2 data-number="4.4" class="anchored" data-anchor-id="the-delta-method"><span class="header-section-number">4.4</span> The Delta Method</h2>
<p>This is one of the most useful simple tools in statistics.</p>
<section id="review-confidence-intervals-standard-errors" class="level3" data-number="4.4.1">
<h3 data-number="4.4.1" class="anchored" data-anchor-id="review-confidence-intervals-standard-errors"><span class="header-section-number">4.4.1</span> Review: confidence intervals, standard errors</h3>
<p>To set the stage, let’s review the statistical concepts of <em>confidence interval</em> and <em>standard error</em>. Say we have an estimator <span class="math inline">\(\widehat{\theta}\)</span> of some population parameter <span class="math inline">\(\theta\)</span>, e.g.&nbsp;<span class="math inline">\(\bar{X}\)</span> for a population mean <span class="math inline">\(\mu\)</span>.</p>
<ul>
<li><p>Loosely speaking, the term <em>standard error</em> of is our estimate of <span class="math inline">\(\sqrt{Var(\widehat{\theta})}\)</span>. More precisely, suppose that <span class="math inline">\(\widehat{\theta}\)</span> is asymptotically normal. The standard error is an estimate of the standard deviation of that normal distribution. For this reason, It is customary to write <span class="math inline">\(AVar(\widehat{\theta})\)</span> rather than <span class="math inline">\(Var(\widehat{\theta})\)</span>.</p>
<p>This can be used to form a confidence interval (see below), but also stands on its own as an indication of the accuracy of <span class="math inline">\(\widehat{\theta}\)</span>.</p></li>
<li><p>A, say 95%, confidence interval for <span class="math inline">\(\mu\)</span> is then</p></li>
</ul>
<p><span class="math display">\[
\widehat{\theta} \pm 1.96 \textrm{ s.e.}(\widehat{\theta})
\]</span></p>
<p>The 95% figure means that of all possible samples of the given size from the population, 95% of the resulting confidence intervals will contain <span class="math inline">\(\theta\)</span>.</p>
</section>
<section id="delta-method-motivating-example" class="level3 page-columns page-full" data-number="4.4.2">
<h3 data-number="4.4.2" class="anchored" data-anchor-id="delta-method-motivating-example"><span class="header-section-number">4.4.2</span> Delta method: motivating example</h3>
<p>Now, for the delta method, as a first example, say we are estimating a population mean <span class="math inline">\(\mu\)</span> and are also interested in estimating <span class="math inline">\(\log(\mu)\)</span>.</p>
<div class="page-columns page-full"><p>We will probably use the sample mean <span class="math inline">\(\bar{X}\)</span> to estimate <span class="math inline">\(\mu\)</span>, and thus use <span class="math inline">\(W = \log{\bar{X}}\)</span> to estimate <span class="math inline">\(\log(\mu)\)</span>.  But how do we obtain a standard error for <span class="math inline">\(W\)</span>?</p><div class="no-row-height column-margin column-container"><span class="">If we just need to form a confidence interval for <span class="math inline">\(\log(\mu)\)</span>, we can form a CI for <span class="math inline">\(\mu\)</span> and then take the log of both endpoints. But again, standard errors are of interest in their own right.</span></div></div>
</section>
<section id="use-of-the-central-limit-theorem" class="level3" data-number="4.4.3">
<h3 data-number="4.4.3" class="anchored" data-anchor-id="use-of-the-central-limit-theorem"><span class="header-section-number">4.4.3</span> Use of the Central Limit Theorem</h3>
<p>The Central Limit Theorem tells as that <span class="math inline">\(\bar{X}\)</span> is asymptotically normally distributed. But what about <span class="math inline">\(\log{\bar{X}}\)</span>?</p>
<p>From calculus, we know that a smooth function <span class="math inline">\(f\)</span> can be written as a Taylor series,</p>
<p><span class="math display">\[
f(x) = f(x_0) + f'(x_0) (x-x_0) + f''(x_0) (x-x_0)^2 /2 + ...
\]</span></p>
<p>where here “’” denotes derivative rather than matrix transpose.</p>
<p>In our case here, setting <span class="math inline">\(f(t) = \log{t}\)</span>, <span class="math inline">\(x_0 = \mu\)</span> and <span class="math inline">\(x = \bar{X}\)</span>, we have</p>
<p><span class="math display">\[
W = \log{\mu} + \log'({\mu}) (\bar{X}-\mu) + \log''({\mu}) (\bar{X}-\mu)^2 /2 + ...
\]</span></p>
<p>and <span class="math inline">\(\log'(t) = 1/t\)</span> and so on.</p>
<p>The key point is that as n grows, <span class="math inline">\(\bar{X}-\mu\)</span> goes to 0, and <span class="math inline">\((\bar{X}-\mu)^2\)</span> goes to 0 even faster. Using theorems from probability theory, one can show that, in the sense of distribution,</p>
<p><span class="math display">\[
W \approx log(\mu) + log'(\mu) (\bar{X}-\mu)
\]</span></p>
<p>In other words, <span class="math inline">\(W\)</span> has an approximate normal distribution that has mean <span class="math inline">\(log(\mu)\)</span> and variance</p>
<p><span class="math display">\[
\frac{1}{\mu^2} \sigma^2/n
\]</span></p>
<p>where <span class="math inline">\(\sigma^2\)</span> is the population variance <span class="math inline">\(Var(X)\)</span> . We estimate the latter by the usual <span class="math inline">\(S^2\)</span> quantity, and thus have our standard error,</p>
<p><span class="math display">\[
\textrm{s.e.}(W) = \frac{S}{\bar{X} \sqrt{n}}
\]</span></p>
</section>
<section id="use-of-the-multivariate-central-limit-theorem" class="level3 page-columns page-full" data-number="4.4.4">
<h3 data-number="4.4.4" class="anchored" data-anchor-id="use-of-the-multivariate-central-limit-theorem"><span class="header-section-number">4.4.4</span> Use of the Multivariate Central Limit Theorem</h3>
<p>Now, what if the function <span class="math inline">\(f\)</span> has two arguments instead of one? The above linear approximation is now</p>
<p><span id="eq-twotaylor"><span class="math display">\[
f(v,w) \approx f(v_0,w_0) + f_1(v_0,w_0) (v-v_0) + f_2(v_0,w_0)(w-w_0)
\tag{4.8}\]</span></span></p>
<div class="page-columns page-full"><p>where <span class="math inline">\(f_1\)</span> and <span class="math inline">\(f_2\)</span> are partial derivatives,</p><div class="no-row-height column-margin column-container"><span class="">A <em>partial derivative</em> of a function of more than one variable is the derivative with respect to one of those variables. E.g. <span class="math inline">\(\partial/\partial v ~ vw^2 = w^2\)</span> and <span class="math inline">\(\partial/\partial w ~ vw^2 = 2vw\)</span>.</span></div></div>
<p><span class="math display">\[
f_1(v,w) = \frac{\partial}{\partial v} f(v,w)
\]</span></p>
<p><span class="math display">\[
f_2(v,w) = \frac{\partial}{\partial w} f(v,w)
\]</span></p>
<p>So if we are estimating, for instance, a population quantity <span class="math inline">\((\alpha,\beta)'\)</span> by <span class="math inline">\((Q,R)'\)</span>, standard error of the latter is</p>
<p><span class="math display">\[
\sqrt{
f_1^2 (Q,R) AVar(Q) +
f_2^2 (Q,R) AVar(R) +
2 f_1(Q,R) f_2(Q,R) ACov(Q,R)
}
\]</span></p>
<p>As usual, use of matrix notation can help clean up messy expressions like this. The <em>gradient</em> of <span class="math inline">\(f\)</span>, say in the two-argument case as above, is the vector</p>
<p><span class="math display">\[
\nabla f =
\left (
\begin{array}{r}
f_1 (v_0,w_0) \\
f_2 (v_0,w_0) \\
\end{array}
\right )
\]</span></p>
<p>so that <a href="#eq-twotaylor">Equation&nbsp;<span>4.8</span></a> can be written as</p>
<p><span class="math display">\[
f(v,w) \approx f(v_0,w_0) + (\nabla f)'
\left (
\begin{array}{r}
v-v_0 \\
w-w_0 \\
\end{array}
\right )
\]</span></p>
<p>Then from <a href="#eq-quadform">Equation&nbsp;<span>4.5</span></a>,</p>
<p><span id="eq-deltaquad"><span class="math display">\[
AVar[f(Q,R)] = (\nabla f)' AV(Q,R) (\nabla f)
\tag{4.9}\]</span></span></p>
</section>
<section id="example-ratio-of-two-means" class="level3 page-columns page-full" data-number="4.4.5">
<h3 data-number="4.4.5" class="anchored" data-anchor-id="example-ratio-of-two-means"><span class="header-section-number">4.4.5</span> Example: ratio of two means</h3>
<p>Say our sample data consists of mother-daughter pairs,</p>
<p><span class="math display">\[
\left (
\begin{array}{r}
M \\
D \\
\end{array}
\right )
\]</span></p>
<p>representing the heights of mother and daughter. Denote the population mean vector by</p>
<p><span class="math display">\[
\nu =
\left (
\begin{array}{r}
\mu_M \\
\mu_D \\
\end{array}
\right )
\]</span></p>
<p>We might be interested in the ratio <span class="math inline">\(\omega = \mu_D / \mu_M\)</span>. Our estimator will be <span class="math inline">\(\widehat{\omega} = \bar{D} / \bar{M}\)</span>, the ratio of the sample means.</p>
<p>So take <span class="math inline">\(Q = \bar{D}\)</span> and <span class="math inline">\(R = \bar{M}\)</span>. Then in <a href="#eq-deltaquad">Equation&nbsp;<span>4.9</span></a>, with <span class="math inline">\(f(q,r) = q/r\)</span></p>
<p><span class="math display">\[
\nabla{f} =
\left (
\begin{array}{r}
1/r \\
-q/r^2  \\
\end{array}
\right )
\]</span></p>
<p>which in our application here we would approximate by</p>
<p><span class="math display">\[
\nabla{f} =
\left (
\begin{array}{r}
1/\bar{M} \\
-\bar{D}/\bar{M}^2  \\
\end{array}
\right )
\]</span></p>
<p>As to <span class="math inline">\(AVar(v,w)\)</span> in <a href="#eq-deltaquad">Equation&nbsp;<span>4.9</span></a>, we would use the multivariate analog of the usual <span class="math inline">\(S^2\)</span> in the univariate case, taking advantage of <a href="#eq-matrixdef">Equation&nbsp;<span>4.2</span></a>. One must be careful, though, making sure we choose the appropriate quantity for <span class="math inline">\(AVar(v,w)\)</span>.</p>
<p>For instance, in our ratio example here, <span class="math inline">\((Q,R)'\)</span> is <span class="math inline">\((\bar{M},\bar{D}\)</span>. To obtain <span class="math inline">\(AVar(v,w)\)</span>, let’s first look at the covariance matrix <span class="math inline">\(\Sigma\)</span>,</p>
<p><span class="math display">\[
\Sigma = E[(M - EM) (D - ED)']
\]</span></p>
<div class="page-columns page-full"><p>This is the average of <span class="math inline">\((M - EM) (D - ED)\)</span> in the population. The sample analog average is</p><div class="no-row-height column-margin column-container"><span class="">The reader may recall that in estmating a variance, it is customary to divide by <span class="math inline">\(n-1\)</span> instead of <span class="math inline">\(n\)</span>, due to unbiasedness. The same is true for the multivariate analog of variance, i.e. covariance matrices, but we will use <span class="math inline">\(n\)</span> to retain the sample analog theme.</span></div></div>
<p><span class="math display">\[
\widehat{\Sigma} =
\frac{1}{n}
\sum_{i=1}^n (M_i - \bar{M}) (D_i - \bar{D})'
\]</span></p>
<p>where <span class="math inline">\((M_i,D_i)\)</span> represents the <span class="math inline">\(i^{th}\)</span> mother-daughter pair in our dataset.</p>
<p>But <span class="math inline">\(\widehat{{\Sigma}}\)</span> is not our <span class="math inline">\(AVar(v,w)\)</span> here, because we need, for instance, the variance of <span class="math inline">\(\bar{M}\)</span> rather than the variance of <span class="math inline">\(M\)</span>. Recall that the former is <span class="math inline">\(1/n\)</span> times the latter. So in this case we have</p>
<p><span class="math display">\[
AVar(v,w) = \frac{1}{n} \widehat{\Sigma}
\]</span></p>
<p>So now we can obtain a standard error for <span class="math inline">\(\widehat{\omega}\)</span>:</p>
<p><span class="math display">\[
\sqrt{
\frac{1}{n}
\nabla f' \widehat{\Sigma} \nabla f
}
\]</span></p>
<p>from which we can form a confidence interval.</p>
<p>In R functions to do parametric regression modeling, Maximum Likelihood Estimation and so on, <span class="math inline">\(AVar(v,w)\)</span> is available from the function’s return value.</p>
</section>
</section>
<section id="example-iranian-churn-data" class="level2" data-number="4.5">
<h2 data-number="4.5" class="anchored" data-anchor-id="example-iranian-churn-data"><span class="header-section-number">4.5</span> Example: Iranian Churn Data</h2>
<p>Here we predict whether a telecom customer will move to another provider. Here we illustrate how to obtain <span class="math inline">\(AVar(v,w)\)</span>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(IranianChurn)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>glmOut <span class="ot">&lt;-</span> <span class="fu">glm</span>(Exited <span class="sc">~</span> ., <span class="at">data =</span> iranChurn, <span class="at">family =</span> binomial)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="fu">vcov</span>(glmOut)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>                   (Intercept)   CreditScore GeographyGermany GeographySpain
(Intercept)       5.993100e-02 -5.051370e-05    -1.589878e-04  -1.388436e-03
CreditScore      -5.051370e-05  7.859301e-08    -2.683572e-07  -2.454056e-07
GeographyGermany -1.589878e-04 -2.683572e-07     4.579770e-03   1.678887e-03
GeographySpain   -1.388436e-03 -2.454056e-07     1.678887e-03   4.989715e-03
GenderMale       -1.350817e-03  2.156023e-07     2.008595e-05  -2.200094e-05
Age              -2.685656e-04 -7.969688e-09     6.042109e-06  -2.107579e-06
Tenure           -4.049072e-04  1.614151e-09    -6.645863e-06   3.828636e-06
Balance          -2.936332e-08  1.033099e-13    -1.358788e-08  -2.156656e-11
NumOfProducts    -3.775016e-03 -8.271320e-08    -3.655500e-04  -3.357316e-05
HasCrCard1       -2.595873e-03  2.066409e-07    -7.611739e-05   3.603876e-05
IsActiveMember1   5.010389e-04 -3.102862e-07    -1.008754e-04  -3.954646e-05
EstimatedSalary  -2.301952e-08  1.070775e-12    -7.124867e-11  -8.778366e-11
                    GenderMale           Age        Tenure       Balance
(Intercept)      -1.350817e-03 -2.685656e-04 -4.049072e-04 -2.936332e-08
CreditScore       2.156023e-07 -7.969688e-09  1.614151e-09  1.033099e-13
GeographyGermany  2.008595e-05  6.042109e-06 -6.645863e-06 -1.358788e-08
GeographySpain   -2.200094e-05 -2.107579e-06  3.828636e-06 -2.156656e-11
GenderMale        2.968982e-03 -4.697135e-06 -7.714322e-06 -9.609423e-10
Age              -4.697135e-06  6.633235e-06 -2.330915e-07  4.769766e-11
Tenure           -7.714322e-06 -2.330915e-07  8.751351e-05 -1.419504e-11
Balance          -9.609423e-10  4.769766e-11 -1.419504e-11  2.644146e-13
NumOfProducts     5.853316e-05  2.887014e-06 -2.575910e-06  6.486034e-09
HasCrCard1       -8.288460e-06  1.405830e-07 -1.356629e-05  6.276417e-10
IsActiveMember1   2.637202e-05 -3.924088e-05  1.978922e-05 -5.129968e-10
EstimatedSalary   1.976874e-10  1.824674e-11 -7.850909e-11 -3.430272e-15
                 NumOfProducts    HasCrCard1 IsActiveMember1 EstimatedSalary
(Intercept)      -3.775016e-03 -2.595873e-03    5.010389e-04   -2.301952e-08
CreditScore      -8.271320e-08  2.066409e-07   -3.102862e-07    1.070775e-12
GeographyGermany -3.655500e-04 -7.611739e-05   -1.008754e-04   -7.124867e-11
GeographySpain   -3.357316e-05  3.603876e-05   -3.954646e-05   -8.778366e-11
GenderMale        5.853316e-05 -8.288460e-06    2.637202e-05    1.976874e-10
Age               2.887014e-06  1.405830e-07   -3.924088e-05    1.824674e-11
Tenure           -2.575910e-06 -1.356629e-05    1.978922e-05   -7.850909e-11
Balance           6.486034e-09  6.276417e-10   -5.129968e-10   -3.430272e-15
NumOfProducts     2.221635e-03 -3.379445e-06   -3.685755e-05   -4.030670e-10
HasCrCard1       -3.379445e-06  3.521179e-03    6.930480e-05    3.401694e-11
IsActiveMember1  -3.685755e-05  6.930480e-05    3.327629e-03    2.676211e-10
EstimatedSalary  -4.030670e-10  3.401694e-11    2.676211e-10    2.243567e-13</code></pre>
</div>
</div>
<p>There are several categorical variables here, so after expansion to dummies, <span class="math inline">\(AVar(v,w)\)</span> is <span class="math inline">\(12 \textrm{ x } 12\)</span>. This is the covariance matrix for the vector of estimated logistic regression coefficients <span class="math inline">\(\widehat{\beta}\)</span>. There are many different functions <span class="math inline">\(f(\beta)\)</span> that might be of interest.</p>
</section>
<section id="example-motherdaughter-height-data" class="level2" data-number="4.6">
<h2 data-number="4.6" class="anchored" data-anchor-id="example-motherdaughter-height-data"><span class="header-section-number">4.6</span> Example: Mother/Daughter Height Data</h2>
<div class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(WackyData)</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(Heights)</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(heights)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>  Mheight Dheight
1    59.7    55.1
2    58.2    56.5
3    60.6    56.0
4    60.7    56.8
5    61.8    56.0
6    55.5    57.9</code></pre>
</div>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>m <span class="ot">&lt;-</span> heights[,<span class="dv">1</span>]</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>d <span class="ot">&lt;-</span> heights[,<span class="dv">2</span>]</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>meanm <span class="ot">&lt;-</span> <span class="fu">mean</span>(m)</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>meand <span class="ot">&lt;-</span> <span class="fu">mean</span>(d)</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>fDel <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">1</span><span class="sc">/</span>meanm,<span class="sc">-</span>meand<span class="sc">/</span>meanm<span class="sc">^</span><span class="dv">2</span>),<span class="at">ncol=</span><span class="dv">1</span>)</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">length</span>(m)</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>sigma <span class="ot">&lt;-</span> (<span class="dv">1</span><span class="sc">/</span>n) <span class="sc">*</span> <span class="fu">cov</span>(<span class="fu">cbind</span>(m,d))</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>se <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">t</span>(fDel) <span class="sc">%*%</span> sigma <span class="sc">%*%</span> fDel)</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>se</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>            [,1]
[1,] 0.001097201</code></pre>
</div>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>meanmd <span class="ot">&lt;-</span>meanm <span class="sc">/</span> meand</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>meanmd</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.9796356</code></pre>
</div>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(meanmd <span class="sc">-</span> <span class="fl">1.96</span><span class="sc">*</span>se, meanmd <span class="sc">+</span> <span class="fl">1.96</span><span class="sc">*</span>se)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.9774850 0.9817861</code></pre>
</div>
</div>
</section>
<section id="regarding-those-pesky-derivatives" class="level2" data-number="4.7">
<h2 data-number="4.7" class="anchored" data-anchor-id="regarding-those-pesky-derivatives"><span class="header-section-number">4.7</span> Regarding Those Pesky Derivatives</h2>
<p>Though finding expressions for the derivatives in the above example was not onerous, the function <span class="math inline">\(f\)</span> can be rather complex, with the expressions for its derivatives even more complicated. Typically such tedious and error-prone operations can be avoided, by having the software calculate approximate derivatives.</p>
<p>Recall the definition of derivative:</p>
<p><span class="math display">\[
f'(x) = \lim_{w \rightarrow 0}
\frac{f(x+w) - f(x)}{w}
\]</span></p>
<p>So an aproximate value of <span class="math inline">\(f'(x)\)</span> is obtained by choosing some small value of <span class="math inline">\(w\)</span> and evaluating</p>
<p><span class="math display">\[
\frac{f(x+w) - f(x)}{w}
\]</span></p>
<p>Though of course there is an issue with one’s choice of <span class="math inline">\(w\)</span>, the point is that one can code the software to find approximate derivatives automatically using this device. <em>This is very common in Data Science libraries.</em></p>
<p>For example, the R package <strong>numDeriv</strong> will compute numerical derivatives.</p>
</section>
<section id="your-turn" class="level2" data-number="4.8">
<h2 data-number="4.8" class="anchored" data-anchor-id="your-turn"><span class="header-section-number">4.8</span> Your Turn</h2>
<p>❄️ <strong>Your Turn:</strong> Show that in the scalar context,</p>
<p><span class="math display">\[
Cov(X,Y) = E(XY) - EX ~ EY
\]</span></p>
<p>❄️ <strong>Your Turn:</strong> Show that in the vector context,</p>
<p><span class="math display">\[
Cov(X) = E(X X') - (EX) (EX)'
\]</span></p>
<p>❄️ <strong>Your Turn:</strong> Suppose</p>
<p><span class="math display">\[
W = X \beta + \alpha S
\]</span></p>
<p>for a random vector <span class="math inline">\(X\)</span>, a scalar random variable <span class="math inline">\(S\)</span>, a nonrandom vector <span class="math inline">\(\beta\)</span> and a nonrandom scalar <span class="math inline">\(\alpha\)</span>. Show that</p>
<p><span class="math display">\[
Cov(W,S) = \beta' Cov(X,S) + \alpha Var(S)
\]</span></p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./Ch2.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Matrix Inverse</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./Ch3.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Linear Statistical Models</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>