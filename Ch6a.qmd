
```{r} 
#| include: false
library(dsld)
library(qeML)
```

{{< pagebreak >}}

# Eigenanalysis

::: {.callout-note}

## Goals of this chapter:

The concept of eigenvalues and eigenvectors is one of the most important
of all appplications of linear algebra to data science. We introduce the
basic ideas and properties in this chapter. 

:::

## Example: African Soils Data

To get things started, let's consider the African Soils dataset.

Let's take a look around:

```{r}
library(WackyData) 
data(AfricanSoil)
dim(AfricanSoil)
names(AfricanSoil)[1:25]
names(AfricanSoil)[3576:3600]
```

Let's try predicting **pH**, the acidity. But that leaves 3599 possible
predictors. There is an old rule of thumb that one should have 
$p < \sqrt{n}$, for $p$ predictors and $n$ data points,
to avoid overfitting, a rule which in our setting of $n = 1157$
is grossly violated.  We need to do dimension reduction. One way to
accomplish this is to use Principal Components Analysis (PCA). 

## Overall Idea 

The goal is to find a few important linear combinations of our original
predictor variables--important in the sense that they roughly summarize
our data.  These new variables are called the *principal components*
(PCs) of the data. PCA will be covered in detail in the next chapter,
but our preview here will set the stage for important general concepts
that we will develop in the current chapter.

### The first PC{#sec-firstPC}

Let **X** denote a set of variables of interest (not necessarily in a
prediction context).
In searching for good linear combinations, we want to aim for ones with
high variance. We certainly don't want ones with low variance; after
all, a random variable with 0 variance is a constant.  So we wish to
find linear combinations 

$$
Xu
$$

which maximize 

$$
Var(Xu) = u' Cov(X) u
$$

where we have invoked @eq-acova.

But that goal is ill-defined, since we could take larger and larger
vectors $u$, thus larger and larger vectors $Xu$, no maximum.  So, let's
constrain it to vectors $u$ of length 1:

$$
u'u = 1
$$

The method of *Lagrange multipliers* is used to solve maximum/minimum
problems that have constraints, by adding a new variable corresponding
to the constraint. In our case, we maximize

$$
u'Cov(X)u + \gamma (u'u - 1)
$$

with respect to $u$ and $\gamma$.

Setting derivatives to 0, we have

$$
0 = 
2 Cov(X) u + 2 \gamma u
$$

In other words,

$$
Cov(X) ~ u = -\gamma u
$${#eq-aha}

We see a situation in which a matrix ($Cov(X)$) times a vector ($u$)
equals a constant ($-\gamma$) times that same vector.  We say that
$-\gamma$ is an *eigenvalue* of the matrix $Cov(X)$, and that $u$ is a
corresponding *eigenvector*.  Seems innocuous, but it opens a huge new
world! We will return to the notion of principal components in the next
chapter, after laying the groundwork in the current chapter.

## Definition

The concept itself is simple:


> Consider an $m \textrm{ x } m$ matrix $M$. If there is a nonzero vector
> $x$ and a number $\lambda$ such that
> 
> $$
> Mx = \lambda x
> $${#eq-simpledef}
> 
> we say that $\lambda$ is an *eigenvalue* of $M$, with *eigenvector*
> $x$.

## A First Look

Here are a few properties to start with:

* The definition is equivalent to

  $$
  (M - \lambda I) x = 0
  $$

  which in turn implies that $M - \lambda I$ is noninvertible. That
  then implies that 

  $$
  det(M - \lambda I) = 0
  $$

* The left-hand side of this equation is a polynomial in $\lambda$. So
  for an $n \textrm{ x } n$ matrix $M$, there are $n$ roots of the
  equation and thus $n$ eigenvalues. Note that some roots may be
  repeated; if $M$ is the zero matrix, it will have an eigenvalue $0$
  with multiplicity $n$.

* In principle, that means we can solve the above determinant equation
  to find the eigenvalues of the matrix, though There are much better
  ways to calculate the eigenvalues than this, but a useful piece
  information arising from that analysis is that $M$ as $m$ eigenvalues
  (not necessarily distinct). 

## The Special Case of Symmetric Matrices

Many matrices in data science are symmetric, such as covariance matrices
and $A'A$ in @eq-linregformula. 

Some eigenvalues may be complex numbers, i.e. of the form $a+bi$, but it
can be shown that if $M$ is symmetric, its eigenvalues are guaranteed to
be real. This is good news for data science, as many matrices in that
field are symmetric, such covariance matrices.

::: {#thm-eigreal}

Eigenvalues of a symmetric matrix are real, not complex numbers, i.e. not of
the form $a+bi$.

:::

::: {#thm-eigortho}

Eigenvectors corresponding to distinct eigenvalues of a symmetric matrix
$M$ are orthogonal.

:::

::: {.proof}

Let $u$ and $v$ be such eigenvectors, corresponding to
eigenvalues $\mu$ and $\nu$.

$$
u' M u = (u' M u)' = u' M' u = u' M u
$$

$$
u'v = (u'v)' = [u'(\frac{-1}{\nu} M v)]' = \frac{-1}{\nu}v'M'u =
\frac{-1}{\nu}v'Mu =
\frac{\mu}{\nu}v'u 
$$

We see that $u'v$, a number, is equal to $\mu/\nu$ times itself, and
since that fraction is not equal to 1 by assumption, $u'v$ must be 0

$\square$

::: 

A square matrix $R$ is said to be *diagonalizable* if there exists an
invertible matrix $P$ such that $P^{-1}RP$ is equal to some diagonal
matrix $D$. We will see that symmetric matrices fall into this category.

One application of this is the computation of powers of a diagonal
matrix:

$$
R^k = (P^{-1}RP) ~ (P^{-1}RP) ~ ... ~ (P^{-1}RP) = P^{-1}D^kP
$$

$D^k$ is equal to the diagonal matrix with elements $d_k^k$, so the
computation of $M^k$ is easy.

::: {#thm-symmeig}

Any symmetric matrix $M$ has the following properties

* $M$ is diagonalizable. 

* In fact, the matrix $P$ is equal to the matrix whose columns are the
  eigenvectors $u_i$ of $M$, chosen to have norm 1. Thus the same holds for
  the rows of $P'$, so that 

  $$
  P'u_i = \lambda_i u_i
  $${#eq-pcols}

* The associated diagonal matrix has as its diagonal elements the
  eigenvalues of $M$. 

* The matrix $P$ has the property that $P^{-1} = P'$.  

* Moreover, the rank of $M$ is equal to the number of nonzero
  eigenvalues of $M$.

:::

::: {.proof}

Use partitioned matrices. 

Let $D = diag(d_1,...,d_m)$, where the $d_i$ are the eigenvalues of $M$,
corresponding to eigenvectors $P_i$. Set the latter to have length 1,
by dividing by their lengths.  

Recall that the eigenvectors of $M$, i.e. the columns of $P$, are
orthogonal. Again using partitioning, we see that 

$$
P'P = I
$$

so that $P^{-1} = P'$.

By the way, any square matrix $Q$ such that $Q'Q = I$ is said to be
*orthogonal*. And, its inverse is its transpose.

Now use partitioning again:

$$
MP = M(P_1 | ... | P_m) = (MP_1 | ... | MP_n) =
(d_1 P_1 | ... | (d_m P_m) = PD
$$

Multiply on the left by $P'$, and we are done:

$$
P'MP = P'P D = D
$$

Regarding rank, @thm-ranknotchange tells us that pre- or postmultiplying
by an invertible matrix does not change rank, and clearly the rank of a
diagonal matrix is the number of nonzero elements.

$\square$

:::

### Example: Census dataset

Let's illustrate all this with the data in @sec-pgtn. We will form the
matrix $A'A$ in @sec-linregformula, which as mentioned, is symmetric.

Recall that in that example, $A$ is not of full rank. Thus we should
expect to see a 0 eigenvalue.

```{r}
library(qeML)
data(svcensus)
svc <- svcensus[,c(1,4:6)]
svc$man <- as.numeric(svc$gender == 'male')
svc$woman <- as.numeric(svc$gender == 'female')
svc$gender <- NULL
a <- as.matrix(svc[,-2])
a <- cbind(1,a)  # add the column of 1s
m <- t(a) %*% a
eigs <- eigen(m)
eigs
m %*% eigs$vectors[,1]
eigs$values[1] %*% eigs$vectors[,1]
```

Yes, that first column is indeed an eigenvector, with the claimed eigenvalue. 

Note that the expected 0 eigenvalue shows up as 2.801489e-12, quite
small but nonzero, due to roundoff error.

## Application: Detecting Multicollinearity

Consider the basic eigenanalysis equation,

$$
Ax = \lambda x
$$

for a square matrix $A$, a conformable vector $x$ and a scalar
$\lambda$. Suppose that, roughly speaking, $\lambda x$ is small relative
to $A$. Then

$$
Ax \approx 0
$$

and since $Ax$ is a linear combination of the columns of $A$,
we thus we have found multicollinearity in $A$.

One often sees use of the *condition number* of a matrix, which is the
ratio of the largest eigenvalue to the smallest one. This too might be
used as a suggestion of multicollinearity, though the main usage is as a
signal that matrix operations such finding inverses may have significant
problems with roundoff error.

## Example: Currency Data

This dataset tracks five pre-euro European currencies.

```{r}
library(qeML)
data(currency)
head(currency)
dim(currency)
crc <- currency
crc <- as.matrix(crc)
crcapa <- t(crc) %*% crc
eigs <- eigen(crcapa)
eigs
# illustrate Ax = lambda x
crcapa %*% eigs$vectors[,5]
eigs$values[5] *  eigs$vectors[,5] 
# is Ax small?
head(crcapa)
```

So yes, this dataset has some multicollinearity.

## Computation: the Power Method

One way to compute eigenvalues and eigenvectors is the *power method*, a
simple iteration. We begin with an initial guess, $x_0$ for an
eigenvector. Substituting in @eq-simpledef, we have the next guess:

$$
x_1 = M x_0
$$

We keep iterating until convergence, generating $x_2$ from $x_1$ and so
on. However, the $x_i$ may grow, so we normalize to length 1:

$$
x_i \leftarrow \frac{x_i}{||x_i||}
$$

## Application: Computation of Long-Run Distribution in Markov Chains

We showed in @sec-markovsolve how matrix inverse can be used to compute
the long-run distribution $\nu$ in a Markov chain. However, this is
inefficient for very large transition matrices. For instance, in Google
PageRank, there is a Markov state for every page on the Web!

Instead, we exploit the fact that @eq-MarkovPi says that the transition
matrix has an eigenvector $\nu$ with eigenvalue 1.  Due to the typical
huge size of the matrix, the power method or a variant is often used.

## Your Turn

❄️  **Your Turn:** Show that the diagonalizing matrix $P$ above must
have determinant $\pm 1$.

❄️  **Your Turn:** Show that if $x$ is an eigenvector of $M$ with
eigenvalue $\lambda \neq 0$, then for any nonzero number $c$, $cx$ will
also be an eigenvector with eigenvalue $\lambda$.

❄️  **Your Turn:** Show that if a matrix $M$ has a 0 eigenvalue, $M$
must be singular. Also prove the converse. (Hint: Consider the column
rank.)

❄️  **Your Turn:** Consider a projection matrix $P_W$. Show that the only
possible eigenvalues are 0 and 1. Hint: Recall that projection matrices
are idempotent.

❄️  **Your Turn:** Say $A$ is an invertible matrix with eigenvalue
$\lambda$ and eigenvector $v$. Show that $v$ is also an eigenvector of
$A^{-1}$, with eigenvalue $1/\lambda$.

❄️  **Your Turn:** Show that if $A$ is nonnegative-definite, its
eigenvalues must be nonnegative.

❄️  **Your Turn:** @thm-symmeig says that for any symmetric matrix $M$
is diagonalizable: There is an orthogonal matrix $P$ such that
$P' M P$ is equal to a diagonal matrix $D$, the latter consisting of the
eigenvalues of $M$. Use this to show that 

$$
x' M x \leq \lambda_{max} ||x||^{2}
$$

where $\lambda_{max}$ is the largest eigenvalue. Hint: First show that

$$
x'Mx = (Px)' D (Px)
$$


