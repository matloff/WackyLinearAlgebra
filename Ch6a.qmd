
```{r} 
#| include: false
library(dsld)
library(qeML)
```

{{< pagebreak >}}

# Eigenanalysis: Properties 

::: {.callout-note}

## Goals of this chapter:

The concept of eigenvalues and eigenvectors is one of the most important
of all appplications of linear algebra to data science. We introduce the
basic ideas and properties in this chapter. 

:::

## Definition

The concept itself is simple:

<blockquote>

Consider an $m \textrm{ x } m$ matrix $M$. If there is a nonzero vector
$x$ and a number $\lambda$ such that

$$
Mx = \lambda x
$${#eq-simpledef}

we say that $\lambda$ is an *eigenvalue* of $M$, with *eigenvector*
$x$.
  
</blockquote>

## A First Look

Here are a few properties to start with:

* The definition is equivalent to

  $$
  (M - \lambda I) x = 0
  $$

  which in turn implies that $M - \lambda I$ is noninvertible. That
  then implies that 

  $$
  det(M - \lambda I) = 0
  $$

* The left-hand side of this equation is a polynomial in $\lambda$. So
  for an $n \textrm{ x } n$ matrix $M$, there are $n$ roots of the
  equation and thus $n$ eigenvalues. Note that some roots may be
  repeated; if $M$ is the zero matrix, it will have an eigenvalue $0$
  with multiplicity $n$.

* In principle, that means we can solve the above determinant equation
  to find the eigenvalues of the matrix. The comments in @sec-detdef
  regarding the "ugly" definition of determinants imply that the
  equation involves a degree-$m$ polynomial in $\lambda$.

* There are much better ways to calculate the eigenvalues than this, but
  a useful piece information arising from that analysis is that $M$ as
  $m$ eigenvalues (not necessarily distinct).  

## The Special Case of Symmetric Matrices

Many matrices in data science are symmetric, such as covariance matrices
and $A'A$ in @eq-linregformula. 

### Eigenvalues are real

Some eigenvalues may be complex numbers, i.e. of the form $a+bi$, but it
can be shown that if $M$ is symmetric, its eigenvalues are guaranteed to
be real. This is good news for data science, as many matrices in that
field are symmetric, such covariance matrices.

### Eigenvectors are orthogonal{#sec-eigortho}

<blockquote>

Eigenvectors corresponding to distinct eigenvalues of a symmetric matrix
$M$ are othogonal.

*Proof:* Let $u$ and $v$ be such eigenvectors, corresponding to
eigenvalues $\mu$ and $\nu$.

$$
\mu' M \nu = (\mu' M \nu)' = \nu' M' \mu = \nu' M \mu
$$

(in that first equality, we used the fact that the transpose of a number
is that number). Substitute for $M\nu$ and $M\mu$, then subtract.

</blockquote>

### Symmetric matrices are diagonalizable{#sec-diagonalizable}

A square matrix $R$ is said to be *diagonalizable* if there exists an
invertible matrix $P$ such that $P^{-1}RP$ is equal to some diagonal
matrix $D$. We see that symmetric matrices fall into this category.

One application of this is the computation of powers of a diagonal
matrix:

$$
R^k = P^{-1}RP ~ P^{-1}RP ~ ... ~ P^{-1}RP = P^{-1}D^kP
$$

$D^k$ is equal to the diagonal matrix with elements $d_k^k$, so the
computation of $M^k$ is easy.

::: {#thm-symmeig}

Any symmetric matrix $M$ has the following properties

* $M$ is diagonalizable. 

* In fact, the matrix $P$ is equal to the matrix whose columns are the
  eigenvectors of $M$, chosen to have length 1. 

* The associated diagonal matrix has as its diagonal elements the
  eigenvalues of $M$. 

* The matrix $P$ has the property that $P^{-1} = P'$.  

* Moreover, the rank of $M$ is equal to the number of nonzero
  eigenvalues of $M$.

:::

::: {.proof}

Use partitioned matrices. 

Let $D = diag(d_1,...,d_m)$, where the $d_i$ are the eigenvalues of $M$,
corresponding to eigenvectors $P_i$. Set the latter to have length 1,
by dividing by their lengths.  

Recall that the eigenvectors of $M$, i.e. the columns of $P$, are
orthogonal. That means the rows of $P'$ are orthogonal. Again using
partitioning, we see that 

$$
P'P = I
$$

so that $P^{-1} = P'$.

By the way, any square matrix $Q$ such that $Q'Q = I$ is said to be
*orthogonal*.

Now write

$$
MP = M(P_1 | ... | P_m) = (MP_1 | ... | MP_n) =
(d_1 P_1 | ... | (d_m P_m) = PD
$$

Multiply on the left by $P'$, and we are done.

Regarding rank, @thm-ranknotchange tells us that pre- or postmultiplying
by an invertible matrix does not change rank, and clearly the rank of a
diagonal matrix is the number of nonzero elements.

$\square$

:::

### Example: Census dataset

Let's illustrate all this with the data in @sec-pgtn. We will form the
matrix $A'A$ in @sec-linregformula, which as mentioned, is symmetric.

Recall that in that example, $A$ is not of full rank. Thus we should
expect to see a 0 eigenvalue.

```{r}
library(qeML)
data(svcensus)
svc <- svcensus[,c(1,4:6)]
svc$man <- as.numeric(svc$gender == 'male')
svc$woman <- as.numeric(svc$gender == 'female')
svc$gender <- NULL
a <- as.matrix(svc[,-2])
a <- cbind(1,a)  # add the column of 1s
m <- t(a) %*% a
eigs <- eigen(m)
eigs
m %*% eigs$vectors[,1]
eigs$values[1] %*% eigs$vectors[,1]
```

Yes, that first column is indeed an eigenvector, with the claimed eigenvalue. 

Note that the expected 0 eigenvalue shows up as 2.801489e-12, quite
small but nonzero, due to roundoff error.

## Application: Detecting Multicollinearity

Consider the basic eigenanalysis equation,

$$
Ax = \lambda x
$$

for a square matrix $A$, a conformable vector $x$ and a scalar
$\lambda$. Suppose that, roughly speaking, $\lambda x$ is small relative
to $A$. Then

$$
Ax \approx 0
$$

and since $Ax$ is a linear combination of the columns of $A$,
we thus we have found multicollinearity in $A$.

One often sees use of the *condition number* of a matrix, which is the
ratio of the largest eigenvalue to the smallest one. This too might be
used as a suggestion of multicollinearity, though the main usage is as a
signal that matrix operations such finding inverses may have significant
problems with roundoff error.

## Example: Currency Data

This dataset tracks five pre-euro European currencies.

```{r}
library(qeML)
data(currency)
head(currency)
dim(currency)
crc <- currency
crc <- as.matrix(crc)
crcapa <- t(crc) %*% crc
eigs <- eigen(crcapa)
eigs
# illustrate Ax = lambda x
crcapa %*% eigs$vectors[,5]
eigs$values[5] *  eigs$vectors[,5] 
# is Ax small?
head(crcapa)
```

So yes, this dataset has some multicollinearity.

## Computation: the Power Method

One way to compute eigenvalues and eigenvectors is the *power method*, a
simple iteration. We begin with an initial guess, $x_0$ for an
eigenvector. Substituting in @eq-simpledef, we have the next guess:

$$
x_1 = M x_0
$$

We keep iterating until convergence, generating $x_2$ from $x_1$ and so
on. However, the $x_i$ may grow, so we normalize to length 1:

$$
x_i \leftarrow \frac{x_i}{||x_i||}
$$

## Application: Computation of Long-Run Distribution in Markov Chains

We showed in @sec-markovsolve how matrix inverse can be used to compute
the long-run distribution $\nu$ in a Markov chain. However, this is
inefficient for very large transition matrices. For instance, in Google
PageRank, there is a Markov state for every page on the Web!

Instead, we exploit the fact that @eq-MarkovPi says that the transition
matrix has an eigenvector $\nu$ with eigenvalue 1.  Due to the typical
huge size of the matrix, the power method or a variant is often used.

## Your Turn

❄️  **Your Turn:** Show that the diagonalizing matrix $P$ above must
have determinant $\pm 1$.

❄️  **Your Turn:** Show that if $x$ is an eigenvector of $M$ with
eigenvalue $\lambda \neq 0$, then for any nonzero number $c$, $cx$ will
also be an eigenvector with eigenvalue $\lambda$.

❄️  **Your Turn:** Show that if a matrix $M$ has a 0 eigenvalue, $M$
must be singular. Also prove the converse. (Hint: Consider the column
rank.)

❄️  **Your Turn:** Consider a projection matrix $P_W$. Show that the only
possible eigenvalues are 0 and 1. Hint: Recall that projection matrices
are idempotent.

❄️  **Your Turn:** Say $A$ is an invertible matrix with eigenvalue
$\lambda$ and eigenvector $v$. Show that $v$ is also an eigenvector of
$A^{-1}$, with eigenvalue $1/\lambda$.

❄️  **Your Turn:** Show that if $A$ is nonnegative-definite, its
eigenvalues must be nonnegative.

