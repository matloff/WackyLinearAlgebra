
```{r} 
#| include: false
library(dsld)
library(qeML)
```

{{< pagebreak >}}

# Eigenanalysis: Properties 

::: {.callout-note}

## Goals of this chapter:

The concept of eigenvalues and eigenvectors is one of the most important
of all appplications of linear algebra to data science. We introduce the
basic ideas and properties in this chapter. 

:::

## Definition

The concept itself is simple:

<blockquote>

Consider an $m \textrm{ x } m$ matrix $M$. If there is a nonzero vector
$x$ and a number $\lambda$ such that

$$
Mx = \lambda x
$${#eq-simpledef}

we say that $\lambda$ is an *eigenvalue* of $M$, with *eigenvector*
$x$.
  
</blockquote>

## A First Look

Here are a few properties to start with:

* The definition is equivalent to

  $$
  (M - \lambda I) x = 0
  $$

  which in turn implies that $M - \lambda I$ is noninvertible. That
  then implies that 

  $$
  det(M - \lambda I) = 0
  $$

* The left-hand side of this equation is a polynomial in $\lambda$. So
  for an $n \textrm{ x } n$ matrix $M$, there are $n$ roots of the
  equation and thus $n$ eigenvalues. Note that some roots may be
  repeated; if $M$ is the zero matrix, it will have an eigenvalue $0$
  with multiplicity $n$.

* In principle, that means we can solve the above determinant equation
  to find the eigenvalues of the matrix. The comments in @sec-detdef
  regarding the "ugly" definition of determinants imply that the
  equation involves a degree-$m$ polynomial in $\lambda$.

* There are much better ways to calculate the eigenvalues than this, but
  a useful piece information arising from that analysis is that $M$ as
  $m$ eigenvalues (not necessarily distinct).  

## The Special Case of Symmetric Matrices

Many matrices in data science are symmetric, such as covariance matrices
and $A'A$ in @eq-linregformula. 

### Eigenvalues are real

Some eigenvalues may be complex numbers, i.e. of the form $a+bi$, but it
can be shown that if $M$ is symmetric, its eigenvalues are guaranteed to
be real. This is good news for data science, as many matrices in that
field are symmetric, such covariance matrices.

### Eigenvectors are orthogonal{#sec-eigortho}

<blockquote>

Eigenvectors corresponding to distinct eigenvalues of a symmetric matrix
$M$ are othogonal.

*Proof:* Let $u$ and $v$ be such eigenvectors, corresponding to
eigenvalues $\mu$ and $\nu$.

$$
\mu' M \nu = (\mu' M \nu)' = \nu' M' \mu = \nu' M \mu
$$

(in that first equality, we used the fact that the transpose of a number
is that number). Substitute for $M\nu$ and $M\mu$, then subtract.

</blockquote>

### Symmetric matrices are diagonalizable{#sec-diagonalizable}

A square matrix $R$ is said to be *diagonalizable* if there exists an
invertible matrix $P$ such that $P^{-1}RP$ is equal to some diagonal
matrix $D$. We see that symmetric matrices fall into this category.

One application of this is the computation of powers of a diagonal
matrix:

$$
R^k = P^{-1}RP ~ P^{-1}RP ~ ... ~ P^{-1}RP = P^{-1}D^kP
$$

$D^k$ is equal to the diagonal matrix with elements $d_k^k$, so the
computation of $M^k$ is easy.

::: {#thm-symmeig}

Any symmetric matrix $M$ has the following properties

* $M$ is diagonalizable. 

* In fact, the matrix $P$ is equal to the matrix whose columns are the
  eigenvectors of $M$, chosen to have length 1. 

* The associated diagonal matrix has as its diagonal elements the
  eigenvalues of $M$. 

* The matrix $P$ has the property that $P^{-1} = P'$.  

* Moreover, the rank of $M$ is equal to the number of nonzero
  eigenvalues of $M$.

:::

::: {.proof}

Use partitioned matrices. 

Let $D = diag(d_1,...,d_m)$, where the $d_i$ are the eigenvalues of $M$,
corresponding to eigenvectors $P_i$. Set the latter to have length 1,
by dividing by their lengths.  

Recall that the eigenvectors of $M$, i.e. the columns of $P$, are
orthogonal. That means the rows of $P'$ are orthogonal. Again using
partitioning, we see that 

$$
P'P = I
$$

so that $P^{-1} = P'$.

By the way, any square matrix $Q$ such that $Q'Q = I$ is said to be
*orthogonal*.

Now write

$$
MP = M(P_1 | ... | P_m) = (MP_1 | ... | MP_n) =
(d_1 P_1 | ... | (d_m P_m) = PD
$$

Multiply on the left by $P'$, and we are done.

Regarding rank, @thm-ranknotchange tells us that pre- or postmultiplying
by an invertible matrix does not change rank, and clearly the rank of a
diagonal matrix is the number of nonzero elements.

$\square$

:::

### Example: Census dataset

Let's illustrate all this with the data in @sec-pgtn. We will form the
matrix $A'A$ in @sec-linregformula, which as mentioned, is symmetric.

Recall that in that example, $A$ is not of full rank. Thus we should
expect to see a 0 eigenvalue.

```{r}
library(qeML)
data(svcensus)
svc <- svcensus[,c(1,4:6)]
svc$man <- as.numeric(svc$gender == 'male')
svc$woman <- as.numeric(svc$gender == 'female')
svc$gender <- NULL
a <- as.matrix(svc[,-2])
a <- cbind(1,a)  # add the column of 1s
m <- t(a) %*% a
eigs <- eigen(m)
eigs
m %*% eigs$vectors[,1]
eigs$values[1] %*% eigs$vectors[,1]
```

Yes, that first column is indeed an eigenvector, with the claimed eigenvalue. 

Note that the expected 0 eigenvalue shows up as 2.801489e-12, quite
small but nonzero, due to roundoff error.

## Application to Detecting Multicollinearity

``` r
> dim(s50)
[1] 50000    91
> s50x <- s50[,-1]
> s50x <- as.matrix(s50x)
> s50x <- scale(s50x)
> s50xpx <- t(s50x) %*% s50x
> eigs <- eigen(s50xpx)
> vals <- eigs$values
> vals
 [1] 521980.499 353847.640 263814.273 192835.529 168004.082 123696.693
 [7] 106312.289 102926.617  95395.040  90556.283  86195.071  84273.630
[13]  82364.200  76811.039  73458.261  69907.030  68227.722  63652.269
[19]  62569.613  61854.568  58809.554  56480.833  55703.595  53247.350
[25]  52265.730  51463.203  50130.529  48956.996  46273.720  44476.704
[31]  43726.557  42680.084  41335.165  39830.522  38430.438  36602.040
[37]  36203.483  36007.059  35500.086  33492.853  32634.517  31968.914
[43]  31149.856  30688.198  29264.204  28305.233  27964.171  27667.055
[49]  27049.362  25781.092  25722.780  24365.896  24070.205  23149.504
[55]  23126.607  22106.878  21802.182  21357.008  20566.637  19671.121
[61]  18788.971  18243.704  17630.699  17220.942  16655.362  15849.797
[67]  15731.124  15052.899  14403.261  14021.594  13915.305  12764.076
[73]  12145.412  12038.024  11705.442  11040.814  10053.480   9494.834
[79]   9393.185   9000.047   8249.742   7645.301   7006.374   6339.556
[85]   5963.902   5707.012   4340.816   4018.670   3881.422   2897.966
> vecs <- eigs$vectors
> vecs[,90]
 [1] -0.0213110657  0.0891433061  0.2282756964 -0.5422748844  0.0691013669
 [6]  0.3434376055  0.2565863520 -0.0563121178 -0.0124856340  0.1668003263
[11]  0.3320041674 -0.1088085427 -0.0255634617  0.0373525006 -0.0072706841
[16]  0.0688690930  0.0067631609  0.2011768165 -0.0484937578  0.2984216574
[21]  0.0218810816 -0.3370196425 -0.0577861554 -0.0039389901 -0.0059234945
[26] -0.0095252585 -0.0340446362 -0.0101573428  0.0068218589 -0.0137991568
[31] -0.0045231262  0.0179140274  0.0276570653 -0.0049008664 -0.0032842182
[36] -0.0163511619  0.0411123232  0.0069270770  0.0310420474 -0.0097104529
[41] -0.0183830186 -0.0016829127  0.0140109331 -0.0061652647  0.0064474620
[46] -0.0105130129 -0.0067497303  0.0003683684  0.0190190735 -0.0073963363
[51]  0.0049540340 -0.0237335423 -0.0153191107 -0.0185628086  0.0025456262
[56] -0.0495299288  0.0201489084  0.0282499935  0.0123103131 -0.0681615812
[61] -0.0396767922 -0.0061256365  0.0325705122 -0.0243608423 -0.0376883744
[66] -0.0062525271 -0.0171980533  0.0348482251  0.0060671410  0.0240426238
[71]  0.0566190187 -0.0269552173  0.0553623285  0.0217856880 -0.0050196954
[76] -0.0242363644  0.0423068711  0.0379175351 -0.0203093405 -0.0194922539
[81] -0.0107854379 -0.0886004144  0.0450796387  0.0020701480  0.0140390613
[86] -0.0323156149 -0.0051611939  0.0242438852 -0.0006087894 -0.0021392653
> s50xpx %*% vecs[,90]  # check, really eigs?
V2    -61.758742
V3    258.334264
V4    661.535190
V5  -1571.494138
V6    200.253407
V7    995.270478
V8    743.578505
V9   -163.190599
V10   -36.182942
V11   483.381662
V12   962.136764
V13  -315.323449
V14   -74.082041
V15   108.246274
V16   -21.070195
V17   199.580285
V18    19.599410
V19   583.003559
V20  -140.533258
V21   864.815795
V22    63.410629
V23  -976.671440
V24  -167.462309
V25   -11.415059
V26   -17.166085
V27   -27.603875
V28   -98.660196
V29   -29.435633
V30    19.769515
V31   -39.989486
V32   -13.107866
V33    51.914241
V34    80.149233
V35   -14.202544
V36    -9.517552
V37   -47.385110
V38   119.142112
V39    20.074433
V40    89.958796
V41   -28.140562
V42   -53.273361
V43    -4.877024
V44    40.603207
V45   -17.866727
V46    18.684525
V47   -30.466353
V48   -19.560489
V49     1.067519
V50    55.116627
V51   -21.434331
V52    14.356622
V53   -68.778997
V54   -44.394261
V55   -53.794387
V56     7.377138
V57  -143.536046
V58    58.390850
V59    81.867519
V60    35.674868
V61  -197.529940
V62  -114.981992
V63   -17.751886
V64    94.388235
V65   -70.596891
V66  -109.219625
V67   -18.119611
V68   -49.839373
V69   100.988969
V70    17.582368
V71    69.674705
V72   164.079987
V73   -78.115301
V74   160.438142
V75    63.134181
V76   -14.546906
V77   -70.236158
V78   122.603871
V79   109.883725
V80   -58.855777
V81   -56.487888
V82   -31.255832
V83  -256.760982
V84   130.639257
V85     5.999218
V86    40.684721
V87   -93.649551
V88   -14.956964
V89    70.257953
V90    -1.764251
V91    -6.199518
> vals[90] * vecs[,90]  # checking, really eigs?
 [1]   -61.758742   258.334264   661.535190 -1571.494138   200.253407
 [6]   995.270478   743.578505  -163.190599   -36.182942   483.381662
[11]   962.136764  -315.323449   -74.082041   108.246274   -21.070195
[16]   199.580285    19.599410   583.003559  -140.533258   864.815795
[21]    63.410629  -976.671440  -167.462309   -11.415059   -17.166085
[26]   -27.603875   -98.660196   -29.435633    19.769515   -39.989486
[31]   -13.107866    51.914241    80.149233   -14.202544    -9.517552
[36]   -47.385110   119.142112    20.074433    89.958796   -28.140562
[41]   -53.273361    -4.877024    40.603207   -17.866727    18.684525
[46]   -30.466353   -19.560489     1.067519    55.116627   -21.434331
[51]    14.356622   -68.778997   -44.394261   -53.794387     7.377138
[56]  -143.536046    58.390850    81.867519    35.674868  -197.529940
[61]  -114.981992   -17.751886    94.388235   -70.596891  -109.219625
[66]   -18.119611   -49.839373   100.988969    17.582368    69.674705
[71]   164.079987   -78.115301   160.438142    63.134181   -14.546906
[76]   -70.236158   122.603871   109.883725   -58.855777   -56.487888
[81]   -31.255832  -256.760982   130.639257     5.999218    40.684721
[86]   -93.649551   -14.956964    70.257953    -1.764251    -6.199518
> # yes!
```

## Computation: the Power Method

One way to compute eigenvalues and eigenvectors is the *power method*, a
simple iteration. We begin with an initial guess, $x_0$ for an
eigenvector. Substituting in @eq-simpledef, we have the next guess:

$$
x_1 = M x_0
$$

We keep iterating until convergence, generating $x_2$ from $x_1$ and so
on. However, the $x_i$ may grow, so we normalize to length 1:

$$
x_i \leftarrow \frac{x_i}{||x_i||}
$$

## Application: Computation of Long-Run Distribution in Markov Chains

We showed in @sec-markovsolve how matrix inverse can be used to compute
the long-run distribution $\nu$ in a Markov chain. However, this is
inefficient for very large transition matrices. For instance, in Google
PageRank, there is a Markov state for every page on the Web!

Instead, we exploit the fact that @eq-MarkovPi says that the transition
matrix has an eigenvector $\nu$ with eigenvalue 1.  Due to the typical
huge size of the matrix, the power method or a variant is often used.

## Your Turn

❄️  **Your Turn:** Show that the diagonalizing matrix $P$ above must
have determinant $\pm 1$.

❄️  **Your Turn:** Show that if $x$ is an eigenvector of $M$ with
eigenvalue $\lambda \neq 0$, then for any nonzero number $c$, $cx$ will
also be an eigenvector with eigenvalue $\lambda$.

❄️  **Your Turn:** Show that if a matrix $M$ has a 0 eigenvalue, $M$
must be singular. Also prove the converse. (Hint: Consider the column
rank.)

❄️  **Your Turn:** Consider a projection matrix $P_W$. Show that the only
possible eigenvalues are 0 and 1. Hint: Recall that projection matrices
are idempotent.

❄️  **Your Turn:** Say $A$ is an invertible matrix with eigenvalue
$\lambda$ and eigenvector $v$. Show that $v$ is also an eigenvector of
$A^{-1}$, with eigenvalue $1/\lambda$.

❄️  **Your Turn:** Show that if $A$ is nonnegative-definite, its
eigenvalues must be nonnegative.

