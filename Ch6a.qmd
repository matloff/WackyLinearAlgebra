
```{r} 
#| include: false
library(dsld)
library(qeML)
```

{{< pagebreak >}}

# Eigenanalysis {#sec-eigenChap}

::: {.callout-note}

## Goals of this chapter:

The concept of eigenvalues and eigenvectors is one of the most important
of all applications of linear algebra to data science. We introduce the
basic ideas and properties in this chapter. 

:::

## Example: African Soils Data

To get things started, let's consider the African Soils dataset.

Let's take a look around:

```{r}
library(WackyData) 
data(AfricanSoil)
dim(AfricanSoil)
names(AfricanSoil)[1:25]
names(AfricanSoil)[3576:3600]
```

Let's try predicting **pH**, the acidity. But that leaves 3599 possible
predictors. There is an old rule of thumb that one should have 
$p < \sqrt{n}$, for $p$ predictors and $n$ data points,
to avoid overfitting, a rule which in our setting of $n = 1157$
is grossly violated.  We need to do dimension reduction. One way to
accomplish this is to use Principal Components Analysis (PCA). 

## Overall Idea 

The goal is to find a few important linear combinations of our original
predictor variables--important in the sense that they roughly summarize
our data.  These new variables are called the *principal components*
(PCs) of the data. PCA will be covered in detail in the next chapter,
but our preview here will set the stage for important general concepts
that we will develop in the current chapter.

### The first PC{#sec-firstPC}

Let **X** denote a set of variables of interest (not necessarily in a
prediction context).  In searching for good linear combinations, we want
to aim for ones with high variance. We certainly don't want ones with
low variance; after all, a random variable with 0 variance is a
constant.  So we wish to find linear combinations 

$$
Xu
$$

which maximize 

$$
Var(Xu) = u' Cov(X) u
$$

where we have invoked @eq-acova.

But that goal is ill-defined, since we could take larger and larger
vectors $u$, thus larger and larger vectors $Xu$, no maximum variance.
So, let's constrain it to vectors $u$ of length 1:

$$
u'u = 1
$$

Let's use the method of Lagrange multipliers (@sec-lagrange).
In our case, we maximize

$$
u'Cov(X)u + \gamma (u'u - 1)
$$

with respect to $u$ and $\gamma$.

Setting derivatives to 0, we have

$$
0 = 
2 Cov(X) u + 2 \gamma u
$$

In other words,

$$
Cov(X) ~ u = -\gamma u
$${#eq-aha}

We see a situation in which a matrix ($Cov(X)$) times a vector ($u$)
equals a constant ($-\gamma$) times that same vector.  We say that
$-\gamma$ is an *eigenvalue* of the matrix $Cov(X)$, and that $u$ is a
corresponding *eigenvector*.  Seems innocuous, but it opens a huge new
world! 

Note too that from @eq-quadform,

$$
\textrm{maximal variance} = u' Cov(X) u = u' (-\gamma u) = -\gamma
$${#eq-maxvar}

We will return to the notion of principal components in the next
chapter, after laying the groundwork in the current chapter.

## Definition

The concept itself is simple:


> Consider an $n \times  n$ matrix $M$. If there is a nonzero vector
> $x$ and a number $\lambda$ such that
> 
> $$
> Mx = \lambda x
> $${#eq-simpledef}
> 
> we say that $\lambda$ is an *eigenvalue* of $M$, with *eigenvector*
> $x$.

## A First Look

Here are a few properties to start with:

* The definition is equivalent to

  $$
  (M - \lambda I) x = 0
  $$

  which in turn implies that $M - \lambda I$ is noninvertible. That
  then implies that 

  $$
  det(M - \lambda I) = 0
  $${#eq-chareqn}

* As mentioned before, determinants can be defined in the form of
  polynomials. Here the left-hand side of this equation is a polynomial
  in $\lambda$. So for an $n \times  n$ matrix $M$, there are $n$ roots
  of the equation and thus $n$ eigenvalues. Note that some roots may be
  repeated; if $M$ is the zero matrix, say, it will have an eigenvalue $0$
  with multiplicity $n$.

* In principle, that means we can solve the above determinant equation
  to find the eigenvalues of the matrix. There are much better
  ways to calculate the eigenvalues than this, though.

## The Special Case of Symmetric Matrices

Some eigenvalues may be complex numbers, i.e. of the form $a+bi$, but it
can be shown that if $M$ is symmetric, its eigenvalues are guaranteed to
be real. This is good news for Data Science, as many matrices in that
field are symmetric, such as covariance matrices and $A'A$ in
@eq-linregformula. 

::: {#thm-eigreal}

The eigenvalues of a symmetric matrix are real, not complex numbers, i.e. not of
the form $a+bi$.

:::

::: {#thm-eigortho}

The eigenvectors corresponding to distinct eigenvalues of a symmetric matrix
$M$ are orthogonal.

:::

::: {.proof}

Let $u$ and $v$ be such eigenvectors, corresponding to
eigenvalues $\mu$ and $\nu$. Keeping mind that $M'=M$,
$(ab)' = b'a'$ that the transpose of a number is just that number, 
we calculate $u'Mv$ in two different ways.

$$
u'Mv = u' \nu v = \nu u'v 
$$

$$
(u'Mv)' = v'(u'M)' = v'Mu = \mu v'u = \mu u'v
$$

So, $\mu u'v = \nu u'v$, forcing $u'v = 0$.

$\square$

::: 

A square matrix $R$ is said to be *diagonalizable* if there exists an
invertible matrix $P$ such that $P^{-1}RP$ is equal to some diagonal
matrix $D$. We will see that symmetric matrices fall into this category.

One application of this is the computation of powers of a diagonal
matrix:

$$
R^k = (P^{-1}DP) ~ (P^{-1}DP) ~ ... ~ (P^{-1}DP) = P^{-1}D^kP
$$

since $PP^{-1} = I$.

$D^k$ is equal to the diagonal matrix with elements $d_i^k$, so the
computation of $R^k$ is easy.

::: {#thm-symmeig}

Any symmetric matrix $M$ has the following properties:

* $M$ is diagonalizable. 

* In fact, the matrix $P$ is equal to the matrix whose columns are the
  eigenvectors $u_i$ of $M$, chosen to have norm 1. Thus the same holds for
  the rows of $P'$, so that 

  $$
  P'u_i = \lambda_i u_i
  $${#eq-pcols}

* The associated diagonal matrix has as its diagonal elements the
  eigenvalues of $M$. 

* The matrix $P$ has the property that $P^{-1} = P'$.  

* Moreover, the rank of $M$ is equal to the number of nonzero
  eigenvalues of $M$.

:::

::: {.proof}

Use partitioned matrices. 

Let $D = diag(d_1,...,d_m)$, where the $d_i$ are the eigenvalues of $M$,
corresponding to the eigenvectors of $P$. Set these eigenvectors $u_i$ to
have length 1, by dividing by their lengths.  

Recall that the eigenvectors of $M$, i.e. the columns of $P$, are
orthogonal. Again using partitioning, write $P$ as

$$
P = (u_1 | ... | u_m)
$$

so that

$$
P'P = 
 \left (
 \begin{array}{r}
 u_1' \\
 ... \\
 u_m' \\
 \end{array}
 \right )
  (u_1 | ... | u_m)
$$

Then the row $i$, column $j$ element of $P'P$ is equal to either 1 or 0,
according to whether $i=j$ or $i \neq j$.

By the way, any square matrix $Q$ such that $Q'Q = I$ is said to be
*orthogonal*. And, its inverse is its transpose.

Now use partitioning again:

\begin{align}
MP &= M(u_1 | ... | u_m) \\
&= (Mu_1 | ... | Mu_n) \\
&= (d_1 P_1 | ... | (d_m P_m) \\
&= PD
\end{align}

Multiply on the left by $P'$, and we are done:

$$
P'MP = P'P D = D
$$

Regarding rank, @thm-ranknotchange tells us that pre- or postmultiplying
by an invertible matrix does not change rank, and clearly the rank of a
diagonal matrix is the number of nonzero elements.

$\square$

:::

::: {#thm-aatsameeigs}

Any square matrix $A$ and its transpose have the same eigenvalues.

:::

::: {.proof}


If $\lambda$ is an eigenvalue of $A$, then it satisfies @eq-chareqn. But
recall that the determinant of a matrix is equal to the determinant of
its transpose. (For example, we can expand either across the top row or
the leftmost row.) Then $\lambda$ also satisfies

$$
0 = \det[(A - \lambda I)'] = \det[(A' - \lambda I)]
$$

Thus $\lambda$ is also an eigenvalue of $A'$. This argument works in
reverse, or better, note the above shows that $A'$ has least the
eigenvalues of $A$ -- and no more than that, since both have $n$
eigenvalues.

$\square$

:::


## Example: Census Dataset

Let's illustrate all this with the data in @sec-pgtn. We will form the
matrix $A'A$ in @sec-linregformula, which as mentioned, is symmetric.

Recall that in that example, $A$ is not of full rank. Thus we should
expect to see a 0 eigenvalue.

```{r}
library(qeML)
data(svcensus)
svc <- svcensus[,c(1,4:6)]
svc$man <- as.numeric(svc$gender == 'male')
svc$woman <- as.numeric(svc$gender == 'female')
svc$gender <- NULL
a <- as.matrix(svc[,-2]) # look at "X" only, not "Y"
a <- cbind(1,a)  # add the column of 1s
m <- t(a) %*% a
eigs <- eigen(m)
eigs
m %*% eigs$vectors[,1]
eigs$values[1] %*% eigs$vectors[,1]
```

Yes, that first column is indeed an eigenvector, with the claimed eigenvalue. 

Note that the expected 0 eigenvalue shows up as 2.801489e-12, quite
small but nonzero, due to roundoff error.

## Application: Detecting Multicollinearity{#sec-detecting}

Consider the basic eigenanalysis equation,

$$
Ax = \lambda x
$$

for a square matrix $A$, a conformable vector $x$ and a scalar
$\lambda$. Suppose that, roughly speaking, $\lambda x$ is small relative
to $A$. Then

$$
Ax \approx 0
$$

and since $Ax$ is a linear combination of the columns of $A$,
we thus we have found multicollinearity in $A$, flagged by the presence
of a small eigenvalue..

One often sees use of the *condition number* of a matrix, which is the
ratio of the largest eigenvalue to the smallest one. This too might be
used as a suggestion of multicollinearity, though the main usage is as a
signal that matrix operations such finding inverses may have significant
problems with roundoff error.

## Example: Currency Data

This dataset tracks five pre-euro European currencies.

```{r}
library(qeML)
data(currency)
head(currency)
dim(currency)
crc <- currency
crc <- as.matrix(crc)
crcapa <- t(crc) %*% crc
eigs <- eigen(crcapa)
eigs

# that last eigenvalue is much smaller than the others

# illustrate Ax = lambda x approx 0
crcapa %*% eigs$vectors[,5]
eigs$values[5] *  eigs$vectors[,5] 
# is Ax small?
head(crcapa)
```

The value for the franc is much smaller than the others.
So yes, this dataset has some multicollinearity.

## Computation: the Power Method

One way to compute eigenvalues and eigenvectors is the *power method*, a
simple iteration. We begin with an initial guess, $x_0$ for an
eigenvector. Substituting in @eq-simpledef, we have the next guess:

$$
x_1 = M x_0
$$

We keep iterating, generating $x_2$ from $x_1$,, generating $x_3$ from
$x_2$ and so on.  until convergence, meaning that $x_{i+1}$ and $x_i$ do
not differ much from each other.

However, the $x_i$ may grow, so we normalize to length 1:

$$
x_i \leftarrow \frac{x_i}{||x_i||}
$$

Now, after obtaining an eigenvector in this manner, how do we get the
associated eigenvalue? We can do this by calculating the *Rayleigh
Quotient*: Denote the eigenvalue associated with $x$ by $\lambda$. Then

$$
\lambda = \frac{(Ax)' x}{x'x} 
$$

To see this, simply substitute $Ax$ in the numerator by $\lambda x$.

It can be shown that the above iterative process yields the maximal eigenvalue
of $A$. But what about the rest of the eigenvalues? This is achieved via 
*deflation* techniques, which we will not present here.

## Application: Computation of Long-Run Distribution in Markov Chains

We showed in @sec-markovsolve how matrix inverse can be used to compute
the long-run distribution $\nu$ in a Markov chain. However, this is
inefficient for very large transition matrices. For instance, in Google
PageRank, there is a Markov state for every page on the Web!

Instead, we exploit the fact that @eq-MarkovPi says that the transition
matrix has an eigenvector $\nu$ with eigenvalue 1.  Due to the typical
huge size of the matrix, the power method or a variant is often used.

## Your Turn

❄️  **Your Turn:** Take a symmetric matrix $A$ of your choice, use R to find
its eigenvalues and eigenvectors. Verify that the latter are orthogonal,
and that the matrix $P$ formed as in @thm-symmeig does indeed have its
transpose as its inverse. (If you choose a noninvertible $A$, try
another.)

❄️  **Your Turn:** Show that any Markov transition matrix has an
eigenvalue 1, with eigenvector consisting of all 1s.

❄️  **Your Turn:** Show that the diagonalizing matrix $P$ for
a symmetric matrix must have determinant $\pm 1$.

❄️  **Your Turn:** Show that if $x$ is an eigenvector of $M$ with
eigenvalue $\lambda \neq 0$, then for any nonzero number $c$, $cx$ will
also be an eigenvector with eigenvalue $\lambda$.

❄️  **Your Turn:** Show that if a matrix $M$ has a 0 eigenvalue, $M$
must be singular. Also prove the converse. (Hint: Consider the column
rank.)

❄️  **Your Turn:** Consider a projection matrix $P_W$. Show that the only
possible eigenvalues are 0 and 1. Hint: Recall that projection matrices
are idempotent.

❄️  **Your Turn:** Say $A$ is an invertible matrix with eigenvalue
$\lambda$ and eigenvector $v$. Show that $v$ is also an eigenvector of
$A^{-1}$, with eigenvalue $1/\lambda$.

❄️  **Your Turn:** Show that if $A$ is nonnegative-definite, its
eigenvalues must be nonnegative.

❄️  **Your Turn:** @thm-symmeig says that for any symmetric matrix $M$
is diagonalizable: There is an orthogonal matrix $P$ such that
$P' M P$ is equal to a diagonal matrix $D$, the latter consisting of the
eigenvalues of $M$. Use this to show that 

$$
x' M x \leq \lambda_{max} ||x||^{2}
$$

where $\lambda_{max}$ is the largest eigenvalue. Hint: First show that

$$
x'Mx = (Px)' D (Px)
$$

