
```{r}
#| include: false
library(dsld)
library(qeML)
```

{{< pagebreak >}}

# Matrix Rank

In our computations in the latter part of the last chapter, we added a
proviso that $(A'A)^{-1}$ exists. In this chapter, we'll present a
counterexample, which will naturally lead into our covering matrix rank,
and in the next chapter,the basics of vector spaces.

## Example: Census Data{#sec-censusrref}

This dataset is also from **qeML**. It is data for Silicon Valley
engineers in the 2000 Census. Let's focus on just a few columns.

```{r}
data(svcensus) 
head(svcensus) 
svc <- svcensus[,c(1,4:6)] 
head(svc) 
lm(wageinc ~ .,data=svc) 
```

So, we estimate that, other factors being equal, men about paid close to
\$11,000 more than women. This is a complex issue, but for our purposes
here, how did **gender** become **gendermale**, no explicit mention of
women?

Let's try to force the issue:

```{r}
svc$man <- as.numeric(svc$gender == 'male')
svc$woman <- as.numeric(svc$gender == 'female')
svc$gender <- NULL
head(svc)
lm(wageinc ~ .,data=svc)
```

Well, we couldn't force the issue after all. Why not? We hinted above
that $A' A$ may not be invertible. Let's check its row-reduced form.

```{r}
A <- cbind(1,svc[,-2])
A <- as.matrix(A)
ApA <- t(A) %*% A
ApA
```

Is this matrix invertible? Let's apply the elementary row operations
introduced in {@sec-pencilpaper}:

```{r}
library(pracma)
rref(ApA) 
```

Aha! Look at that row of 0s!  The row operations process ended
prematurely. This matrix has no inverse. We say that the matrix has
*rank* 4 -- 4 nonzero rows --  when it needs to be 5; we also say that
the matrix is not of *full rank*. We will return to this point shortly,
but first we formalize introduce the row operations process.

## Reduced Row Echelon Form (RREF) of a Matrix

We formalize and extend @sec-pencilpaper.

### Elementary row operations

These are:

* Multiply a row by a nonzero constant.

* Add a multiple of one row to another.

* Swap two rows.

Again, each operation can be implemented via pre-multiplying the given
matrix by a corresponding elementary matrix. The latter is the result of
applying the given operation to the identity matrix $I$. For example,
here is the matrix corresponding to swapping rows 2 and 3:

$$
\left ( \begin{array}{rrr} 
1 & 0 & 0 \\
0 & 0 & 1 \\
0 & 1 & 0 \\
\end{array} \right )
$$

For example:

```{r}
e <- rbind(c(1,0,0),c(0,0,1),c(0,1,0))
e
a <- matrix(runif(12),nrow=3)
a
e %*% a  # NOT e * a!
```

Yes, rows 2 and 3 were swapped.

### The RREF

By applying these operations to a matrix $A$, we can compute its
*reduced row echelon form* $A_{rref}$, which is defined by the following
properties:

* Each row, if any, that consists of all 0s is at the bottom of the
  matrix.

* The first nonzero entry in a row, called the *pivot*, is a 1.

* Each pivot will be to the right of the pivots in the rows above it.

* All entries below a pivot are 0.

* Each pivot is the only nonzero entry in its column.

The reader should verify that the matrix at the end of 
@sec-censusrref has these properties.

### Review of @sec-pencilpaper{#sec-review}

* Each row operation can be performed via premultiplication by an
elementary matrix. Each such matrix is invertible, and its inverse is
itself an elementary matrix.

* Thus $B_{rref} = E_k... E_2 E_1 B$, for a sequence of  invertible elementary
matrices.

* And 

    $B = (E_1)^{-1} (E_2)^{-1} ... (E_k)^{-1} B_{rref}$,

    where each $(E_i)^{-1}$ is itself an elementary row operation.


## Rank andLinear Dependent/Independent Vectors

### Motivation

Recall that in the nonfull rank example we presented in @sec-noinverse,
one row was double another, 

row2 + 2 row1 = 0

In the census example,

```{r}
head(A)
```

the sum of the last two columns of $A$ was equal to the first column:

column1 - column4 - column5 = 0

Write this more fully as a formal linear combination of the columns of
this matrix:

1 column1 + 0 column2 + 0 column3 + (-1) column4 + (-1) column5 = 0

The vector of coefficients is (1,0,0,-1,-1).

### Formal definitions

We say a set of vectors is *linearly dependent* if some linear
combination of them (excluding the trivial case in which all the
coefficients are 0) equals 0, as in both cases above. 

If no nontrival
linear combination of the vectors is 0,
we say the vectors are
*linearly independent*.

So, here is the formal definition:

<blockquote>

The *rank* of a matrix $B$ is its maximal number of linearly independent 
rows.

</blockquote>

### Relevance

::: {.callout-important}
## A Central Concept
As we will see, linear dependence of the rows or columns is the culprit
in non-full rank matrices. This is a major issue for linear models, in
which an inverse is necessary. 

Moreover, in many cases a matrix will technically be invertible, but
"approximately" nonfull rank. This can wreak havoc in linear models.

The reader can see, then, that understanding of matrix rank is key. That
will be our initial motivation for learning about vector spaces
in the next chapter, which will in turn open the door to many central
concepts and methods of linear algebra.

:::

## Some Properties of Rank

In @sec-censusrref, we found the matrix $A'A$ to be noninvertible, as
its RREF had a row of 0s. We mentioned a relation to rank, but didn't
formalize it, which we will do now. 

### Yes, one can tell the rank of a matrix by looking at its RREF

<blockquote>

The rank of a matrix $B$ is equal to the rank of its RREF, $B_{rref}$.
Thus we can determine the rank of $B$ by counting the nonzero rows of
its RREF.

*Proof:*

Say $B$ is $m \textrm{x} n$ and of rank $k$. Denote its rows by
$r_1,...,r_m$, and consider a linear combination

$$
u_1 r_1 + ... + u_k r_k \neq 0
$$

Now, what happens to this equation as we step through the process of
forming $B_{rref}$? Each step will change one or two of the rows $r_i$. Will
their new versions wlll still be linearly independent?

The answer is yes. If for instance we multiply row $i$ by $c$, and we
set the new value of $u_i$ to be the old one divided by $c$, the new
linear combination will still have the same sum, and thus still have a
nonzero sum.  Swapping two rows will swap the corresponding $u_j$ and so
on.

In other words, any nonzero linear combination of rows in $B$ will
correspond to a nonzero linear combination of the rows of $B_{rref}$.
Moreover, it's all reversible: @sec-review shows that by doing the
inverse elementary operations in reverse, any set of linearly
independent rows in $B_{rref}$ corresponds to some set of linearly
independent rows in $B$.

Putting all this together, we see that the two matrices have the same
number of maximally linearly independent rows, i.e. the two matrices
have the same rank.

### Rank is bounded above by the "narrow" dimension of the matrix

In @sec-censusrref, we found the matrix $A'A$ to not be of full rank.
It is surprising that so was $A$:

```{r}
qr(ApA)$rank  # qr is a built-in R function
qr(A)$rank
```

This is rather startling. $A$ has over 20,000 rows --- yet only 4
linearly independent ones?[There are many subsets of 4 rows that are
linearly independent. But no sets of 5 or more are linearly independent.
]{.column-margin} But it follows from this fact:

<blockquote>

For any $r \textrm{x} s$ matrix $G$, the rank of $G$ is less than or equal to 
$\min(r,s)$.

</blockquote>

We will see shortly why this is true.



