
```{r}
#| include: false
library(dsld)
library(qeML)
```

{{< pagebreak >}}

# Matrix Rank

::: {.callout-note}

## Goals of this chapter:

Many matrices are noninvertible. The subject of this chapter. matrix
*rank* is closely tied to the invertibility issue, and is central to
understanding matrix applications. 

:::

In our computations in the latter part of the last chapter, we added a
proviso that $(A'A)^{-1}$ exists. In this chapter, we'll present a
counterexample, which will naturally lead into our covering matrix rank,
and in the next chapter,the basics of vector spaces.

## Example: Census Data{#sec-censusrref}

This dataset is also from **qeML**. It is data for Silicon Valley
engineers in the 2000 Census. Let's focus on just a few columns.

```{r}
data(svcensus) 
head(svcensus) 
svc <- svcensus[,c(1,4:6)] 
head(svc) 
lm(wageinc ~ .,data=svc) 
```

So, we estimate that, other factors being equal, men about paid close to
\$11,000 more than women. This is a complex issue, but for our purposes
here, how did **gender** become **gendermale**, no explicit mention of
women?

Let's try to force the issue:

```{r}
svc$man <- as.numeric(svc$gender == 'male')
svc$woman <- as.numeric(svc$gender == 'female')
svc$gender <- NULL
head(svc)
lm(wageinc ~ .,data=svc)
```

Well, we couldn't force the issue after all. Why not? We hinted above
that $A' A$ may not be invertible. Let's take a look.

```{r}
A <- cbind(1,svc[,-2])
A <- as.matrix(A)
ApA <- t(A) %*% A
ApA
```

Is this matrix invertible? Let's apply the elementary row operations
introduced in @sec-pencilpaper:

```{r}
library(pracma)
rref(ApA) 
```

Aha! Look at that row of 0s!  The row operations process ended
prematurely. This matrix has no inverse. We say that the matrix has
*rank* 4 -- 4 nonzero rows --  when it needs to be 5; we also say that
the matrix is not of *full rank*. We will return to this point shortly,
but first we formalize introduce the row operations process.

## Reduced Row Echelon Form (RREF) of a Matrix

We formalize and extend @sec-pencilpaper.

### Elementary row operations

These are:

* Multiply a row by a nonzero constant.

* Add a multiple of one row to another.

* Swap two rows.

Again, each operation can be implemented via pre-multiplying the given
matrix by a corresponding elementary matrix. The latter is the result of
applying the given operation to the identity matrix $I$. For example,
here is the matrix corresponding to swapping rows 2 and 3:

$$
\left ( \begin{array}{rrr} 
1 & 0 & 0 \\
0 & 0 & 1 \\
0 & 1 & 0 \\
\end{array} \right )
$$

For example:

```{r}
e <- rbind(c(1,0,0),c(0,0,1),c(0,1,0))
e
a <- matrix(runif(12),nrow=3)
a
e %*% a  # matrix mult. is NOT e * a!
```

Yes, rows 2 and 3 were swapped.

### The RREF

By applying these operations to a matrix $A$, we can compute its
*reduced row echelon form* $A_{rref}$, which is defined by the following
properties:

* Each row, if any, that consists of all 0s is at the bottom of the
  matrix.

* The first nonzero entry in a row, called the *pivot*, is a 1.

* Each pivot will be to the right of the pivots in the rows above it.

* All entries below a pivot are 0.

* Each pivot is the only nonzero entry in its column.

The reader should verify that the matrix at the end of 
@sec-censusrref has these properties.

### Review of @sec-pencilpaper{#sec-review}

* Each row operation can be performed via premultiplication by an
elementary matrix. Each such matrix is invertible, and its inverse is
an elementary matrix.

* Thus 

$$
B_{rref} = E_k ... E_2 E_1 B
$$ {#eq-eeeb}

for a sequence of  invertible elementary matrices.

* And 

    $B = (E_1)^{-1} (E_2)^{-1} ... (E_k)^{-1} B_{rref}$,

    where each $(E_i)^{-1}$ is itself an elementary row operation.


## Rank andLinear Dependent/Independent Vectors

### Motivation

Recall that in the nonfull rank example we presented in @sec-noinverse,
one row was double another, 

row2 + 2 row1 = 0

In the census example,

```{r}
head(A)
```

the sum of the last two columns of $A$ was equal to the first column:

column1 - column4 - column5 = 0

Write this more fully as a formal linear combination of the columns of
this matrix:

1 column1 + 0 column2 + 0 column3 + (-1) column4 + (-1) column5 = 0

The vector of coefficients is (1,0,0,-1,-1).

### Formal definitions

<blockquote>


*Definition:* We say a set of vectors is *linearly dependent* if some
linear combination of them (excluding the trivial case in which all the
coefficients are 0) equals 0, as in both cases above.  If no nontrival
linear combination of the vectors is 0, we say the vectors are *linearly
independent*.

</blockquote>

So, here is the formal definition of rank:

<blockquote>

*Definition* The *rank* of a matrix $B$ is its maximal number of
linearly independent rows.

</blockquote>

### Relevance

::: {.callout-important}

## A Central Concept
As we will see, linear dependence of the rows or columns is the culprit
in non-full rank matrices. This is a major issue for linear models, in
which an inverse is necessary. 

Moreover, in many cases a matrix will technically be invertible, but
"approximately" nonfull rank. This can wreak havoc in linear models.

The reader can see, then, that understanding of matrix rank is key. That
will be our initial motivation for learning about vector spaces
in the next chapter, which will in turn open the door to many central
concepts and methods of linear algebra.

:::

## Some Properties of Rank

In @sec-censusrref, we found the matrix $A'A$ to be noninvertible, as
its RREF had a row of 0s. We mentioned a relation to rank, but didn't
formalize it, which we will do now. 

### Yes, one can tell the rank of a matrix by looking at its RREF{#sec-yessamerank}

::: {#thm-ranknotchange}

## Rank Is Preserved in Pre-/Post-multiplication by Invertible Matrices

Let $A$ be any matrix, and let $V$ and $W$ be square, invertible
matrices with sizes compatible with the products below. Then the ranks
of $VA$ and $AW$ are equal to that of $A$.
:::

::: {.proof}
Say $A$ has rank $r$. Then there exist $r$ linearly independent
columns of $A$. For convenience of notation, say these are the first $r$
columns, and call them $c_1,...,c_r$. Then the first $r$ columns of
$WA$ are $Wc_1,...,Wc_r$, by matrix partitioning. 

Note that for a vector $x$, $Vx = 0$ if and only if $x = 0$. (Just
multiply $Vx = 0$ on the left by $V^{-1}$.) So a linear combination
$\lambda_1 c_1+...+\lambda_r c_r$ is 0 if and only if the corresponding
linear combination $\lambda_1 Vc_1+...+\lambda_r Vc_r$ is 0. Thus $VA$
has rank $r$.

The argument for the case of $AW$ is identical, this time involving rows
of $A$.

$\blacksquare$

:::

This then gives us:

<blockquote>

*Corollary:* The rank of a matrix $B$ is equal to the rank of its RREF,
$B_{rref}$.  Thus we can determine the rank of $B$ by counting the
nonzero rows of its RREF.

*Proof:*

The @thm-ranknotchange
above theorem says that pre- or postmultiplying $B$ will not 
change its rank. Then invoke @eq-eeeb.

</blockquote>


### Row rank and column rank

We have defined the rank of a matrix to be the number of maximally
linearly independent rows. We'll now call that the *row rank*, and
define the *column rank* to be the number of maximally 
linearly independent columns.

<blockquote>

*Theorem:* The column rank of a matrix $B$ is equal to that of its RREF.

*Proof:* Simply follow the steps in the proof for the row rank case
above. (Or apply that case to $B'$.)

</blockquote>

### Rank is bounded above by the "narrow" dimension of the matrix

In @sec-censusrref, we found the matrix $A'A$ to not be of full rank.
It is surprising that so was $A$:

```{r}
qr(ApA)$rank  # qr is a built-in R function
qr(A)$rank
```

This is rather startling. $A$ has over 20,000 rows --- yet only 4
linearly independent ones?[There are many subsets of 4 rows that are
linearly independent. But no sets of 5 or more are linearly independent.
]{.column-margin} But it follows from this fact:

<blockquote>

For any $r \textrm{x} s$ matrix $G$, the rank of $G$ is less than or equal to 
$\min(r,s)$.

</blockquote>

But first, a more far-reaching theorem that will also imply the above:

<blockquote>

*Theorem:* The row rank and column rank of a matrix $B$ are equal.

*Proof:* First, it's clear from looking at the form of the RREF that
both the row rank and the column rank of an RREF are equal to the number
of rows with leading 1s. (Again, see the example at the end of
@sec-censusrref to picture this.) In other words,

$$
rowrank(B_{rref}) = colrank(B_{rref})
$$

But we found earlier that

$$
rowrank(B) =  rowrank(B_{rref})
$$

and

$$
colrank(B) =  colrank(B_{rref})
$$

The result follows.

</blockquote>

And thus:

<blockquote>

*Corollary:* The (common value of) row and column ranks of an $m x n$
matrix $B$ must be less than or equal to the minimum of the number of
rows and columns of $B$ .

</blockquote>

## Your Turn

❄️  **Your Turn:** Consider an $m \textrm{x} n$ matrix $A$ with $m \geq n$.
Then consider the partitioned matrix

$$
 B =
 \left (
 \begin{array}{r}
 A \\
 I \\
 \end{array}
 \right )
$$

where $I is the $n \textrm{x} n$ identity matrix. Prove that $B$ is of
full rank.

❄️  **Your Turn:** Consider the three basic elementary matrices discussed
here: Swap rows $i$ and $j$; multiply row $i$ by a constant $b$; adding
$c$ times row $i$ to row $j$, for a constant $c$. Find general formulas
for the determinants of the three matrices.

❄️  **Your Turn:** Prove that if the matrix $A$ has a 0 row, then
$det(A) =  0$.
