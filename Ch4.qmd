
```{r}
#| include: false
library(dsld)
library(qeML)
```

{{< pagebreak >}}

# Matrix Rank

::: {.callout-note}

## Goals of this chapter:

Many matrices are noninvertible. The subject of this chapter. matrix
*rank* is closely tied to the invertibility issue, and is central to
understanding matrix applications. 

:::

In our computations in the latter part of the last chapter, we added a
proviso that $(A'A)^{-1}$ exists. In this chapter, we'll present a
counterexample, which will naturally lead into our covering matrix rank,
and in the next chapter, the basics of vector spaces.

## Example: Census Data{#sec-censusrref}

This dataset is also from **qeML**. It is data for Silicon Valley
engineers in the 2000 Census. Let's focus on just a few columns.

```{r}
data(svcensus) 
head(svcensus) 
svc <- svcensus[,c(1,4:6)] 
head(svc) 
lm(wageinc ~ .,data=svc) 
```

So, we estimate that, other factors being equal, men about paid close to
\$11,000 more than women. This is a complex issue, but for our purposes
here, how did **gender** become **gendermale**, no explicit mention of
women?

Let's try to force the issue:

```{r}
svc$man <- as.numeric(svc$gender == 'male')
svc$woman <- as.numeric(svc$gender == 'female')
svc$gender <- NULL
head(svc)
lm(wageinc ~ .,data=svc)
```

Well, we couldn't force the issue after all. Why not? We hinted above
that $A' A$ may not be invertible. Let's take a look.

```{r}
A <- cbind(1,svc[,-2])
A <- as.matrix(A)
ApA <- t(A) %*% A
ApA
```

Is this matrix invertible? Let's apply the elementary row operations
introduced in @sec-pencilpaper:

```{r}
library(pracma)
rref(ApA) 
```

Aha! Look at that row of 0s!  The row operations process ended
prematurely. This matrix will be seen to have no inverse.  We say that
the matrix has *rank* 4 -- meaning 4 nonzero rows --  when it needs to
be 5. We also say that the matrix is of *nonfull rank*.

Though we have motivated the concept of matrix rank here with a linear
model example, and will do so below,  the notion pervades all of linear
algebra, as will be seen in the succeeding chapters.

We still have not formally defined rank, just building intuition,
but toward that end, let us first formalize the row operations process.

## Reduced Row Echelon Form (RREF) of a Matrix

We formalize and extend @sec-pencilpaper.

### Elementary row operations

These are:

* Multiply a row by a nonzero constant.

* Add a multiple of one row to another.

* Swap two rows.

Again, each operation can be implemented via pre-multiplying the given
matrix by a corresponding elementary matrix. The latter is the result of
applying the given operation to the identity matrix $I$. For example,
here is the matrix corresponding to swapping rows 2 and 3:

$$
\left ( \begin{array}{rrr} 
1 & 0 & 0 \\
0 & 0 & 1 \\
0 & 1 & 0 \\
\end{array} \right )
$$

For example:

```{r}
e <- rbind(c(1,0,0),c(0,0,1),c(0,1,0))
e
a <- matrix(runif(12),nrow=3)
a
e %*% a  # matrix mult. is NOT e * a!
```

Yes, rows 2 and 3 were swapped.

### The RREF

By applying these operations to a matrix $A$, we can compute its
*reduced row echelon form* $A_{rref}$, which is defined by the following
properties:

* Each row, if any, that consists of all 0s is at the bottom of the
  matrix.

* The first nonzero entry in a row, called the *pivot*, is a 1.

* Each pivot will be to the right of the pivots in the rows above it.

* Each pivot is the only nonzero entry in its column.

The reader should verify that the matrix at the end of 
@sec-censusrref has these properties. 

### Recap of @sec-pencilpaper{#sec-review}

* Each row operation can be performed via premultiplication by an
elementary matrix. Each such matrix is invertible, and its inverse is
an elementary matrix.

* Thus 

  $$
  B_{rref} = E_k ... E_2 E_1 B
  $$ {#eq-eeeb}
  
  for a sequence of  invertible elementary matrices.

* And 

    $B = (E_1)^{-1} (E_2)^{-1} ... (E_k)^{-1} B_{rref}$,

    where each $(E_i)^{-1}$ is itself an elementary row operation.

### Partitioned-matrix view of Reduced Echelon Forms

It is much easier to gain insight from RREF by viewing it in
partitioned-matrix form:

> Say $A$ is of size $m \textrm{ x } n$. Then $A_{rref}$ has the form
> 
> $$
> \left (
> \begin{array}{rr}
> I_{s} & U \\
> 0 & 0  \\
> \end{array}
> \right )
> $${#eq-rref}
> where $I_s$ is an identity matrix of some size $s$
> and $U$ has dimension $s \textrm{ x } (n-s)$.

The Column-Reduced Echelon Form is similar:

$$
A_{cref} =
\left (
\begin{array}{rr}
I_{t} & 0 \\
V & 0  \\
\end{array}
\right )
$${#eq-cref}

where $I_t$ is an identity matrix of some size $t$
and $V$ has dimension $(m-t) \textrm{ x } t$.

Clearly, the row rank of $A_{rref}$ is $s$, and its column rank is $t$.
We will prove shortly that the $s$ is the column rank of $A$ as well.

## Formal Definitions

> *Definition:* A *linear combination* of vectors $v_1,v_2,...v_k$ is a
> sum of scalar multiples of the vectors, i.e.
> 
> $$
> a_1 v_1 + ... + a_k v_k
> $$
> 
> where the $a_i$ are scalars.

(The reader may wish to review @sec-easyops.)

> *Definition:* We say a set of vectors is *linearly dependent* if some
> linear combination of them (excluding the trivial case in which all
> the coefficients are 0) equals 0. If no nontrival linear combination
> of the vectors is 0, we say the vectors are *linearly independent*.

For instance, consider the matrix

$$
\left ( \begin{array}{rrr} 
1 & 3 & 1 \\
1 & 9 & 4 \\
0 & 8 & 0 \\
\end{array} \right )
$$

Denote its columns by $c_1$, $c_2$ and $c_3$. Observe that

$$
(-1) c_1 + (1) c_2 + (-2) c_3 =
\left (
\begin{array}{r}
0 \\
0 \\
0 \\
\end{array}
\right )
$$

Then $c_1$, $c_2$ and $c_3$ are not linearly independent.

So, here is the formal definition of rank:

> *Definition* The *rank* of a matrix $B$ is its maximal number of
> linearly independent rows.

As an example involving real data, recall that in the nonfull rank
let's look again at the census example,

```{r}
head(A)
```

the sum of the last two columns of $A$ is equal to the first column, so

column1 - column4 - column5 = 0

Write this more fully as an explicit linear combination of the columns of
this matrix:

1 column1 + 0 column2 + 0 column3 + (-1) column4 + (-1) column5 = 0

So we have found a linear combination of the columsn of this matrix,
with coefficients (1,0,0,-1,-1), that evaluates to 0. Though we have
defined rank in terms of rows, one can do so in terms of columns as
well:

## Row and Column Rank

We have defined the rank of a matrix to be the number of maximally
linearly independent rows. We'll now call that the *row rank*, and
define the *column rank* to be the number of maximally linearly
independent columns. So for instance the matrix analyzed at the end of
the last section is seen to be of nonfull column rank.

As will be seen below, the row and column ranks will turn out to be equal.
Thus in the sequel, we will use the term *rank* to refer to their common
value. For now, though, the unmodified term "rank" will mean row rank.

## Some Properties of Ranks{#sec-rankprops}

In @sec-censusrref, we found the matrix $A'A$ to be noninvertible, as
its RREF had a row of 0s. We mentioned a relation to rank, but didn't
formalize it, which we will do now. 

::: {#thm-ranknotchange}

Let $A$ be any matrix, and let $V$ and $W$ be square, invertible
matrices with sizes conformable with the products below. Then the column
rank of $VA$ is equal to that of $A$, and the row rank of
$AW$ is equal to that of $A$.

:::

::: {.proof}

Say $A$ has column rank $r$. Then there exist $r$ (and no more than $r$)
linearly independent columns of $A$. For convenience of notation, say 
these are the first $r$ columns, and call them $c_1,...,c_r$. Then 
the first $r$ columns of $VA$ are $Vc_1,...,Vc_r$, by matrix partitioning. 

Note that for a vector $x$, $Vx = 0$ if and only if $x = 0$. (Just
multiply $Vx = 0$ on the left by $V^{-1}$.) So a linear combination
$\lambda_1 c_1+...+\lambda_r c_r$ is 0 if and only if the corresponding
linear combination $\lambda_1 Vc_1+...+\lambda_r Vc_r$ is 0. Thus 
the vectors $Vc_i$ are linearly independent, and $VA$ has column rank $r$.

The argument for the case of $AW$ is identical, this time involving rows
of $A$.

$\square$

:::


::: {#thm-twocolranks}

The column rank of a matrix $B$ is equal to the column rank
of its RREF, $B_{rref}$, which in turn is $s$ in @eq-rref.

:::

::: {.proof}

The above @thm-ranknotchange says that pre-postmultiplying $B$ will not
change its column rank. Then invoke @eq-eeeb.

It is clear that any column in $U$ in @eq-rref can be written as a
linear combination of the columns of $I_s$. Thus the column rank of $B$
is $s$.

$\square$

:::

::: {#lem-elemnotchangerank} 

An elementary row operation on a matrix leaves the row rank unchanged.  

:::

::: {.proof}
Let $r_i, i=1,...,m$ denote the rows of the matrix. Consider a 
linear combination

$$
a_1 r_1 + ... + a_m r_m \neq 0
$$

For any of the three elementary operations, a slightly modified set of
the $a_i$ works (i.e. will be nonzero), using the modified elementary
matrix:

* Swap rows $i$ and $j$: Swap $a_i$ and $a_j$.

* Multiplying row $i$ by a constant $c$. Since $r_i \rightarrow c r_i$,
set $a_i \rightarrow (1/c) a_i$.

* Add *c* times row $j$ to row $i$: Set $a_j \rightarrow a_j - c a_j$.

$\square$

:::


::: {#thm-tworankseq} 
## The row rank and column rank of a matrix are equal.
:::

::: {.proof}

The lemma shows that every nonzero linear combination of the rows of $A$
corresponds to one for the rows of $A_{rref}$, and vice versa.  Thus the
row rank of $A$ is the same as that of $A_{rref}$.  But that is $s$ in
@eq-rref, which we found earlier was equal to the column rank of $A$.

$\square$

:::

In @sec-censusrref, we found the matrix $A'A$ to not be of full rank.
It is surprising that so was $A$:

```{r}
qr(ApA)$rank  # qr is a built-in R function
qr(A)$rank
```

This is rather startling. $A$ has over 20,000 rows --- yet only 4
linearly independent ones?[There are many subsets of 4 rows that are
linearly independent. But no sets of 5 or more are linearly independent.
]{.column-margin} But it follows from this fact:


::: {#thm-narrowfat}

The (common value of) row and column ranks of an $m \textrm{ x } n$
matrix $B$ must be less than or equal to the minimum of the number of
rows and columns of $B$ .

:::

::: {.proof}

The row rank of $b$ equals its column rank. The former is bounded above
by $m$ while the latter's bound is $n$.

$\square$

:::

::: {#thm-rankapa}

The matrix $A'A$ has the same rank as $A$.

:::

::: {.proof}

Using the column analog of @eq-eeeb, we can write 

$$
A_{cref} = AF
$$

where $F$ is an invertible product of matrices for elementary column
operations. Then

$$
(A_{rref})' A_{rref}) = F'A'AF
$$

@thm-ranknotchange tells us that the rank of $F'A'A$ has the same rank
as $A'A$, and $F'A'AF$ has the same rank as $F'A'A$, in turn the same
rank as $A'A$. So we can concentrate on $(A_{rref})' A_{rref})$

Using @eq-rref, we have

$$
(A_{rref})' A_{rref})  = 
\left (
\begin{array}{rr}
I_s & U \\
U' & U'U  \\
\end{array}
\right )
$$

The presence of $I_s$ tells us there are at least $s$ linearly
independent rows, thus rank at least $s$. So the rank of $A'A$ is at
least that of $A$. 

On the other hand, again pply our knowlege of partitioning. 

* Say $A$ is $m \textrm{ x } n$. Write

  $$
  A'A = 
  \left (
  \begin{array}{r}
  c_1 \\
  ... \\
  c_n \\
  \end{array}
  \right )
  A =
  \left (
  \begin{array}{r}
  c_1 A \\
  ... \\
  c_n A \\
  \end{array}
  \right )
  $$
  
  where the $c_i$ are the columns of $A$, thus the rows of $A'$.

  Thus each row $c_i A$ of $A'A$ is a linear combination of the rows of $A$. 

* Recall that the rank of $A'A$ is its maximal number of linearly
  independent rows. We saw above that each row of $A'A$ is a linear
  combination of the rows of $A$. Thus in turn, each linear combination 
  of the rows of $A'A$ is a linear combination of linear combinations of 
  rows of $A$ -- still a linear combination of the rows of $A$! 

* At most $s$ members of that latter linear combination are
  linearly independent. In other words, the rank of $A'A$ is 
  at most $s$.

* Thus the rank of $A'A$ is also $s$.

$\square$

:::

## Your Turn

❄️  **Your Turn:** Show that a set of vectors is linearly dependent if
and only if one of the vectors is a linear combination of the others.

❄️  **Your Turn:** Consider an $m \textrm{ x } n$ matrix $A$ with $m \geq n$.
Then consider the partitioned matrix

$$
 B =
 \left (
 \begin{array}{r}
 A \\
 I \\
 \end{array}
 \right )
$$

where $I is the $n \textrm{ x } n$ identity matrix. Prove that $B$ is of
full rank.

❄️  **Your Turn:** Consider the three basic elementary matrices discussed
here: Swap rows $i$ and $j$; multiply row $i$ by a constant $b$; adding
$c$ times row $i$ to row $j$, for a constant $c$. Find general formulas
for the determinants of the three matrices.

❄️  **Your Turn:** Prove that if the matrix $A$ has a 0 row, then
$det(A) =  0$.

