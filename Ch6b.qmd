
```{r} 
#| include: false
library(dsld)
library(qeML)
```

{{< pagebreak >}}

# Principal Components 

::: {.callout-note}

## Goals of this chapter:

It turns out that eigenanalysis can be quite useful for dimension
reduction, by determining the *principal components* of our data. This
is the focus of the current chapter.

::: 

So in the last chapter we had a very brief introduction to Principal
Components Analysis (PCA), and that finding the first component entails
finding an eigenvector and eigenvalue.  It turns out that this is true
for the second, third and all the components. In other words, PCA is
basically an eigenvectors and eigenvalues application.

### The second, third etc. PCs

Say we have a dataset $X$ in matrix form.  There are as many principal
components as there are columns in $X$. So, how are the second, third
and so on components defined and computed?

The key issue is that we want our PCs to be orthogonal, because as we
have seen orthogonal random vectors are statistically uncorrelated, and
independent in the multivariate normal case. So if we summarize our data
with say, the first few PCs, the fact that they are (at least loosely
speaking) uncorrelated makes for a neater summary.

So with $u$ and $v$ denoting the first and second PCs, we want $v$ 
to maximize $v'Cov(X)v$ so we set the derivative of

$$
v'Cov(X)v + \omega (v'v - 1) + \tau (u'v - 0)
$$

with respect to $v$ to 0. 

$$
0 = 2 Cov(X) v + 2 \omega v + \tau u 
$${#eq-eig2a}

Pre-multiply by $u'$:

$$
0 = 2 u' Cov(X) v + 2 \omega u'v + \tau u'u = 2 u' Cov(X) v + \tau 1
$${#eq-eig2b}

Also, since $Cov(X)$ is symmetric,

$$
u' Cov(X) v = [Cov(X) u]' v = \gamma u' v = 0
$$

So @eq-eig2b now tells us that $\tau = 0$, and thus @eq-eig2a 
reduces to

$$
Cov(X) v = -\omega v 
$${#eq-eig2c}

showing that the second PC is again an eigenvector of $Cov(X)$.

## Properties

The full set of eigenvectors is then known as the *principal
components* (PCs). They have some further important properties:

> * The PCs are orthogonal to each other. This follows from our earlier
>   finding that eigenvectors of a symmetric matrix, in this case
>   $Cov(X)$, are orthogonal. We say that a symmetric is *orthogonal*
>   if its columns are orthogonal as vectors.
> 
> * This in turn implies that the PCs are uncorrelated, and in the
>   multivariate normal case, statistically independent.  
> 
> * The eigenvalues are the variances of the PCs. The PCs are in order of
>   decreasing (or at least nonincreasing) variance. 
> 
>   This first statement follows from pre-mutiplying @eq-aha by $u'$,
>   giving us $Var(u)$ on the left and $-\lambda u'u = -\lambda$ on the
>   right. The second statement follows from the fact that as we go from
>   the first PC to the second, third and so on, we are maximizing under
>   more and more constraints, hence smaller maxima.

## Eigenanalysis Relation between $A$ and $Cov(X)$ in Linear Regression{#sec-eiglm}

Consider our usual linear regression setting, in which the matrix $A$
contains the data for our predictor variables $X$.  If we center and
scale our data, then 

$$
Cov(X) = A'A 
$$

To see an example of the implications of this, say the PCA of X has a 0
eigenvalue. That implies that some linear combination $u'X$ has 0
variance. Then

$$
0 = Var(u'X) = u' Cov(X) u = u' A'A u = (Au)'Au
$$

Thus 

$$
Au = 0
$$

Since $Au$ is a linear combination of the columns of $A$, we see that
the columns of $A$ are linearly dependent.

The same argument works in reverse. If $Au = 0$, we fine that $Var(u'X)
= 0$, and since the eigenvalues of $Cov(X)$ are variances of linear
combinations of $X$, we see that one of those variances is 0.

## Back to the African Soils Example

So, we remove the ``Y'' variable, **pH**, number 3598 as seen above, and
proceed. We will also remove the nonnumeric columns,
**PIDN** and **Depth**. We could use R's **prcomp** function here, but
to better illustrate the concepts, we do things from scratch.

```{r} 
library(WackyData)
data(AfricanSoil)
x <- AfricanSoil[,-c(1,3595,3598)]
dim(x)
xcov <- cov(x)
eigs <- eigen(xcov)
eigs$values[1:100]  # don't print out all 3597!
```

Ah, the eigenvalues fall off rapidly after the first few. So, let's say
we decide to use the first 9 $u$ vectors.  

```{r}
eigvecs <- eigs$vectors[,1:9]
dim(eigvecs)
```

So, recalling the sequence of equations leading to @eq-aha, we are ready
to replace our old variables by the new ones, which are linear
combinations of the old ones.

```{r} 
x <- as.matrix(x)
dim(x)
xnew <- x %*% eigvecs
dim(xnew)  
lmout <- lm(AfricanSoil$pH ~ xnew)
# and so on
```

## PCA in Detection of Multicollinearity{#sec-detect}

Recall the itemized list in @sec-nearly of various conditions 
under which we have multicollinearity. We can add the following item to
that list:  

* $A$ has an eigenvalue that is nearly 0.

This follows from what we found about 0 eigenvalues in @sec-eiglm.     
If an eigenvalue is 0, the rank is exactly less than full. If it is
approximately 0, the rank is technically full, but there is a linear
combination of the variables that is approximately 0.

In fact, PCA can warn us of specific linear combinations of $Cov(A'A)$
that are nearly 0, as follows.

* Say a PC corresponds to a small eigenvalue.

* Then from @sec-firstPC that linear combination has small variance.

* Random variables with small variance are nearly constant.

* Thus with high probability, the linear combination is near to $c$
for some number $c$. If we are working with numerical data, that number
$c$ is nonzero with high probability.

* If our design matrix $A$ includes a 1s column, then the above linear
  combination, together with this 1s column, gives us a linear
  combination that is usually close to the 0 vector, thus
  multicollinear.

In other words, PCA can point out to us specific linear combinations of
our variables that may be problematic. If we are considering solving the
problem by removing one or more predictor variables, this analysis could
suggest which ones to remove.


## How Many Principal Components Should We Use?

We chose to use the first 9 principal components in our example here,
but that was just for illustration purposes.  There are no formal rules
for how many to use. Various "rules of thumb" exist. 

If we are doing prediction, there is a very natural way to choose our
number of PCs -- do cross-validation (@sec-xval), and use whichever 
number gives the most accurate prediction.

### Example: New York City taxi trips{#sec-nyc}

This is a well-known dataset, a version of which is included in the
**qeML** package. The object is to predict trip time, given pickup and
dropoff locations, trip distance and day of the week.

In the raw form of the data, there are just 5 columns, thus 4
predictors.  But the pickup and dropoff locations are R factors, coding
numerous locations. These must be decoded to dummy variables (values 1
or 0, coding whether or not, say, a given trip began at pickup location
121). If for instance one calls the R linear regression function **lm**,
that function will do the conversion internally, but in our case we will
perform the conversion ourselves, using **regtools::factorsToDummies**.
(The **regtools** package is included by **qeML**.)

```{r}
library(qeML)
data(nyctaxi)
dim(nyctaxi)
nyc <- factorsToDummies(nyctaxi,dfOut=T)
dim(nyc)
```

Wow! That's quite a lot of predictors, and well in excess of the common
rule of thumb that one should have no more than $\sqrt{n}$ predictors,
100 in this case.

So, we might try dimension reduction via PCA, then use the PCs as
predictors. The function **qeML::qePCA** combines these two operations.
Let's see how it works.

```{r}
args(qePCA)
```

Here **qeName** indicates which function is desired for prediction, e.g.
**qeLin** for a linear model. (This function wraps **lm**.) But a key
argument here is **pcaProp**. Recall that:

* The PCs come in order of decreasing variance.

* We are mainly interested in the first few PCs. The later ones have
small variance, which makes them approximately constant and thus of no
use to us.

The name 'pcaProp' stands for "proportion of total variance." If we set
this to, say, 0.25, we are saying "Give us whatever number of the first
few PCs that have a total variance of at least 25% of the total." Let's
give that a try:

``` r 
qePCA(nyc,'tripTime','qeLin',pcaProp=0.25)$testAcc
[1] 311.9159
```

We asked to predict the column 'tripTime' in the dataset **nyc** using
the **qeLin** function, based on as many of the PCs that will give us
25% of the total variance.  Most **qeML** prediction functions split the
data into a training set and a holdout set.  The model is fit to the
training set, and then applied to prediction of the holdout set. The
output value is the mean absolute prediction error (MAPE).

However, since the holdout set is randomly generated, the MAPE value is
random, so we should do multiple runs. Some experimentation showed that
MAPE here is highly variable, so we decided to perform 500 runs, e.g.

``` r  
mean(sapply(1:500,function(i) qePCA(nyc,'tripTime','qeLin',pcaProp=0.1)$testAcc)) 
[1] 359.663
```

| pcaProb | MAPE | 
|---------|:-----|
| 0.1      | 359.6630  | 
| 0.2      | 359.3068  | 
| 0.3      | 403.9878  | 
| 0.4      | 397.3783  |
| 0.5      | 430.8958  | 
| 0.6      | 423.7299  |
| 0.7      | 517.7917  | 
| 0.8      | 969.1304  |
| 0.9      | 2275.091  | 

: Mean Absolute Predictive Error 

Among other things, this shows the dangers of overfitting, in this case
using too many PCs in our linear regression model. It seems best here to
use only the first 10 or 20% of the PCs.

## Your Turn

❄️  **Your Turn:** Modify the code for **qePCA** for the case of linear
regression by adding a component **xCoeffs** to its return value. This
will give the regression coefficients in terms of the original X
predictors.

❄️  **Your Turn:** Say the symmetric matrix $A$ has *block diagonal* form

$$
 A = 
 \left (
 \begin{array}{rrr}
 A_1 & 0 & 0 ... \\
 0 & A_2 & 0 ...  \\
 ...
 \end{array}
 \right )
$$

where $A_i$ ($i=1,...,r)$ 

* is symmetric 

* is of size $k_i \textrm{ x } k_i$

* has eigenvalues $\gamma_{1},...,\gamma_{k_i}$

* has eigenvectors $u_{1,j},...,u_{k,j}$, where $j=1,...,k_i$

State the form of the eigenvalues and eigenvectors of $A$.

❄️  **Your Turn:** In @sec-detect,
there was a statement "$c$ is nonzero with high probability." Why was
that important?

❄️  **Your Turn:** Use PCA to do dimension reduction on the **s50**
dataset.
