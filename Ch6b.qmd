
```{r} 
#| include: false
library(dsld)
library(qeML)
```

{{< pagebreak >}}

# Principal Components {#sec-pca}

::: {.callout-note}

## Goals of this chapter:

It turns out that eigenanalysis can be quite useful for dimension
reduction, by determining the *principal components* of our data. This
is the focus of the current chapter.

::: 

So in the last chapter we had a very brief introduction to Principal
Components Analysis (PCA), and that finding the first component entails
finding an eigenvector and eigenvalue.  It turns out that this is true
for the second, third and all the components. In other words, PCA is
basically an eigenvectors and eigenvalues application.

## The Second, Third Etc. PCs

Say we have a dataset $X$ in matrix form, again using the **mtcars**
data for concreteness.

```{r}
head(mtcars)
```

There are as many principal
components as there are columns in $X$. We derived the first PC in
@sec-firstPC.  So, how are the second, third and so on components defined
and computed?

The key issue is that we want our PCs to be orthogonal, because as we
have seen, orthogonal random vectors are statistically uncorrelated, and
independent in the multivariate normal case. So if we summarize our data
with say, the first few PCs, the fact that they are uncorrelated makes
for a neater summary, in a sense no duplication.

So with $u$ and $v$ denoting the first and second PCs, we want $v$ 
to maximize $v'Cov(X)v$, so we set the derivative of

$$
v'Cov(X)v + \omega (v'v - 1) + \tau (u'v - 0)
$$

with respect to $v$ to 0. (Note the two Lagrange variables, $\omega$ and
$\tau$.)

$$
0 = 2 Cov(X) v + 2 \omega v + \tau u 
$${#eq-eig2a}

Pre-multiply by $u'$:

$$
\begin{aligned}
0 &= 2 u' Cov(X) v + 2 \omega u'v + \tau u'u \\
&= 2 u' Cov(X) v + \tau 
\end{aligned}
$${#eq-eig2b}

Also, since $Cov(X)$ is symmetric and $u$ is an eigenvector of $Cov(X)$,
say with eigenvalue $\gamma$,

$$
u' Cov(X) v = [Cov(X) u]' v = \gamma u' v = 0
$${#eq-eig2c}

So @eq-eig2b and @eq-eig2c now tell us that $\tau = 0$, and thus @eq-eig2a 
reduces to

$$
Cov(X) v = -\omega v 
$${#eq-eig2c}

showing that the second PC is again an eigenvector of $Cov(X)$.
@eq-maxvar then says that $-\omega$ is the corresponding variance.

The story is the same for the remaining PCs. We thus have:

::: {#thm-thestory}

The principle components have the following properties:

* They are orthogonal to each other.

* They are eigenvectors of $Cov(X)$.

* The eigenvalue corresponding to a PC $z$ is $Var(z'X)$.

* The eigenvalues form a nondecreasing sequence.

* Say $X$ is of length $m$. Normalize the PCs $u_i$ (i.e. divide a PC by
  its length, so that we have a vector of length 1) and form the $m
  \times m$ (partitioned) matrix

  $$
  P = (u_1 | u_2 | ... | u_m)
  $$

  Then 

  $$
  P' Cov(X) P = D
  $$ {#eq-diagonalize}

  where $D$ is a diagonal matrix whose entries 
  are $Var(u_i'X)$, and $P'P = I$.

:::

## Eigenanalysis Relation between $A$ and $Cov(X)$ in Linear Regression{#sec-eiglm}

Consider our usual linear regression setting, in which the matrix $A$
contains the data for our predictor variables $X$. Say we are in the
*random-X* setting discussed in @sec-varbhat.  If we center and scale
our data, then 

$$
\frac{1}{n} \widehat{Cov}(X) = A'A 
$$

To see an example of the implications of this, say the PCA of X has a 0
eigenvalue. That implies that some linear combination $u'X$ has 0
variance. Then

$$
0 = Var(u'X) = u' Cov(X) u = u' A'A u = (Au)'Au
$$

Thus 

$$
Au = 0
$$

Since $Au$ is a linear combination of the columns of $A$, we see that
the columns of $A$ are linearly dependent.

The same argument works in reverse. If $Au = 0$, we fine that $Var(u'X)
= 0$, and since the eigenvalues of $Cov(X)$ are variances of linear
combinations of $X$, we see that one of those variances is 0.

## Back to the African Soils Example{#sec-backtoafrica}

So, we remove the ``Y'' variable, **pH**, number 3598 as seen above, and
proceed. We will also remove the nonnumeric columns,
**PIDN** and **Depth**. We could use R's **prcomp** function here, but
to better illustrate the concepts, we do things from scratch.

```{r} 
library(WackyData)
data(AfricanSoil)
x <- AfricanSoil[,-c(1,3595,3598)]
dim(x)
xcov <- cov(x)
eigs <- eigen(xcov)
eigs$values[1:100]  # don't print out all 3597!
```

Ah, the eigenvalues fall off rapidly after the first few. So, let's say
we decide to use the first 9 $u$ vectors.  

```{r}
eigvecs <- eigs$vectors[,1:9]
dim(eigvecs)
```

So, recalling the sequence of equations leading to @eq-aha, we are ready
to replace our old variables by the new ones, which are linear
combinations of the old ones.

```{r} 
x <- as.matrix(x)
dim(x)
xnew <- x %*% eigvecs
dim(xnew)  
lmout <- lm(AfricanSoil$pH ~ xnew)
# and so on
```

## PCA in Detection of Multicollinearity{#sec-detect}

Recall the itemized list in @sec-nearly of various conditions 
under which we have multicollinearity. We can add the following item to
that list:  

* $A$ has an eigenvalue that is nearly 0.

This follows from what we found about 0 eigenvalues in @sec-eiglm.     
If an eigenvalue is 0, the rank is exactly less than full. If it is
approximately 0, the rank is technically full, but there is a linear
combination of the variables that is approximately 0.

In fact, PCA can warn us of specific linear combinations of $Cov(A'A)$
that are nearly 0, as follows.

* Say a PC corresponds to a small eigenvalue.

* Then from @sec-firstPC that linear combination has small variance.

* Random variables with small variance are nearly constant.

* Thus with high probability, the linear combination is near to $c$
  for some number $c$. If we are working with numerical data, that number
  $c$ is nonzero with high probability (see @sec-prob0).

* If our design matrix $A$ includes a 1s column, then the above linear
  combination, together with this 1s column, gives us a linear
  combination that is usually close to the 0 vector, thus
  multicollinear.

In other words, PCA can point out to us specific linear combinations of
our variables that may be problematic. If we are considering solving the
problem by removing one or more predictor variables, this analysis could
suggest which ones to remove.

## An Important Property of All-Numeric Predictor Data{#sec-prob0}

Say we throw a dart at the interval (0,1). If the distribution of the
hitting location $D$ is U(0,1) or any other continuous distribution, the
probability of hitting a specific point is 0, e.g. $P(D = 0.324) = 0$.
Such is the nature of continuous distributions. The problem is that the
length of the line segment (0.324,0.324) is 0. In math, we say it has
*measure* 0.

If we throw our dart at the unit square, $(0,1) \times (0,1)$,
then e.g. the probability that $D$ lies on a specified straight line or
some other specified curve is also 0. Here the problem is that the area
of that line or curve is 0.

This has implications for material in this book. For instance, consider
linear regression models. If our $A$ matrix consists of all numeric
data, then (in the absence of perfect correlation among the columns), we
will have 

$$
P(determinant(A'A) = 0) = 0
$$

for the same 0-measure reasons. In other words, $A'A$ will be invertible
with probability 1.

## How Many Principal Components Should We Use?

We chose to use the first 9 principal components in our example here,
but that was just for illustration purposes.  There are no formal rules
for how many to use. Various "rules of thumb" exist. 

If we are doing prediction, there is a very natural way to choose our
number of PCs -- do cross-validation (@sec-xval), and use whichever 
number gives the most accurate prediction.

### Example: New York City taxi trips{#sec-nyc}

This is a well-known dataset, a version of which is included in the
**qeML** package. The object is to predict trip time, given pickup and
dropoff locations, trip distance and day of the week.

In the raw form of the data, there are just 5 columns, thus 4
predictors.  But the pickup and dropoff locations are R factors, coding
numerous locations. These must be decoded to dummy variables (values 1
or 0, coding whether or not, say, a given trip began at pickup location
121). If for instance one calls the R linear regression function **lm**,
that function will do the conversion internally, but in our case we will
perform the conversion ourselves, using **regtools::factorsToDummies**.
(The **regtools** package is included by **qeML**.)

```{r}
library(qeML)
data(nyctaxi)
dim(nyctaxi)
nyc <- factorsToDummies(nyctaxi,dfOut=T)
dim(nyc)
```

Wow! That's quite a lot of predictors, and well in excess of the common
rule of thumb that one should have no more than $\sqrt{n}$ predictors,
100 in this case.

So, we might try dimension reduction via PCA, then use the PCs as
predictors. The function **qeML::qePCA** combines these two operations.
Let's see how it works.

```{r}
args(qePCA)
```

Here **qeName** indicates which function is desired for prediction, e.g.
**qeLin** for a linear model. (This function wraps **lm**.) But a key
argument here is **pcaProp**. Recall that:

* The PCs come in order of decreasing variance.

* We are mainly interested in the first few PCs. The later ones have
small variance, which makes them approximately constant and thus of no
use to us.

The name 'pcaProp' stands for "proportion of total variance." If we set
this to, say, 0.25, we are saying "Give us whatever number of the first
few PCs that have a total variance of at least 25% of the total." Let's
give that a try:

``` r 
qePCA(nyc,'tripTime','qeLin',pcaProp=0.25)$testAcc
[1] 311.9159
```

We asked to predict the column 'tripTime' in the dataset **nyc** using
the **qeLin** function, based on as many of the PCs that will give us
25% of the total variance.  Most **qeML** prediction functions split the
data into a training set and a holdout set.  The model is fit to the
training set, and then applied to prediction of the holdout set. The
output value is the mean absolute prediction error (MAPE).

However, since the holdout set is randomly generated, the MAPE value is
random, so we should do multiple runs. Some experimentation showed that
MAPE here is highly variable, so we decided to perform 500 runs, e.g.

``` r  
mean(sapply(1:500,function(i) qePCA(nyc,'tripTime','qeLin',pcaProp=0.1)$testAcc)) 
[1] 359.663
```

| pcaProb | MAPE | 
|---------|:-----|
| 0.1      | 359.6630  | 
| 0.2      | 359.3068  | 
| 0.3      | 403.9878  | 
| 0.4      | 397.3783  |
| 0.5      | 430.8958  | 
| 0.6      | 423.7299  |
| 0.7      | 517.7917  | 
| 0.8      | 969.1304  |
| 0.9      | 2275.091  | 

: Mean Absolute Predictive Error 

Among other things, this shows the dangers of overfitting, in this case
using too many PCs in our linear regression model. It seems best here to
use only the first 10 or 20% of the PCs.


## A "Square Root" Matrix, and MV Normal Simulation{#sec-squareroot}

::: {#thm-sqrt}
Any covariance matrix $A$ has a "square root" matrix $Q$, i.e.
$$
Q^2 = A
$$

:::

::: {.proof}

From @thm-thestory, we have $P' A P = D$ for some matrix $P$ and
diagonal matrix $D$, with the entries of the latter being the variances
of the PCs $\sigma_i^2$.  That latter point implies that

$$
D_1^2 = D
$$

where $D_1 = diag(\sigma_1,\sigma_2,...,\sigma_n)$. Then setting
$Q = P D_1 P'$, we have

$$
Q^2 = (P D_1 P') (P D_1 P') = P D_1^2 P' = P D P' = A
$$

### Implications for simulation of multivariate normal X

Say we wish to write code to simulate a random vector $Q$ having an
m-variate normal distribution with mean vector $\mu$ and covariance
matrix $\Sigma$. Here is how it works:

* We start with generating $Z$, a vector o $m$ independent N(0,1)
  variables. That means $Z$ is m-variate normally distributed, with
  mean vector consisting of $m$ 0s, and covariance matrix $I$, the
  $m \times m$ identity matrix.

* We compute $W = \Sigma^{0.5} Z$. By @eq-covax, $W$ will again be
  multivariate normally distributed, with mean vector consisting of $m$ 0s, 
  and covariance matrix equal to

  $$
  \Sigma^{0.5} I \Sigma^{0.5} = \Sigma
  $$

  Our solution is then

  $$
  Q = \mu + \Sigma^{0.5} Z
  $$




$\square$
:::

## Your Turn

❄️  **Your Turn:** Use PCA to do dimension reduction on the **s50**
dataset.

❄️  **Your Turn:** The reader has likely seen the concept of a
*cumulative distribution function* (CDF). For a scalar random variable
$X$, this is $F_x(t) = P(X \leq t)$. If $X$ is an $m$-dimensional random
vector, the definition is

$$
F_X(t_1,...,t_m) = P(X_1 \leq t_1,...,X_m \leq t_m)
$$

Write an R function with call form

``` r
multiCDF(mu,Sigma,t,n)
```

that uses simulation to evaluate the multivariate CDF, where $t =
(t_1,...,t_m)$ and $n$ is the number of replications to simulate.

❄️  **Your Turn:** Say $X$ has mean vector $\mu$ and covariance matrix
$\Sigma$. Show how we can use @thm-sqrt to find a square, constant
matrix $A$ such that $Cov(AX) = I$. If in addition $X$ has a
multivariate normal distribution, then $AX$ will then consist of
independent random variables with variance 1. Explain why.

❄️  **Your Turn:** Modify the code for **qePCA** for the case of linear
regression by adding a component **xCoeffs** to its return value. This
will give the regression coefficients in terms of the original X
predictors.

❄️  **Your Turn:** Say the symmetric matrix $A$ has *block diagonal* form

$$
 A = 
 \left (
 \begin{array}{rrr}
 A_1 & 0 & 0 ... \\
 0 & A_2 & 0 ...  \\
 ...
 \end{array}
 \right )
$$

where $A_i$ ($i=1,...,r)$ 

* is symmetric 

* is of size $k_i \times k_i$

* has eigenvalues $\gamma_{1},...,\gamma_{k_i}$

* has eigenvectors $u_{1,j},...,u_{k,j}$, where $j=1,...,k_i$

State the form of the eigenvalues and eigenvectors of $A$.

❄️  **Your Turn:** In @sec-detect,
there was a statement "$c$ is nonzero with high probability." Why was
that important?

❄️  **Your Turn:** Write an R function with call form

``` r
bestPCAPred(data,yName)
```

It will apply PCA to the X portion of **data**, then apply **lm**
successivly to the first PC, then the first two, then the first three
and so on assessing with cross-validation in each case. It will then
return the number of PCs (and the PCs themselves) that predicts best.
Try your function on various datasets.

❄️  **Your Turn:** Say the symmetric $n \times n$ matrix $A$ has
rank $r$. Show that $n-r$ of its eigenvalues are 0s.

