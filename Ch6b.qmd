
```{r} 
#| include: false
library(dsld)
library(qeML)
```

{{< pagebreak >}}

# Principal Components 

::: {.callout-note}

## Goals of this chapter:

It turns out that eigenanalysis can be quite useful for dimension
reduction, by determining the *principal components* of our data. This
is the focus of the current chapter.

::: 


## Example: African Soils Data

To get things started, let's consider the African Soils dataset.

Let's take a look around:

```{r}
library(WackyData) 
data(AfricanSoil)
dim(AfricanSoil)
names(AfricanSoil)[1:25]
names(AfricanSoil)[3576:3600]
```

Let's try predicting **pH**, the acidity. But that leaves 3599 possible
predictors. There is an old rule of thumb that one should have 
$p < \sqrt{n}$, for $p$ predictors and $n$ data points,
to avoid overfitting, a rule which in our setting of $n = 1157$
is grossly violated.  We need to do dimension reduction. One way to
accomplish this is to use PCA. 

(Note, though, that this presumes our goal is prediction, rather than
effect estimation. In the latter case, our new variables will be
principal components, and their regression coefficients may not be
meaningful to us. If we are doing prediction, regression coefficients
may not be of interest.)

## Overall Idea 

The goal is to find a few important linear combinations of our original
predictor variables--important in the sense that they roughly summarize
our data.  These new variables are called the *principal components*
(PCs) of the data.  Let's see how this is done.

### The first PC{#sec-firstPC}

Let **X** denote a set of variables of interest (not necessarily in a
prediction context).
In searching for good linear combinations, we want to aim for ones with
high variance. We certainly don't want ones with low variance; after
all, a random variable with 0 variance is a constant.  So we wish to
find linear combinations 

$$
Xu
$$

which maximize 

$$
Var(Xu) = u' Cov(X) u
$$

where we have invoked @eq-acova.

But that goal is ill-defined, since we could take larger and larger
vectors $u$, thus larger and larger vectors $Xu$, no maximum.  So, let's
constrain it to vectors $u$ of length 1:

$$
u'u = 1
$$

The method of *Lagrange multipliers* is used to solve maximum/minimum
problems that have constraints, by adding a new variable corresponding
to the constraint. In our case, we maximize

$$
u'Cov(X)u + \gamma (u'u - 1)
$$

with respect to $u$ and $\gamma$.

Setting derivatives to 0, we have

$$
0 = 
2 Cov(X) u + 2 \gamma u
$$

and

$$
0 = u'u - 1
$$

In other words,

$$
Cov(X) u = -\gamma u
$${#eq-aha}

Aha! The vector $u$ is an eigenvector of the matrix $Cov(X)$ with
eigenvalue $-\gamma$.  We've discovered that our quest for dimension
reduction can be couched as an eigenanalysis problem!

### The second, third etc. PCs

There are as many principal components as there are columns in columns
in $X$. So, how are the second, third and so on defined and computed?

The key issue is that we want our PCs to be orthogonal, because as we
have seen orthogonal random vectors are statistically uncorrelated, and
independent in the multivariate normal case. So if we summarize our data
with say, the first few PCs, the fact that they are (at least loosely
speaking) uncorrelated makes for a neater summary.

So with $u$ and $v$ denoting the first and second PCs, we want $v$ 
to maximize

$$
v'Cov(X)v + \gamma (v'v - 1)
$$

subject to 

$$
u'v = 0
$$

For the latter condition, let's add another Lagrange multiplier term:

$$
v'Cov(X)v + \omega (v'v - 1) + \eta u'v
$$

Setting derivatives (with respect to $v$) to 0, we have

$$
0 = 2 Cov(X) v + 2 \omega v + \eta u
$${#eq-2ndeig}

Multiplying on the left by $u'$, we have

$$
0 = 2 u'Cov(X) v + 2 \omega u'v + \eta u'u
$${#eq-ucovv}

But from before we have 

$$
u'Cov(X) = -\gamma u
$$

so that

$$
u'Cov(X) v = 0
$$

in @eq-ucovv. Moreover, $u'v = 0$ there as well, so that we have $\eta =
0$, which reduces @eq-ucovv to a statement that $v$ too must be an
eigenvector of $Cov(X)$.

## Properties

The full set of eigenvectors is then known as the *principal
components* (PCs). They have some further important properties:

<blockquote>

* The PCs are orthogonal to each other. This follows from our earlier
  finding that eigenvectors of a symmetric matrix, in this case
  $Cov(X)$, are orthogonal. We say that a symmetric is *orthogonal*
  if its columns are orthogonal as vectors.

* This in turn implies that the PCs are uncorrelated, and in the
  multivariate normal case, statistically independent.  

* The eigenvalues are the variances of the PCs. The PCs are in order of
  decreasing (or at least nonincreasing) variance. 

  This first statement follows from pre-mutiplying @eq-aha by $u'$,
  giving us $Var(u)$ on the left and $-\lambda u'u = -\lambda$ on the
  right. The second statement follows from the fact that as we go from
  the first PC to the second, third and so on, we are maximizing under
  more and more constraints, hence smaller maxima.

</blockquote>

By the way, though we used the covariance matrix here, it is also common
to simply do eigenanalysis on the original data matrix.

## Eigenanalysis Relation between $A$ and $Cov(X)$ in Linear Regression

Consider our usual linear regression setting, in which the matrix $A$
contains our data for our predictor variables $X$.  If we center and
scale our data, then '

$$
Cov(X) = A'A 
$$

Now say the PCA of X has a 0 eigenvalue. That implies that some linear
combination $u'X$ has 0 variance. Then

$$
0 = Var(u'X) = u' Cov(X) u = u' A'A u = (Au)'Au
$$

Thus 

$$
Au = 0
$$

Since $Au$ is a linear combination of the columns of $A$, we see that
the columns of $A$ are linearly dependent.

The same argument works in reverse. If $Au = 0$, we fine that $Var(u'X)
= 0$, and since the eigenvalues of $Cov(X)$ are variances of linear
combinations of $X$, we see that one of those variances is 0.

## Back to the Example

So, we remove the ``Y'' variable, $pH$, number 3598 as seen above, as
proceed. We will also remove the nonnumeric columns,
**PIDN** and **Depth**. We could use R's **prcomp** function here, but
to better illustrate the concepts, we do things from scratch.

```{r} 
x <- AfricanSoil[,-c(1,3595,3598)]
dim(x)
xcov <- cov(x)
eigs <- eigen(xcov)
eigs$values[1:100]  # don't print out all 3597!
```

Ah, the eigenvalues fall off rapidly after the first few. So, let's say
we decide to use the first 9 $u$ vectors.  

```{r}
eigvecs <- eigs$vectors[,1:9]
dim(eigvecs)
```

So, recalling the sequence of equations leading to @eq-aha, we are ready
to replace our old variables by the new:

```{r} 
x <- as.matrix(x)
dim(x)
xnew <- x %*% eigvecs
dim(xnew)  
ph <- AfricanSoil[,3595]
lmout <- lm(AfricanSoil$pH ~ xnew)
# and so on
```

## PCA in Detection of Multicollinearity

Recall the itemized list in @sec-nearly of various conditions 
under which we have multicollinearity. We can add the following item to
that list:  

* $A$ has an eigenvalue that is nearly 0.

The reasons are straightforward. If $Ax = 0$ for some nonzero $x, then
the material in @sec-partitioned on partitioning says that the
coefficients in $x$ form a linear combination of the columns of $A$ that
results in the 0 vector, i.e. $A$ is not of full rank.


In fact, PCA can warn us of specific linear combinations of $Cov(A'A)$
that are nearly 0, as follows.

* Say a PC corresponds to a small eigenvalue.

* Then from @sec-firstPC that linear combination has small variance.

* Random variables with small variance are nearly constant.

* Thus with high probability, the linear combination is near to $c$
for some number $c$.

* If our design matrix $A$ includes a 1s column, then the above linear
  combination, together with this 1s column, gives us a linear
  combination that is usually close to the 0 vector, thus
  multicollinear.

In other words, PCA can point out to us specific linear combinations of
our variables that may be problematic. If we are considering solving the
problem by removing one or more predictor variables, this analysis could
suggest which ones to remove.


## How Many Principal Components Should We Use?

We chose to use the first 9 principal components in our example here,
but just for illustration purposes.  There are no formal rules for how
many to use.  Many "rules of thumb" exist. 

If we are doing prediction, there is a very natural way to choose our
number of PCs--do cross-validation (@sec-lassoxval), and use whichever
number gives the most accurate prediction.

### Example: New York City taxi trips{#sec-nyc}

This is a well-known dataset, a version of which is included in the
**qeML** package. The object is to predict trip time, given pickup and
dropoff locations, trip distance and day of the week.

In the raw form of the data, there are just 5 columns, thus 4
predictors.  But the pickup and dropoff locations are R factors, coding
numerous locations. These must be decoded to dummy variables (values 1
or 0, coding whether or not, say, a given trip began at pickup location
121. If for instance one calls the R linear regression function **lm**,
that function will do the conversion internally, but in our case we will
perform the conversion ourselves, using **regtools::factorsToDummies**.
(The **regtools** package is included by **qeML**.)

```{r}
library(qeML)
data(nyctaxi)
dim(nyctaxi)
nyc <- factorsToDummies(nyctaxi,dfOut=T)
dim(nyc)
```

Wow! That's quite a lot of predictors, and well in excess of the common
rule of thumb that one should have no more than $\sqrt{n}$ predictors,
100 in this case.

So, we might try dimension reduction via PCA, then use the PCs as
predictors. The function **qeML::qePCA** combines these two operations.
Let's see how it works.

```{r}
args(qePCA)
```

Here **qeName** indicates which function is desired for prediction, e.g.
**qeLin** for a linear model. (This function wraps **lm**.) But a key
argument here is **pcaProp**. Recall that:

* The PCs come in order of decreasing variance.

* We are mainly interested in the first few PCs. The later ones have
small variance, which makes them approximately constant and thus of no
use to us.

The name 'pcaProp' stands for "proportion of total variance." If we set
this to, say, 0.25, we are saying "Give us whatever number of the first
few PCs that have a total variance of at least 25% of the total." Let's
give that a try:

``` r 
qePCA(nyc,'tripTime','qeLin',pcaProp=0.25)$testAcc
[1] 311.9159
```
We asked to predict the column 'tripTime' in the dataset **nyc** using
the **qeLin** function, based on as many of the PCs that will give us
25% of the total variance.  As seen above, most of the **qeML**
prediction functions split the data into a training set and a holdout
set.  The model is fit to the training set, and then applied to
prediction of the holdout set. The output value is the mean absolute
prediction error (MAPE).

However, since the holdout set is randomly generated, the MAPE value is
random, so we should do multiple runs. Some experimentation showed that
MAPE here is highly variable, so we decided to perform 500 runs, e.g.

``` r  
mean(sapply(1:500,function(i) qePCA(nyc,'tripTime','qeLin',pcaProp=0.1)$testAcc)) 
[1] 359.663
```

| pcaProb | MAPE | 
|---------|:-----|
| 0.1      | 359.6630  | 
| 0.2      | 359.3068  | 
| 0.3      | 403.9878  | 
| 0.4      | 397.3783  |
| 0.5      | 430.8958  | 
| 0.6      | 423.7299  |
| 0.7      | 517.7917  | 
| 0.8      | 969.1304  |
| 0.9      | 2275.091  | 

: Mean Absolute Predictive Error 

Among other things, this shows the dangers of overfitting, in this case
using too many PCs in our linear regression model. It seems best here to
use only the first 10 or 20% of the PCs.

## Your Turn

❄️  **Your Turn:** Modify the code for **qePCA** for the case of linear
regression by addig a component **xCoeffs** to its return value. This
will give the regression coefficients in terms of the original X
predictors.

❄️  **Your Turn:** Say the symmetric matrix $A$ has *block diagonal* form

$$
 A = 
 \left (
 \begin{array}{rrr}
 A_1 & 0 & 0 ... \\
 0 & A_2 & 0 ...  \\
 ...
 \end{array}
 \right )
$$

where $A_i$ ($i=1,...,r)$ 

* is symmetric 

* is of size $k_i \textrm{ x } k_i$

* has eigenvalues $\gamma_{1},...,\gamma_{k}$

* has eigenvectors $u{1,j},...,u{k,j}$, where $j=1,...,k_i$

State the form of the eigenvalues and eigenvectors of $A$.
