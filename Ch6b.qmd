
```{r} 
#| include: false
library(dsld)
library(qeML)
```

{{< pagebreak >}}

# Principal Components 

::: {.callout-note}

## Goals of this chapter:

Today's large datasets being so common, we need a way to "cut things
down to size," i.e. *dimension reduction*. It turns out that
eigenanalysis can be quite useful for this purpose, by determining the
*principal components* of our data. This is the focus of the current
chapter.

::: 


### Example: African soils data

To get things started, let's consider the African Soils dataset.

Let's take a look around:

```{r}
load('data/AfricanSoil.Rd')
dim(AfricanSoil)
names(AfricanSoil)[1:25]
names(AfricanSoil)[3576:3600]
```

Let's try predicting **pH**, the acidity. But that leaves 3599 possible
predictors. There is an old rule of thumb that one should have 
$p < \sqrt{n}$, for $p$ predictors and $n$ data points,
to avoid overfitting, which in our setting of $$n = 1157$$
grossly violates.  We need to do dimension reduction. One way to
accomplish this is to use PCA. 

(Note, though, that this presumes our goal is prediction, rather than
effect estimation. In the latter case, our new variables will be
principal components, and their regression coefficients may not be
meaningful to us. If we are doing prediction, regression coefficients
may not be of interest.)

## Overall Idea 

The goal is to find a few important linear combinations of our original
predictor variables--important in the sense that they roughly summarize
our data.  These new variables are called the *principal components*
(PCs) of the data.  Let's see how this is done.

### The first PC

Let **X** denote a set of variables of interest (not necessarily in a
prediction context).
In searching for good linear combinations, we want to aim for ones with
high variance. We certainly don't want ones with low variance; after
all, a random variable with 0 variance is a constant.  So we wish to
find linear combinations 

$$
Xu
$$

which maximize 

$$
Var(Xu) = u' Cov(X) u
$$

where we have invoked @eq-acova.

But that goal is ill-defined, since we could take larger and larger
vectors $u$, thus larger and larger vectors $Xu$, no maximum.  So, let's
constrain it to vectors $u$ of length 1:

$$
u'u = 1
$$

The method of *Lagrange multipliers* is used to solve maximum/minimum
problems that have constraints, by adding a new variable corresponding
to the constraint. In our case, we maximize

$$
u'Cov(X)u + \gamma (u'u - 1)
$$

with respect to $u$ and $\gamma$.

Setting derivatives to 0, we have

$$
0 = 
2 Cov(X) u + 2 \gamma u
$$

and

$$
0 = u'u - 1
$$

In other words,

$$
Cov(X) u = -\gamma u
$${#eq-aha}

Aha! The vector $u$ is an eigenvector of the matrix $Cov(X)$ with
eigenvalue $-\gamma$.  We've discovered that our quest for dimension
reduction can be couched as an eigenanalysis problem!

### The second, third etc. PCs

There are as many principal components as there are columns in columns
in $X$. So, how are the second, third and so on defined and computed?

The key issue is that we want our PCs to be orthogonal, because as we
have seen orthogonal random vectors are uncorrelated, and independent in
the multivariate normal case. So if we summarize our data
with say, the first few PCs, the fact that they are (at least loosely
speaking) uncorrelated makes for a neater summary.

So with $u$ and $v$ denoting the first and second PCs, we want $v$ 
to maximize

$$
v'Cov(X)v + \gamma (v'v - 1)
$$

subject to 

$$
u'v = 0
$$

For the latter condition, let's add another Lagrange multiplier term:

$$
v'Cov(X)v + \omega (v'v - 1) + \eta u'v
$$

Setting derivatives (with respect to $v$) to 0, we have

$$
0 = 2 Cov(X) v + 2 \omega v + \eta u
$${#eq-2ndeig}

Multiplying on the left by $u'$, we have

$$
0 = 2 u'Cov(X) v + 2 \omega u'v + \eta u'u
$${#eq-ucovv}

But from before we have 

$$
u'Cov(X) = -\gamma u
$$

so that

$$
u'Cov(X) v = 0
$$

in @eq-ucovv. Moreover, $u'v = 0$ there as well, so that we have $\eta =
0$, which reduces @eq-ucovv to a statement that $v$ too must be an
eigenvector of $Cov(X)$.





### Properties

The full set of eigenvectors is then known as the *principal
components* (PCs). They have some further important properties:

<blockquote>

* The PCs are orthogonal to each other. This follows from our earlier
  finding that eigenvectors of a symmetric matrix, in this case
  $Cov(X)$, are orthogonal.

* This in turn implies that the PCs are uncorrelated, and in the
  multivariate normal case, statistically independent.  

</blockquote>

By the way, though we used the covariance matrix here, it is more common
to simply do eigenanalysis on the original data matrix.

### Back to the Example

So, we remove the ``Y'' variable, $pH$, number 3598 as seen above, as
proceed. We will also remove the nonnumeric columns,
**PIDN** and **Depth**.

```{r} 
x <- AfricanSoil[,-c(1,3595,3598)]
dim(x)
xcov <- cov(x)
eigs <- eigen(xcov)
eigs$values[1:100]  # don't print out all 3597!
```

Ah, the eigenvalues fall off rapidly after the first few. So, let's say
we decide to use the first 9 $u$ vectors.  

```{r}
eigvecs <- eigs$vectors[,1:9]
dim(eigvecs)
```

So, recalling the sequence of equations leading to @eq-aha, we are ready
to replace our old variables by the new:

```{r} 
x <- as.matrix(x)
dim(x)
xnew <- x %*% eigvecs
dim(xnew)  
ph <- AfricanSoil[,3595]
lmout <- lm(AfricanSoil$pH ~ xnew)
# and so on
```

### PCA in detection of multicollinearity

### How Many Principal Components Should We Use?

We chose to use the first 9 principal components in our example here,
but just for illustration purposes.  There are no formal rules for how
many to use.  Many "rules of thumb" are in use, but they are beyond the
scope of this book.   
