
```{r} 
#| include: false
library(dsld)
library(qeML)
```

{{< pagebreak >}}

# Principal Components {#sec-pca}

::: {.callout-note}

## Goals of this chapter:

It turns out that eigenanalysis can be quite useful for dimension
reduction, by determining the *principal components* of our data. This
is the focus of the current chapter.

::: 

So in the last chapter we had a very brief introduction to Principal
Components Analysis (PCA), finding the first component entails
finding an eigenvector and eigenvalue.  It turns out that this is true
for the second, third and all the components. In other words, PCA is
basically an eigenvectors and eigenvalues application.

## The Second, Third Etc. PCs

Say we have a dataset $Q$ in matrix form, again using the **mtcars**
data for concreteness.

```{r}
head(mtcars)
```

There are as many principal components as there are columns in $Q$. We
derived the first PC in @sec-firstPC.  So, how are the second, third and
so on components defined and computed?

The key issue is that we want our PCs to be orthogonal vectors, because
as we saw in @sec-rvomegaprojs, orthogonal random vectors are statistically
uncorrelated, and independent in the multivariate normal case. So if we
summarize our data with say, the first few PCs, the fact that they are
uncorrelated makes for a neater summary, in a sense having no duplication.


[In the sense of random-X regression (@sec-varbhat), what we write as
"Cov(X)" below should actually be $\widehat{Cov}(X)$, a sample estimate of
a population quantity. However, we omit the "hat" in the interests of
simplicity.]{.column-margin} So with $u$ and $v$ denoting the first and
second PCs, we want $v$ to maximize $v'Cov(Q)v$, so we set the
derivative of

$$
v'Cov(Q)v + \omega (v'v - 1) + \tau (u'v - 0)
$$

with respect to $v$ to 0. (Note the two Lagrange variables, $\omega$ and
$\tau$.)

$$
0 = 2 Cov(Q) v + 2 \omega v + \tau u 
$${#eq-eig2a}

Pre-multiply by $u'$:

$$
\begin{aligned}
0 &= 2 u' Cov(Q) v + 2 \omega u'v + \tau u'u \\
&= 2 u' Cov(Q) v + \tau 
\end{aligned}
$${#eq-eig2b}

Also, since $Cov(Q)$ is symmetric and $u$ is an eigenvector of $Cov(Q)$,
say with eigenvalue $\gamma$,

$$
u' Cov(Q) v = [Cov(Q) u]' v = \gamma u' v = 0
$${#eq-eig2c}

So @eq-eig2b and @eq-eig2c now tell us that $\tau = 0$, and thus @eq-eig2a 
reduces to

$$
Cov(Q) v = -\omega v 
$${#eq-eig2c}

showing that the second PC is again an eigenvector of $Cov(Q)$.
@eq-maxvar then says that $-\omega$ is the corresponding variance.

The story is the same for the remaining PCs. 

::: {#thm-thestory}

The principal components have the following properties:

* They are orthogonal to each other.

* They are eigenvectors of $Cov(Q)$.

* The eigenvalue corresponding to a PC $z$ is $Var(z'Q)$.

* The eigenvalues form a nondecreasing sequence.

* Say $Q$ has $m$ columns. Normalize the PCs $u_i$ (i.e. divide a PC by
  its length, so that we have a vector of length 1) and form the $m
  \times m$ (partitioned) matrix

  $$
  P = (u_1 | u_2 | ... | u_m)
  $$

  Then 

  $$
  P' Cov(Q) P = D
  $$ {#eq-diagonalize}

  where $D$ is a diagonal matrix whose entries 
  are $Var(u_i'Q)$, and $P'P = I$.

* PC1, i.e. $u_1$, is a linear combination of the original variables, 
  PC2, i.e. $u_2$, is a linear combination of the original variables, 
  and so on. The PCi are our new predictor variables, though we will 
  probably use only the first few.

* Let $G$ denote a matrix in the same form at the matrix $A$ in the
  linear regression model (without a 1s column). In other words, each
  row contains the data on one sampling unit, e.g. one person. To
  convert from the original predictor variables to PCs, compute

  ``` r
  GP[,1:nPC]
  ```

  where **nPC** denotes our desired number of PCs. (A topic to be
  explored in @sec-howmanypcs.)

:::

## Review of Goals Achieved

* We hope to achieve *dimension reduction*, meaning a reduction in our
  number of predictor variables, for two reasons:

  * Parsimony: All else being equal, smaller models are easier to deal
    with and interpret.

  * Avoidance of overfitting: As the model size $p$ increases relative
    to fixed data size $n$, there is a point past which the predictive
    power of a model actually begins to decline. (But see @sec-chapDD.)

* PCA achieves those goals:
 
  * We produce new predictors, the PCs, from our original ones, ranked
    according to variance. (Recall: A variable with low variance is
    nearly constant, thus rather useless.)

  * We can reduce our number of predictors by using only some of the
    PCs.

  * The new predictors don't duplicate each other, in that they are
    uncorrelated.

## The PCs as Our New Predictor Variables

So, let's see how all this works. We will start with a simple dataset,
our census data.

```{r} 
data(svcensus) 
head(svcensus) 
svc <- factorsToDummies(svcensus) 
# remove our "Y" variable 
svc <- svc[,-11]  
z <- prcomp(svc) 
names(z)
z$sdev 
``` 

[It's called a rotation because geometrically the change of variables
really does cause a rotation of the coordinate system.]{.column-margin}
The entity **z$rotation** is our matrix $P$.  Here **sdev** is the
square roots of the variances, i.e. the eigenvalues. It appears that
there are two main PCs, the rest minor.  Here we can see which of the
original variables each of these two PCs focuses on.

``` r 
> z$rotation
                        PC1           PC2          PC3           PC4
age            0.0071442921  0.9999532343  0.000726208 -2.397875e-05
educ.14       -0.0010745227  0.0010525943 -0.321503608  4.907644e-01
educ.16       -0.0004109197  0.0010580494 -0.036350998  2.631539e-02
educ.zzzOther  0.0014854423 -0.0021106436  0.357854606 -5.170798e-01
occ.100        0.0009216241  0.0009575240  0.242783424 -8.554274e-02
occ.101        0.0016559718 -0.0011507204  0.101640265 -1.731946e-01
occ.102       -0.0016720082 -0.0037463757 -0.245946083  3.378269e-01
occ.106       -0.0001386860 -0.0001002262  0.017018077  3.846487e-03
occ.140       -0.0001607628 -0.0003608480 -0.010823679 -1.106524e-02
occ.141       -0.0006061389  0.0044006464 -0.104672004 -7.187076e-02
wkswrkd       -0.9999670218  0.0071416042  0.003457670 -9.540378e-04
gender.female  0.0015140583 -0.0001762090  0.559515249  4.088654e-01
gender.male   -0.0015140583  0.0001762090 -0.559515249 -4.088654e-01
...
```

In that first PC, the largest component by far is **wkswrkd**, trailed
substantially by the next-largest, **age**. Those two variables more or
less trade places in the second PC.

This suggests that none of the other predictors is very powerful, though
this should not be viewed as meaning that the others are collectively
useless. 

In other words, if our project is to predict wage income, we might use
PCs 1 and 2 as our predictors, as opposed to using all 13 of the
original variables.

Well, how do we do that? Say we have several new people for which we
need to predict wage income. Place their predictor values in a matrix,
say $Q$, one person per row (i.e. the same format at the matrix $A$ in
@eq-linregformula, though without a 1s column). Then, applying our usual
knowledge of matrix partitioning, our matrix of new predictor data is 
simply the matrix product shown above

``` r
G %*% P[,1:nPC]
```

So to make the full switch, convert the entire dataset:

```{r}
P <- z$rotation 
svcNew <- svc %*% P[,1:2] 
head(svcNew)  # our new data!
lmOut <- lm(svcensus$wageinc ~ svcNew) 
lmOut 
```

## Back to the African Soils Example{#sec-backtoafrica}

So, we remove the ``Y'' variable, **pH**, number 3598 as seen above, and
proceed. We will also remove the nonnumeric columns,
**PIDN** and **Depth**. 

```{r} 
library(WackyData)
data(AfricanSoil)
x <- AfricanSoil[,-c(1,3595,3598)]
z <- prcomp(x) 
head(z$sdev,25)
```

Ah, the eigenvalues fall off rapidly after the first few. So we might
use, say, the first dozen PCs.

That's quite a feat! We started with over 3,000 predictors, and cut it
down to 12.

```{r}
newx <- as.matrix(x) %*% z$rotation[,1:12]
head(newx)
lmOut <- lm(AfricanSoil[,3598] ~ newx)
lmOut
```

## How Many Principal Components Should We Use?{#sec-howmanypcs}

We chose to use the first 9 principal components in our example here,
but that was just for illustration purposes.  There are no formal rules
for how many to use. Various "rules of thumb" exist. 

If we are doing prediction, there is a very natural way to choose our
number of PCs -- do cross-validation (@sec-xval), and use whichever 
number gives the most accurate prediction.

### Example: New York City taxi trips{#sec-nyc}

This is a well-known dataset, a version of which is included in the
**qeML** package. The object is to predict trip time, given pickup and
dropoff locations, trip distance and day of the week.

In the raw form of the data, there are just 5 columns, thus 4
predictors.  But the pickup and dropoff locations are R factors, coding
numerous locations. These must be decoded to dummy variables (values 1
or 0, coding whether or not, say, a given trip began at pickup location
121). If for instance one calls the R linear regression function **lm**,
that function will do the conversion internally, but in our case we will
perform the conversion ourselves, using **regtools::factorsToDummies**.
(The **regtools** package is included by **qeML**.)

```{r}
library(qeML)
data(nyctaxi)
dim(nyctaxi)
nyc <- factorsToDummies(nyctaxi,dfOut=T)
dim(nyc)
```

Wow! That's quite a lot of predictors, and well in excess of the common
rule of thumb that one should have no more than $\sqrt{n}$ predictors,
100 in this case.

So, we might try dimension reduction via PCA, then use the PCs as
predictors. The function **qeML::qePCA** combines these two operations.
Let's see how it works.

```{r}
args(qePCA)
```

Here **qeName** indicates which function is desired for prediction, e.g.
**qeLin** for a linear model. (This function wraps **lm**.) But a key
argument here is **pcaProp**. Recall that:

* The PCs come in order of decreasing variance.

* We are mainly interested in the first few PCs. The later ones have
small variance, which makes them approximately constant and thus of no
use to us.

The name 'pcaProp' stands for "proportion of total variance." If we set
this to, say, 0.25, we are saying "Give us whatever number of the first
few PCs that have a total variance of at least 25% of the total." Let's
give that a try:

``` r 
qePCA(nyc,'tripTime','qeLin',pcaProp=0.25)$testAcc
[1] 311.9159
```

We asked to predict the column 'tripTime' in the dataset **nyc** using
the **qeLin** function, based on as many of the PCs that will give us
25% of the total variance.  Most **qeML** prediction functions split the
data into a training set and a holdout set.  The model is fit to the
training set, and then applied to prediction of the holdout set. The
output value is the mean absolute prediction error (MAPE).

However, since the holdout set is randomly generated, the MAPE value is
random, so we should do multiple runs. Some experimentation showed that
MAPE here is highly variable, so we decided to perform 500 runs, e.g.

``` r  
mean(sapply(1:500,function(i) qePCA(nyc,'tripTime','qeLin',pcaProp=0.1)$testAcc)) 
[1] 359.663
```

| pcaProb | MAPE | 
|---------|:-----|
| 0.1      | 359.6630  | 
| 0.2      | 359.3068  | 
| 0.3      | 403.9878  | 
| 0.4      | 397.3783  |
| 0.5      | 430.8958  | 
| 0.6      | 423.7299  |
| 0.7      | 517.7917  | 
| 0.8      | 969.1304  |
| 0.9      | 2275.091  | 

: Mean Absolute Predictive Error 

Among other things, this shows the dangers of overfitting, in this case
using too many PCs in our linear regression model. It seems best here to
use only the first 10 or 20% of the PCs.


## A "Square Root" Matrix, and MV Normal Simulation{#sec-squareroot}

::: {#thm-sqrt}
Any covariance matrix $A$ has a "square root" matrix $Q$, i.e.
$$
Q^2 = A
$$

:::

::: {.proof}

From @thm-thestory, we have $P' A P = D$ for some matrix $P$ and
diagonal matrix $D$, with the entries of the latter being the variances
of the PCs $\sigma_i^2$.  That latter point implies that

$$
D_1^2 = D
$$

where $D_1 = diag(\sigma_1,\sigma_2,...,\sigma_n)$. Then setting
$Q = P D_1 P'$, we have

$$
Q^2 = (P D_1 P') (P D_1 P') = P D_1^2 P' = P D P' = A
$$

### Implications for simulation of multivariate normal X

Say we wish to write code to simulate a random vector $Q$ having an
m-variate normal distribution with mean vector $\mu$ and covariance
matrix $\Sigma$. Here is how it works:

* We start with generating $Z$, a vector o $m$ independent N(0,1)
  variables. That means $Z$ is m-variate normally distributed, with
  mean vector consisting of $m$ 0s, and covariance matrix $I$, the
  $m \times m$ identity matrix.

* We compute $W = \Sigma^{0.5} Z$. By @eq-covax, $W$ will again be
  multivariate normally distributed, with mean vector consisting of $m$ 0s, 
  and covariance matrix equal to

  $$
  \Sigma^{0.5} I \Sigma^{0.5} = \Sigma
  $$

  Our solution is then

  $$
  Q = \mu + \Sigma^{0.5} Z
  $$

$\square$
:::

## Eigenanalysis of the Relation between $A$ and $Cov(X)$ in Linear Regression{#sec-eiglm}

Consider our usual linear regression setting, in which the matrix $A$
contains the data for our predictor variables $X$. Say we are in the
*random-X* setting discussed in @sec-varbhat.  

Recall that the population definition is

$$
Cov(X) = E[(X-EX)(X-EX)']
$${#eq-popcov}

which if we center our data becomes

$$
Cov(X) = E[XX']
$${#eq-centercov}

the average value of $XX'$ in the population.

Let $x_i$ denote the value of the vector $X$ for our $i^{th}$ data
point, so that $x_i'$ is row $i$ of $A$. Then the sample analog of
@eq-popcov is the average value of $XX'$ in the sample,

\begin{align}
Cov(X) 
&= \frac{1}{n} \sum_{i=1}^n x_i x_i' \\
&= \frac{1}{n} (x_1 | ... | x_n) 
\left (
\begin{array}{r}
x_1' \\
... \\
x_n' \\
\end{array}
\right ) \\
&= \frac{1}{n} A'A
\end{align}

To see an example of the implications of this, say the PCA of X has a 0
eigenvalue. That implies that some linear combination $u'X$ has 0
variance. Then

\begin{align}
0 &= Var(u'X) \\
&= u' Cov(X) u \\
&= \frac{1}{n} u' A'A u \\
&= \frac{1}{n} (Au)'Au
\end{align}

Thus 

$$
Au = 0
$$

Since $Au$ is a linear combination of the columns of $A$, we see that
the columns of $A$ are linearly dependent.

The same argument works in reverse. If $Au = 0$, we find that $Var(u'X)
= 0$, and since the eigenvalues of $Cov(X)$ are variances of linear
combinations of $X$, we see that one of those variances is 0. Similar
statements can be made for small but nonzero eigenvalues.

In other words:

> The presence of small eigenvalues in $A'A$ does indeed reflect
> approximate linear dependencies among the predictors, reaffirming our
> discussion in @sec-eigenChap.

## Your Turn

❄️  **Your Turn:** Use PCA to do dimension reduction on the **s50**
dataset.

❄️  **Your Turn:** The reader has likely seen the concept of a
*cumulative distribution function* (CDF). For a scalar random variable
$X$, this is $F_x(t) = P(X \leq t)$. If $X$ is an $m$-dimensional random
vector, the definition is

$$
F_X(t_1,...,t_m) = P(X_1 \leq t_1,...,X_m \leq t_m)
$$

Write an R function with call form

``` r
multiCDF(mu,Sigma,t,n)
```

that uses simulation to evaluate the multivariate CDF, where $t =
(t_1,...,t_m)$ and $n$ is the number of replications to simulate.

❄️  **Your Turn:** Say $X$ has mean vector $\mu$ and covariance matrix
$\Sigma$. Show how we can use @thm-sqrt to find a square, constant
matrix $A$ such that $Cov(AX) = I$. If in addition $X$ has a
multivariate normal distribution, then $AX$ will then consist of
independent random variables with variance 1. Explain why.

❄️  **Your Turn:** Modify the code for **qePCA** for the case of linear
regression by adding a component **xCoeffs** to its return value. This
will give the regression coefficients in terms of the original X
predictors.

❄️  **Your Turn:** Say the symmetric matrix $A$ has *block diagonal* form

$$
 A = 
 \left (
 \begin{array}{rrr}
 A_1 & 0 & 0 ... \\
 0 & A_2 & 0 ...  \\
 ...
 \end{array}
 \right )
$$

where $A_i$ ($i=1,...,r)$ 

* is symmetric 

* is of size $k_i \times k_i$

* has eigenvalues $\gamma_{1},...,\gamma_{k_i}$

* has eigenvectors $u_{1,j},...,u_{k,j}$, where $j=1,...,k_i$

State the form of the eigenvalues and eigenvectors of $A$.

❄️  **Your Turn:** In @sec-detect,
there was a statement "$c$ is nonzero with high probability." Why was
that important?

❄️  **Your Turn:** Write an R function with call form

``` r
bestPCAPred(data,yName)
```

It will apply PCA to the X portion of **data**, then apply **lm**
successivly to the first PC, then the first two, then the first three
and so on assessing with cross-validation in each case. It will then
return the number of PCs (and the PCs themselves) that predicts best.
Try your function on various datasets.

❄️  **Your Turn:** Say the symmetric $n \times n$ matrix $A$ has
rank $r$. Show that $n-r$ of its eigenvalues are 0s.

