
```{r} 
#| include: false
library(dsld)
library(qeML)
```

{{< pagebreak >}}

# Principal Components 

::: {.callout-note}

## Goals of this chapter:

Today's large datasets being so common, we need a way to "cut things
down to size," i.e. *dimension reduction*. It turns out that
eigenanalysis can be quite useful for this purpose, by determining the
*principal components* of our data. This is the focus of the current
chapter.

::: 

```{r} 
#| include: false
library(dsld)
library(qeML)
```

{{< pagebreak >}}

# PCA and SVD

::: {.callout-note}

## Goals of this chapter:

Here we delve into one of the most famous application areas of
eigenanalysis.

:::

## Application: Dimension Reduction (PCA)

The term *dimension reduction* means thinning out ones set of predictor
variables/features. This could be for the purpose of avoiding
overfitting, or just to keep things simple. One classical method for
achieving this is Principal Components Analysis (PCA).  

### Example: African Soils Data

To get things started, let's consider the African Soils dataset,
available at various Internet locations, such as
[Kaggle](https://www.kaggle.com/c/afsis-soil-properties). Unzip the file
to obtain **train.zip**, and unzip that to get **training.csv**. Make
sure the latter file is in the same directory/folder as your Quarto
files for this document, so that the code will run.

Let's take a look around:

```{r}
load('data/AfricanSoil.Rd')
dim(AfricanSoil)
names(AfricanSoil)[1:25]
names(AfricanSoil)[3576:3600]
```

Let's try predicting **pH**, the acidity. But that leaves 3599 possible
predictors. There is an old rule of thumb that one should have 
$p < \sqrt{n}$, for $p$ predictors and $n$ data points,
to avoid overfitting, which in our setting of $$n = 1157$$
grossly violates.  We need to do dimension reduction. One way to
accomplish this is to use PCA. 

(Note, though, that this presumes our goal is prediction, rather than
effect estimation. In the latter case, our new variables will be
principle components, and their regression coefficients may not be
meaningful to us. If we are doing prediction, regression coefficients
may not be of interest.)

### Overall Idea 

The goal is to find a few important linear combinations of our original
predictor variables--important in the sense that they roughly summarize
our data.  Let's see how this is done.

Let **X** denote a set of variables of interest (not necessarily in a
prediction context).
In searching for good linear combinations, we want to aim for ones with
high variance. We certainly don't want ones with low variance; after
all, a random variable with 0 variance is a constant.  So we wish to
find linear combinations 

$$
Xu
$$

which maximize 

$$
Var(Xu) = u' Cov(X) u
$$

where we have invoked @eq-acova.

But that goal is ill-defined, since we could take larger and larger
vectors $u$, thus larger and larger vectors $Xu$, no maximum.  So, let's
constrain it to vectors $u$ of length 1:

$$
u'u = 1
$$

The method of *Lagrange multipliers* is used to solve maximum/minimum
problems that have constraints, by adding a new variable corresponding
to the constraint. In our case, we maximize

$$
u'Cov(X)u + \gamma (u'u - 1)
$$

with respect to $u$ and $\gamma$.

Setting derivatives to 0, we have
$$
0 = 
2 Cov(X) u + 2 \gamma u
$$

and

$$
0 = u'u - 1
$$

In other words,

$$
Cov(X) u = -\gamma u
$${#eq-aha}

Aha! The vector $u$ is an eigenvector of the matrix $Cov(X)$ with
eigenvalue $-\gamma$.  We've discovered that our quest for dimension
reduction can be couched as an eigenanalysis problem!

The full set of eigenvectors is then known as the *principal
components* (PCs). They have some further important properties:

<blockquote>

* The PCs are orthogonal to each other. This follows from our earlier
  finding that eigenvectors of a symmetric matrix, in this case
  $Cov(X)$, are orthogonal.

* This in turn implies that the PCs are uncorrelated, and in the
  multivariate normal case, statistically ndp.  

</blockquote>

By the way, though we used the covariance matrix here, it is more common
to simply do eigenanalysis on the original data matrix.

### Back to the Example

So, we remove the ``Y'' variable, $pH$, number 3598 as seen above, as
proceed. We will also remove the nonnumeric columns,
**PIDN** and **Depth**.

```{r} 
x <- AfricanSoil[,-c(1,3595,3598)]
dim(x)
xcov <- cov(x)
eigs <- eigen(xcov)
eigs$values[1:100]  # don't print out all 3597!
```

Ah, the eigenvalues fall off rapidly after the first few. So, let's say
we decide to use the first 9 $u$ vectors.  

```{r}
eigvecs <- eigs$vectors[,1:9]
dim(eigvecs)
```

So, recalling the sequence of equations leading to @eq-aha, we are ready
to replace our old variables by the new:

```{r} 
x <- as.matrix(x)
dim(x)
xnew <- x %*% eigvecs
dim(xnew)  
ph <- AfricanSoil[,3595]
lmout <- lm(AfricanSoil$pH ~ xnew)
# and so on
```

### How Many Principal Components Should We Use?

We chose to use the first 9 principal components in our example here,
but just for illustration purposes.  There are no formal rules for how
many to use.  Many "rules of thumb" are in use, but they are beyond the
scope of this book.   

