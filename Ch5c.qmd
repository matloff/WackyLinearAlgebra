
```{r}
#| include: false
library(dsld)
library(qeML)
```

{{< pagebreak >}}

 
# Shrinkage Estimators

::: {.callout-note}

## Goals of this chapter:

In the previous chapter, we introduced the norm of a vector. In many
data science applications, solutions with smaller norms may be more
accurate. This point is explored here.

:::

## Ridge Regression

In seminal paper, Hoerl and Kennard[*Technometrids*,
Februaru1970]{.column-margin} discussed the problem of
*multicollinearity* of predictor variables in a linear model.  

### Crux of the problem

Multicollinearity referred to settings in which the following concerns
arise:

* One column of the matrix $A$ in @eq-linregformula is nearly equal to
  some linear combination of the others.

* Thus $A$ is nearly not of full rank.

* Thus $A'A$ is nearly not of full rank.

* Thus $\widehat{\beta}$ is unstable, in the form of high variance.

Needless to say, the word "nearly" is vague, and leaves open the
question of "How near is 'near'?" and "What can we do about it?" Ridge
regression is Hoerl'ss and Kennards's answer to the latter question, and
it is based on a parameter that can be used to explore the former one.

### The ridge solution

Their solution is simple; Add some quantity to the diagonal of $A'A$.
Specifically, @eq-linregformula now becomes

$$
\widehat{\beta} - (A'A + \lambda I)^{-1} A'S
$${#eq-ridge}

where $\lambda$ is a positive number chosen by the analyst.

Here $A$ has dimensions $n \textrm{ x } p$, and $I$ is the $p \textrm{ x } p$
identity matrix.

### Matrix formulation

Using partitioned matrices helps understand ridge. Replace  $A$ and $S$
by

$$
A_{new} =
 \left (
 \begin{array}{r}
 A \\
 \lambda I \\
 \end{array}
 \right )
$$

and

$$
S_{new} =
 \left (
 \begin{array}{r}
 S \\
 0 \\
 \end{array}
 \right )
$$

where 0 means $p$ 0s. In essence, we are adding artificial data here,
consisting of $p$ new rows to $A$, and $p$ new elements to $S$.
So @eq-ridge is just the result of applying @eq-linregformula to
$A_{new}$ and $S_{new}$.

Loosely speaking, we can think of the addition of $\lambda I$ to $A'A$
makes the latter "larger", and thus its inverse smaller. In other words,
we are "shrinking" $\widehat{\beta}$ towards 0. This effect is made even
stronger by the fact that we added 0s data to $S$. This will be made
more precise below.

❄️  **Your Turn:** Show that $A_{new}$ is of full rank, $p$.

### Modern view

There are many ways to deal with multicollinearity, which we must remind
the reader only concerns *approximate* linear dependence pf the columns of
$A$. These days, ridge is often used for situations in which there is
*exact* linear dependence.

We saw such a setting in @sec-censusrref. There we deliberately induced
exact linear dependence by inclusion of both male and female dummy
variables. Let's apply ridge:

```{r}
library(qeML) 
data(svcensus) 
svc <- svcensus[,c(1,4:6)]  
svc$man <- as.numeric(svc$gender == 'male') 
svc$woman <- as.numeric(svc$gender == 'female') 
svc$gender <- NULL 
a <- svc[,-2] 
lambda <- 0.1 
a <- as.matrix(a) 
tmp1 <- solve(t(a) %*% a + lambda * diag(4)) 
tmp1 %*% t(a) %*% as.matrix(svc$wageinc) 
```

The results essentially are the same as what we obtained by having only
one dummy, thus no linear dependence: Men still enjoy about an \$11,000
advantage. But ridge allowed us to avoid deleting one of our dummies.
Such deletion is easy in this case, but for large $p$, say in the
hundreds or even more, some analysts prefer the convenience of ridge.

Of course, there is still the issue of how to choose the value of
$\lambda$. This is beyond the scope of this book, which focuses on linar
algebra.

### Formalizing the notion of shrinkage

In the early 1980s, the statistical world was shocked by research by
James and Stein that found, in short that:

<blockquote>

Say $W$ has $q$-dimensional normal distribution with mean vector $\mu$
and independent components having variance $\sigma^2$, each. We have a
random sample of size $n$, i.e. $n$ independent observations on $W$.
Then if $q \geq 3$, in terms of Mean Squared Estimation Error, the best
estimator of $\mu$ is NOT the sample mean $\bar{W}$. Instead, it's

$$
\left (
1 - \frac{(q-2) \sigma^2/n}{||\bar{W}||^2} 
\right )
\bar{W}
$$

</blockquote>

The quantity within the parentheses is typically smaller than 1, giving
us the shrinkage property. Note, though, that with larger $n$, the
amount of shrinkage is minor.

In the case of linear regression, shrinkage works there too, with $q$
being the number of columns in the $A$ matrix. 

### Shrinkage through length penalization

Say instead of minimizing @eq-matrixss, we minimize

$$
(S - Ab)'(S - Ab) + \lambda ||b||^2 
$${#eq-matrixssridge}

The larger $b$ is, the harder it is to minimize the overall quantity
@eq-matrixssridge. We say that we *penalize* large values of $b$,
an indirect way of pursuing shrinkage. Now take the derivative and set
to 0:

$$
0 = A'(S-Ab) + \lambda b
$$

i.e.

$$
(A'A + \lambda I) b = A'S
$$ 

and thus

$$
\widehat{\beta} = (A'A+\lambda I)^{-1} A'S
$$

It's ridge! So here is formalization of our "almost singular" etc.
language above to justify ridge as a shrinkage estimator..

## The LASSO

The LASSO (Least Absolute Shrinkage and Selection Operator) was
developed by Robert Tibshirani in 1996, following earlier work by Leo
Breiman.  It takes $\widehat{\beta}$ to be the value of $b$ that
minimizes

$$
(S - Ab)'(S - Ab) + \lambda ||b||_1
$${#eq-matrixsslasso}

where the "l1 norm" is 

$$
||b|| = \sum_{i=1}^p |b_i|
$$

We will write our original norm as $||b||_2$.

This is a seemingly minor change, but with important implications.  What
Breiman and Tibshirani were trying to do was to obtain a *sparse*
$\widehat{\beta}, i.e. a solution $ with lots of 0s, thereby providing a
method for predictor variable selection. This is important because
so-called "parsimonious" prediction models are desirable.

To that end, first note that it can be shown that, under some technical
conditions, that the ridge solution minimizes 

$$
(S - Ab)'(S - Ab) 
$$

subject to the constraint

$$
||b||_2 \leq \gamma
$$

while in the LASSO case the constraint is

$$
||b||_1 \leq \gamma
$$

As with $\lambda$ in the original formulation, $\gamma$ is a positive
number chosen by the analyst.

Comparison between the ridge and LASSO concepts is often done via this
[famous
picture](https://www.google.com/imgres?q=ridge%20vs%20lasso%20famous%20picture&imgurl=https%3A%2F%2Fmiro.medium.com%2Fv2%2Fresize%3Afit%3A1400%2F1*9bPyll00vcCKUQBSaps7YA.jpeg&imgrefurl=https%3A%2F%2Fmedium.com%2F%40kiprono_65591%2Fregularization-a-technique-used-to-prevent-over-fitting-886d5b361700&docid=Irc6S64NB1SwpM&tbnid=dREcVXNWMoEpkM&vet=12ahUKEwixrP7t3euKAxX0ITQIHUjqDl8QM3oECD0QAA..i&w=1400&h=749&hcb=2&ved=2ahUKEwixrP7t3euKAxX0ITQIHUjqDl8QM3oECD0QAA).
Here is what is involved:

Here $p = 2$, with $b = (b_0,b_1)'$. The horizontal and vertical axes
represent $b_0$ and $b_1$.  The constraints $||b||_2 \leq \gamma$ and
$||b||_1 \leq \gamma$ then take the form of a circle and a diamond,
respectively.

Meanwhile, the concentric ellipses depict the values of $(S - Ab)'(S - Ab)$.
On the one hand, we want that quantity to be small, thus a smaller
ellipse, but we need the ellipse to go through the circle or diamond.
The solution is then the one in which the ellipse just barely touches
the circle or diamond, respectively.

The key point is that in the LASSO case, that "barely touching" point 
will be one of the four corners of the diamond, points at which either
$b_0 = 0$ or $b_1 = 0$ -- hence a sparse solution.  This will not happen
with the circle; the touch point will almost certainly have both $b_0$
and $b_1$ as nonzero.

## A Warning

Many statistical quantities now have *regularized*, i.e. shrunken
versions. It is also standard practice in neural networks. This may be
quite helpful in prediction contexts. However, note the following:

::: {.callout-important}

## No Statistical Inference on Shrinkage Estimators

Shrinkage produces a bias, of unknown size. Thus classical statistical
inference (confidence intervals, hypothesis tests), e.g. those based on
@eq-varcovar for linear models, is not possible.

:::



