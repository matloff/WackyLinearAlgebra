
```{r}
#| include: false
library(dsld)
library(qeML)
```

{{< pagebreak >}}

 
# Shrinkage Estimators

::: {.callout-note}

## Goals of this chapter:

In the previous chapter, we introduced the norm of a vector. In many
data science applications, solutions with smaller norms may be more
accurate. This point is explored here.

:::
## Multicollinearity{#sec-nearly}

The term *multicollinearity* refers to settings in which the following concerns
arise:

* One column of the matrix $A$ in @eq-linregformula is nearly equal to
  some linear combination of the others.

* Thus $A$ is nearly not of full rank.

* Thus $A'A$ is nearly not of full rank.

* Thus $\widehat{\beta}$ is unstable, in the form of high variance.

(Technically, the above conditions refer to *approximate*
multicollinearity, with the exact version consisting of the first three
bullet points, minus the word "nearly." However, informally, the term
*multicollinearity* is usually taken to mean the approximate condition.)

That latter point is often quantified by the *Variance Inflation
Factor*. To motivate it, consider the "R-squared" value
from linear regression analyis, which is the squared correlation between
"Y" and predicted "Y". Let $R^2_j$ denote that measure in the case of
predicting column $j$ of $A$ from the other columns. The quantity

$$
VIF_j = \frac{1}{1-R^2_j}
$$

then measures the negative impact due to multicollinearity on estimating 
$\widehat{\beta}_j$. The intuition is that, say, column 3 of $A$ can be
predicte well using a linear model, then that column is approximately
equal to a linear combination of the other columns. This is worrisome in
light of the problems described above.

Needless to say, the word "nearly" above, e.g. in "nearly not of full
rank," is vague, and leaves open the question of "What can we do about
it?" We will present several answers to these questions in this and the
succeeding chapters.

### Example: Million Song dataset{#sec-millionsong}

Let's consider the Million Song Dataset, varous versions of which are on
the Web. Ours is the one with 515345 rows and 91 columns. The first
column is the year of release, followed by 90 columns of various audio
measurements. The goal is to predict the year, **V1**, from the audio variables
**V2** through **V91**.

The function **regclass::VIF** will compute the VIF values for us.

``` r
> lmout <- lm(V1 ~ .,data=s50)
> VIF(lmout)
     V2       V3       V4       V5       V6       V7       V8       V9
3.215081 2.596474 4.236853 7.414375 1.534492 5.844729 2.794018 3.192805
     V10      V11      V12      V13      V14      V15      V16      V17
2.072851 3.673590 4.705886 1.708520 2.446279 2.827296 3.672250 7.409782
     V18      V19      V20      V21      V22      V23      V24      V25
2.634033 9.472261 4.217311 7.147952 5.122114 7.984860 9.675134 3.591586
     V26      V27      V28      V29      V30      V31      V32      V33
1.818596 1.758043 3.879968 1.663670 2.108174 2.321385 2.056017 1.854913
     V34      V35      V36      V37      V38      V39      V40      V41
3.011534 2.040587 2.760111 2.879667 1.918229 2.176048 2.074103 1.946859
     V42      V43      V44      V45      V46      V47      V48      V49
1.704259 2.138794 1.690651 1.556782 1.817380 3.000759 1.547592 2.140282
     V50      V51      V52      V53      V54      V55      V56      V57
2.496121 1.612253 2.042571 2.208492 1.723669 2.024290 2.016403 2.033654
     V58      V59      V60      V61      V62      V63      V64      V65
2.004260 2.998469 2.074152 3.410124 2.153116 1.378160 3.270617 1.502543
     V66      V67      V68      V69      V70      V71      V72      V73
2.581171 1.725809 2.167673 2.379354 2.062862 1.703360 2.036596 1.984427
     V74      V75      V76      V77      V78      V79      V80      V81
2.557163 1.465020 1.515436 2.260728 1.840509 2.078497 3.604771 1.595064
     V82      V83      V84      V85      V86      V87      V88      V89
2.528307 2.005876 2.283956 1.448379 1.895053 1.601004 1.581099 2.252362
     V90      V91
1.332590 1.570891
```

As a rough guide, values of VIF about 5.0 are considered concerning by
many analysts. Under that criterion, variables **V5**, **V7**, **V17**
and so on look troublesome.

What can be done? One simple approach would be to delete those columns
from the dataset. This is indeed is a common solution, but another is
*ridge regression*, which we present next.


## Ridge Regression

In seminal paper, Hoerl and Kennard[*Technometrids*,
Februaru1970]{.column-margin} discussed the problem of
*multicollinearity* of predictor variables in a linear model.  


### The ridge solution

Their solution is simple; Add some quantity to the diagonal of $A'A$.
Specifically, @eq-linregformula now becomes

$$
\widehat{\beta} - (A'A + \lambda I)^{-1} A'S
$${#eq-ridge}

where $\lambda$ is a positive number chosen by the analyst.

Here $A$ has dimensions $n \textrm{ x } p$, and $I$ is the $p \textrm{ x } p$
identity matrix.

### Matrix formulation{#sec-matform}

Using partitioned matrices helps understand ridge. Replace  $A$ and $S$
by

$$
A_{new} =
 \left (
 \begin{array}{r}
 A \\
 \lambda I \\
 \end{array}
 \right )
$$

and

$$
S_{new} =
 \left (
 \begin{array}{r}
 S \\
 0 \\
 \end{array}
 \right )
$$

where 0 means $p$ 0s. In essence, we are adding artificial data here,
consisting of $p$ new rows to $A$, and $p$ new elements to $S$.
So @eq-ridge is just the result of applying @eq-linregformula to
$A_{new}$ and $S_{new}$.

Loosely speaking, we can think of the addition of $\lambda I$ to $A'A$
makes the latter "larger", and thus its inverse smaller. In other words,
we are "shrinking" $\widehat{\beta}$ towards 0. This effect is made even
stronger by the fact that we added 0s data to $S$. This will be made
more precise below.

### Modern view{#sec-pgtn}

There are many ways to deal with multicollinearity, which we must remind
the reader only concerns *approximate* linear dependence pf the columns
of $A$. These days, many analysts go further, using ridge for situations
in which there is *exact* linear dependence.

We saw such a setting in @sec-censusrref. There we deliberately induced
exact linear dependence by inclusion of both male and female dummy
variables. Let's apply ridge:

```{r}
library(qeML) 
data(svcensus) 
svc <- svcensus[,c(1,4:6)]  
svc$man <- as.numeric(svc$gender == 'male') 
svc$woman <- as.numeric(svc$gender == 'female') 
svc$gender <- NULL 
a <- svc[,-2] 
lambda <- 0.1 
a <- as.matrix(a) 
tmp1 <- solve(t(a) %*% a + lambda * diag(4)) 
tmp1 %*% t(a) %*% as.matrix(svc$wageinc) 
```

The results essentially are the same as what we obtained by having only
one dummy, thus no linear dependence: Men still enjoy about an \$11,000
advantage. But ridge allowed us to avoid deleting one of our dummies.
Such deletion is easy in this case, but for large $p$, say in the
hundreds or even more, some analysts prefer the convenience of ridge.

Of course, there is still the issue of how to choose the value of
$\lambda$. One common approach is *cross-validation*, to be presented in
@sec-lassoxval.

### Formalizing the notion of shrinkage

In the early 1980s, the statistical world was shocked by research by
James and Stein that found, in short that:

<blockquote>

Say $W$ has $q$-dimensional normal distribution with mean vector $\mu$
and independent components having variance $\sigma^2$, each. We have a
random sample of size $n$, i.e. $n$ independent observations on $W$.
Then if $q \geq 3$, in terms of Mean Squared Estimation Error, the best
estimator of $\mu$ is NOT the sample mean $\bar{W}$. Instead, it's

$$
\left (
1 - \frac{(q-2) \sigma^2/n}{||\bar{W}||^2} 
\right )
\bar{W}
$$

</blockquote>

The quantity within the parentheses is typically smaller than 1, giving
us the shrinkage property. Note, though, that with larger $n$, the
amount of shrinkage is minor.

In the case of linear regression, shrinkage works there too, with $q$
being the number of columns in the $A$ matrix. 

### Shrinkage through length penalization

Say instead of minimizing @eq-matrixss, we minimize

$$
(S - Ab)'(S - Ab) + \lambda ||b||^2 
$${#eq-matrixssridge}

The larger $b$ is, the harder it is to minimize the overall quantity
@eq-matrixssridge. We say that we *penalize* large values of $b$,
an indirect way of pursuing shrinkage. Now take the derivative and set
to 0:

$$
0 = A'(S-Ab) + \lambda b
$$

i.e.

$$
(A'A + \lambda I) b = A'S
$$ 

and thus

$$
\widehat{\beta} = (A'A+\lambda I)^{-1} A'S
$$

It's ridge! So here is formalization of our "almost singular" etc.
language above to justify ridge as a shrinkage estimator..

### Shrinkage through length limitation

Instead of penalizing $||b||$, we could simply constrain it, i.e. we
could set our optimization problem to:

<blockquote>

minimize $(S-Ab)'(S-Ab)$, subject to the constraint $||b||^2 \leq \gamma$

</blockquote>

We say that this new formulation is the *dual* of the first one. One can
show that they are typically equivalent.

## The LASSO

The LASSO (Least Absolute Shrinkage and Selection Operator) was
developed by Robert Tibshirani in 1996, following earlier work by Leo
Breiman.  It takes $\widehat{\beta}$ to be the value of $b$ that
minimizes

$$
(S - Ab)'(S - Ab) + \lambda ||b||_1
$${#eq-matrixsslasso}

where the "l1 norm" is 

$$
||b|| = \sum_{i=1}^p |b_i|
$$

We will write our original norm as $||b||_2$.

This is a seemingly minor change, but with important implications.  What
Breiman and Tibshirani were trying to do was to obtain a *sparse*
$\widehat{\beta}$ , i.e. a solution with lots of 0s, thereby providing a
method for predictor variable selection. This is important because
so-called "parsimonious" prediction models are desirable.

### Properties

To that end, first note that it can be shown that, under some technical
conditions, that the ridge solution minimizes 

$$
(S - Ab)'(S - Ab) 
$$

subject to the constraint

$$
||b||_2 \leq \gamma
$$

while in the LASSO case the constraint is

$$
||b||_1 \leq \gamma
$$

As with $\lambda$ in the original formulation, $\gamma$ is a positive
number chosen by the analyst.

### Geometric view

Comparison between the ridge and LASSO concepts is often done via this
graph depicting the LASSO setting:

![LASSO sparsity](LASSO_gray.png)

Here $p = 2$, with $b = (b_1,b_2)'$. The horizontal and vertical axes
represent $b_1$ and $b_2$.  

* The constraint $||b||_1 \leq \gamma$ then takes the form of a diamond,
  with corners at 
  $(\gamma,0)$, 
  $(0,\gamma)$, 
  $(-\gamma,0)$ and
  $(0,-\gamma)$. The constraint $||b||_1 \leq \gamma$
  requires us to choose a point $b$ somewhere in the
  diamond, including the boundary.

* The concentric ellipses depict the values of $c(b) = (S - Ab)'(S -
  Ab)$, as follows.  

* Consider one particular value of $c(b)$, say 1.68. 

* Many different points $b$ in the graph will have $c(b) = 1.68$; in
  fact, the locus of all such points is an ellipse.  

* There is one ellipse for each possible value of $c(b)$. So, there are
  infinitely many ellipses, though only two are shown here.

* Larger values of $c(b)$ yield larger ellipses.

* On the one hand, we want to choose a $b$ for which $c(b)$ -- our total
  squared prediction error -- is small, thus a smaller ellipse. 


* But on the other hand, we need at least one point on the ellipse to 
  be in common with the diamond.  

* The solution is then a point $b$ in which the ellipse
  just barely touches the diamond, respectively.

The key point is that that "barely touching" point 
will be one of the four corners of the diamond, points at which either
$b_1 = 0$ or $b_2 = 0$ -- *hence a sparse solution*.  

This will not happen with ridge. The diamond would now be a circle (not
shown). The "barely touching point" will almost certainly will be at a
place in which both $b_1$ and $b_2$ are nonzero. Hence no sparsity.

### Use of LASSO for dimension reduction{#sec-lassoxval} 

In prediction applications, one goal is to avoid *overfitting*, meaning
using using such a complex model that it actually has poorer predictive
power than a simpler one. An example is using too many predictors.

The LASSO, in producing a sparse estimated coefficient vector, reduces
the number of predictors. This would seem to achieve our goal, but a
problem is that we must still choose $\lambda$. 

A common way to choose among models is *cross-validation*: We set aside
a subset of the data for use in testing predictive accuracy.  The
remaining data is the *training set*. For each of our competing models,
we fit the model on the training set, then use the result to predict the
test set. We then use whichever model does best in the test set.

*Example: Census data*

Let's look once again at our census dataset (@sec-censusrref). With $n =
20090$ and $p=14$, we probably would not have to worry about overfitting
even if we used the full dataset, it is convenient to use as an example
here.

CRAN's **glmnet** does automatic cross-validation, but let's use the
**lars** package here for simplicity, just to see the effects of using
different values of $\lambda$.

```{r}
library(lars)
# lars pkg needs numeric inputs
svcdumm <- factorsToDummies(svcensus)
colnames(svcdumm)
z <- lars(svcdumm[,-11],svcdumm[,11],'lasso')
z$lambda
z$beta
```

Again, we are not doing cross-validation here, but it appears to
Occupation 141 is an expendable predictor, and we might consider
dropping, say, the **educ.14** variable.

## A Warning

Many statistical quantities now have *regularized*, i.e. shrunken
versions. It is also standard practice in neural networks. This may be
quite helpful in prediction contexts. However, note the following:

::: {.callout-important}

## No Statistical Inference on Shrinkage Estimators

Shrinkage produces a bias, of unknown size. Thus classical statistical
inference (confidence intervals, hypothesis tests), e.g. those based on
@eq-varcovar for linear models, is not possible.

:::

## Your Turn

❄️  **Your Turn:** Show that $A_{new}$ in @sec-matform is of full rank, $p$.



