
```{r}
#| include: false
library(dsld)
library(qeML)
```

{{< pagebreak >}}


# Vector Spaces 

$$
x^2  
$$

::: {.callout-note}

## Goals of this chapter:

As noted earlier, the two main structures in linear algebra are matrices
and vector spaces. We now introduce the latter, a bit more abstract than
matrices but even more powerful. We lay the crucial foundation in this
chapter, then reap the benefits in the applications in the remaining
chapters.

:::

## Review of Matrix Rank Properties

In the last chapter, we presented the concepts of matrix row and column
rank, defined to be the maximal number of linearly independent
combinations of the rows or columns of the matrix, respectively. We
proved that

<blockquote>

For any matrix $B$, 

$$
\textrm{rowrank}(B)
= \textrm{colrank}(B)
= \textrm{rowrank}(B_{rref})
= \textrm{colrank}(B_{rref})
$$

</blockquote>

We can say something stronger: 

<blockquote>

*Theorem:* Let $\cal V$ denote the *span* of the rows of $B$,i.e.\ the
set of all possible linear combinations of rows of $B$. Define $\cal
V_{rref}$ similarly. Then

$$
\cal V = \cal V_{rref}
$$

The analogous result holds for columns.

*Proof:* Actually, we already proved this in the proof regarding rank in
@sec-yessamerank, in which we wrote, 
``any nonzero linear combination of rows in $B_{rref}$ will correspond
to a nonzero linear combination of the rows of $B$,'' and vice versa.
This showed a one-to-one correspondence between the two sets of linear
combinations.

</blockquote>

So, not only do the two matrices have the same maximal numbers of 
linearly independent rows, they also generate *the same linear
combinations* of those rows.

The sets $\cal V$ and $\cal V_{rref}$ are called the *row spaces*
of the two matrices, and yes, they are examples of vector spaces, as we
will now see.

## Vector Space Definition

<blockquote>

A set of objects $\cal W$ is called a *vector space* if it satisfies the
following conditions:

* Some form of addition between vectors $u$ and $v$, denoted $u+v$, is
  defined in $\cal W$, with the result that *u+v* is also in $\cal W$.
  We describe that latter property by saying $\cal W$ is *closed* under
  addition. 

* There is a unique element called ``0'' such that  $u +
  0 = 0 + u = u$.

* Some form of scalar multiplication is defined, so that for any number
  $c$ and and $u$ in $\cal W$, $cw$ exists and is in $\cal W$. 
  We describe that latter property by saying $\cal W$ is *closed* 
  under scalar multiplication

* This being a practical book with just a dash of theory, we'll skip the
  remaining conditions, involving algebraic properties such as
  commutativity of addition ($u+v = v+u$).

</blockquote>

### Examples

### $\cal R^{n}$

In the vast majority of examples in this book, our vector space will be
$\cal R^{n}$.

Here $\cal R$ represents the set of all real numbers, and $\cal R^{n}$ is
simply the set of all vectors consisiting of $n$ real numbers. In an $m
\textrm{x} k$ matrix the rows are members of so $\cal R^{m}$ and the
columns are in $\cal R^{k}$.

### The set C(0,1) of all continuous functions on the interval [0,1]

No surprises here.  Vector addition and scalar multiplication are done
as functions. If say $u$ is the squaring function and $v$$ is the sine
function, then

$$
3u = 3x^{0.5}
$$

and 

$$
u+v = x^{0.5} + \sin(x)
$$

### The set $\cal RV(\Omega)$ of all random variables defined on some probability space $\Omega$

Consider the example in {@sec-mlb} on major league baseball players.  We
choose a player at random.  Denote weight, height and age by $W$, $H$
and $A$. 

Vector addition and scalar multiplication are defined in a
straightforward manner. For instance, the sum of $H$ and $A$
is simply height + age. This may seem like a rather nonsensical sum, but
it fits the technical definition, and moreover, we have already been
doing things like this! This after all is what is happening in our
prediction expression from that section,

$$
\textrm{predicted weight} =
-187.6382 + 4.9236 H+ 0.9115 A
$$

In fact, in this vector space, the above is a linear combination of the
random variables $1$, $H$ and $A$. Note that random variables such as
$HW^{1.2}$ and so on are also members of this vector space, essentially
any function of $W$, $H$ and $A$.

## Subspaces

Say $\cal W_{1}$ a subset of a vector space $\cal W$, such that $\cal
W_{1}$ is closed under addition and scalar multiplication. $\cal W_{1}$
is called a *subspace* of $\cal W$.  Note that a subspace is also a
vector space in its own right.

### Examples

$R^3$: 

For instance, take $\cal W_{1}$ to be all vectors of the form
(a,b,0). Clearly, $\cal W_{1}$ is closed under addition and scalar
multiplication.

Another subspace of $R^3$ is the set of vectors (a,a,b), i.e. those
vectors whose first two element are equal. What about vectors of the
form (a,b,a+b)? Yes.

$C(0,1)$: 

One subspace is the set of all polynomial functions. Again,
the sum of two polynomials is a polynomial, and the same holds for
scalar multiplication, so the set of polynomials is closed under those
operations, and is a subspace.

$\cal RV(\Omega)$: 

The set of all random variables that
are functions of $H$, say, is a subspace.

## Basis 

Consider a set of vectors $u_1,...u_r$ in a vector space $\cal W$.
Recall that the span of these vectors is defined to be the set of all
linear combinations of them. In verb form, we say that $u_1,...u_r$
*spans* $\cal W$ if we can generate the entire vector space from those
vectors via linear combinations. It's even nicer if the vectors are
linearly independent:

<blockquote>

We say the vectors $u_1,...u_r$ in a vector space $\cal W$ form a *basis*
for $\cal W$ if they are linearly independent and span $\cal W$.
</blockquote>

❄️  **Your Turn:** Prove that the coefficients in basis representations
are unique. In other words, in a representation of the vector $x$ in
terms of a basis $u_1,...,u_n$,

### Examples

$\cal R^3$: 

The vectors (1,0,0), (0,1,0) and (0,0,1) are easily seen to be a basis
here.[Our convention has been that vectors are considered in matrix
terms as column vectors by default. However, in nonmatrix contexts, it
will be convenient to write $\cal R^n$ vectors as rows.]{.column-margin}
They are linearly independent, and clearly span $\cal R^3$. For
instance, to generate (3,1,-0.2), we use this linear combination:

(3,1,-0.2) = 3 (1,0,0) + 1 (0,1,0) + (-0.2) (0,0,,1)

But bases are not unique; for instance, the set (1,0,0, (0,1,0), (0,1,1) works
equally well as a basis for this space, as are (infinitely) many others..

A basis for the subspace of vectors of the form (a,a,b) is (1,1,0) and
(0,0,1). 

$C(0,1)$: 

Alas, there is no finite basis here. Even infinite ones have
issues in their mathematical formulation.

$\cal RV(\Omega)$:

The situation here is the same as for C(0,1).

$$
x = a_1 + u_1 + ... + a_n u_n
$$

the coefficients $a_i$ are unique.

## Dimension

Geometrically, we often refer to what is called $\cal R^3$ here as
``3-dimensional.'' We extend this to general vector spaces as follows:

<blockquote>

The \textit{dimension} of a vector space is the number of vectors in any
of its bases.

</blockquote>

There is a bit of a landmine in that definition, as it presumes that all
the bases do consist of the same number of vectors. This is true, but
must be proven. Here we follow the 
[elegant proof](https://www.google.com/books/edition/Algebra_and_Geometry/tVTd78vYzPkC?hl=en&gbpv=1&pg=PA106&printsec=frontcover) AF Beardon (*Algebra and Geometry*, 2005, Cambridge).

<blockquote>

*Theorem:* The number of vectors is the same in every basis.

*Proof:* Consider two bases, $b_u = u_1,...,u_m$ and $b_v = v_1,...,v_n$
By definition, each $u_i$ in $b_u$ can be represented by $b_v$ and vice
versa:

$$
u_i = r_{i1} v_1 + ... + r_{in} v_n = \sum_{q=1}^n r_{iq} v_q
$${#eq-ui}

$$
v_j = s_{j1} u_1 + ... + s_{jm} u_m \sum_{w=1}^m s_{jw} u_w
$${#eq-vj}

Substituting the second equation into the first, we have

$$
u_k = \sum_{q=1}^n r_{kq} \sum_{w=1}^m s_{qw} u_w =
\sum_{q=1}^n \sum_{w=1}^m r_{kq}s_{qw} u_w
$$

Since the $u_w$ form a basis, then the above Your Turn problem on the
uniqueness of coefficients in a basis representation says we can equate
coefficients on both sides of the last equation. The coefficient of
$u_k$ on the right side is 

$$
\sum_{q=1}^n r_{kq} s_{qk}
$$

whereas on the left side, it is 1. So, 

$$
1 = \sum_{q=1}^n r_{kq} s_{qk}
$$

Summing over $k$, we obtain

$$
n = \sum_{k=1}^m \sum_{q=1}^n r_{kq} s_{qk}
$$

But going through the same steps as above, but now substituting @eq-ui
into @eq-vj, we would have

$$
m = \sum_{p=1}^m \sum_{q=1}^m 
r_{pq} 
s_{qp}
$$

The sums in the last two equations are the same!

Therefore $m = n$, i.e. the two bases are of the same size.

</blockquote>
 
## Inner Product Spaces

The usefulness of vector spaces is greatly enhanced with the addition of
an *inner product* structure.

### Geometric aspirations

You may recall from your high school geometry course the key concept of
perpendicularity, represented by the ⊥ symbol. You may also recall that
in 2-dimensional space,, given a point P and a line L, the line drawn
from point P to the closest point P' within L is perpendicular to L.
The same is true if L is a plane. The point P' is called the
*projection* of P onto L.

This was shown in this book's cover, shown here:

![Projections](Projections.png)

The early developers of linear algebra wanted to extend such concepts to
abstract vector spaces. This aids intuition, and has very powerful
applications.

### Definition

You may have seen *dot products in a course on vector calculus or
physics. For instance, the dot product of the vectors (3,1,1.5)' and
(0,5,6)' is

3x0 + 1x5 + 1.5x6 = 14

This in fact is a standard inner product on $\cal R^3$, but the general
definition is as follows.

<blockquote>

An *inner product* on a vector space $\cal V$, denoted by the ``angle
brackets'' notation $<u,v>$ is a function with two vectors as arguments
and a numerical output, with the following properties:

* $<u,v> = <v,u>$

* The function is bilinear: 

  $$
  <u,av+bw> = a <u,v> + b <u,w>
  $$

* $<u,u> ~ \geq 0$, with equality if and only if $u = 0$.

</blockquote>

### Examples

$\cal R^n$:

As noted, ordinary dot product is the most common inner product on this
space.

$<(a_1,...,a_n),(b_1,...,b_n> =
a_1 b_1 + ... + a_n b_n$

❄️  **Your Turn:** An $n ~ \textrm{x} ~ n$ symmetric matrix $M$ is called
*positive definite* if 

$$
w'Mw > 0
$$

Show that $<u,v> = u'Mv$ is an inner product on $\cal R^n$.

*C(0,1)*:

One inner product on this space as

$$
<f,g> = \int_{0}^{1} f(t) g(t) ~ dt
$$  

For instance, with $f(t) = t^2$ and $g(t) = \sin(t)$, the inner product
can be computed with R:

```{r}
f <- function(t) t^2
g <- function(t) sin(t)
fg <- function(t) f(t) * g(t)
integrate(fg,0,1)
```

This clearly fits most requirements for inner products, but what about
$<f,f> = 0$ only if $f = 0$?[Note that the 0 vector in this space is the
function that is identically 0, not just 0 at some points]{.column-margin}
A non-0 $f$ will have $f^2(t) > 0$ for at least one $t$, and by
continuity, $f^2(t) > 0$ on an interval containing that $t$, thus making
a nonzero contribution to the integral and thus to the inner product.


$\cal RV(\Omega)$:

From here on, we'll restrict to the vector space of all random variables
on $\Omega$ having finite variance. We define

$$
<X,Y> = E(XY)
$$

The properties of expected value, e.g. linearity, show most of the
requirements for an inner product hold.  But again, we need to show that

$$
<X,X> = E(X^2) > 0 
$$

for any $X$ such that $P(X = 0) < 1$. To see this,
recall that for any random variable $X$ with finite variance, we have

$$
Var(X) = E[(X - E(X))^2] = E(X^2) - [E(X)]^2
$$

so that $E(X^2) > 0$ for any nonzero $X$, i.e. any $X$ that is not
identically 0.

### Norm of a vector

This concept extends the notion of a the length of a vector, as we know
it in $\cal R^2$ and $\cal R^3$.

<blockquote>

The *norm* of a vector $x$ is

$$
(<x,x>)^{0.5}
$$

</blockquote>

## Projections

As mentioned, the extension of classical geometry to abstract vector
spaces has powerful applications. There is no better example of this
than the idea of *projections*.

### Definition

<blockquote>

Let $v$ be a vector in an inner product space $\cal V$, with a subspace $\cal
W$.

</blockquote>

## Shrinkage Estimators

