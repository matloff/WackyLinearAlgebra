

```{r}
#| include: false
library(dsld)
library(qeML)
```

{{< pagebreak >}}

# Confidence Sets {#sec-confsets}

::: {.callout-note}

## Goals of this chapter:  

The reader is probably already familiar with the concept of a *confidence
interval* (CI). But that is just in one dimension. Confidence intervals are
even more useful in the multidimensional setting. In this chapter, we
introduce a fairly general method of forming a CI from multivariate
data, and then extend the notion to *confidence sets* for unknown
multivariate quantities themselves. As the reader may have guessed, the
word "multivariate" here is a sign that linear algebra is involved,
which will indeed be the case.

:::

## Review: Confidence Intervals, Standard Errors

To set the stage, let's review the statistical concepts of 
*confidence interval* and *standard error*. Say we have
an estimator $\widehat{\theta}$ of some population parameter $\theta$,
e.g.\ $\bar{X}$ for a population mean $\mu$.

Loosely speaking, the term *standard error* of is our estimate of
$\sqrt{Var(\widehat{\theta})}$.  More precisely, suppose that
$\widehat{\theta}$ is asymptotically normal.  The standard error is an
estimate of the standard deviation of that normal distribution. For
this reason, it is customary to write $AVar(\widehat{\theta})$ 
rather than $Var(\widehat{\theta})$. Similarly we use $ACov$ rather than $Cov$.
[Recall the Warning in @sec-covmats. Just as Cov can mean either the
covariance between two random variables or the covariance matrix of a
random vector, the analogous statement holds for ACov.]{.column-margin}

This can be used to form a confidence interval (see below), but 
also stands on its own as an indication of the accuracy of 
$\widehat{\theta}$.

A, say 95%, confidence interval (CI) for $\mu$ is then

$$
\widehat{\theta} \pm 1.96 ~ \textrm{SE}(\widehat{\theta})
$$

where we denote the standard error of $\widehat{\theta}$ by
$\textrm{SE}(\widehat{\theta})$. 

The 95% figure means that of all possible samples of the given size from
the population, 95% of the resulting confidence intervals will contain
$\theta$. In many cases, the 95% figure is only approximate, stemming
from a derivation that uses the Central Limit Theorem.

In general, for confidence level $1-\alpha$ replace 1.96 by
$z_{\alpha}$, the $1-\alpha/2$ quantile of the N(0,1) distribution, Then
our CI is

$$
\widehat{\theta} \pm z_{\alpha} \textrm{SE}(\widehat{\theta})
$${#eq-pmse}

::: {.callout-important}
## Note Regarding Sensitivity of Phrasing

There is a bit of drama in this word *contain* in the phrase "will
contain $\theta$." Instead of saying the intervals *contain* $\theta$,
why not simply say $\theta$ is *in* the intervals? Aren't these two
descriptions equivalent in terms of English?  Of course they are. 

But many instructors of statistics classes worry that students will take
the description based on "in" to mean that $\theta$ is the random
quantity, when in fact the CI is random (random center, random radius)
and $\theta$ is fixed (though unknown). The instructors thus insist on
the more awkward phrasing "contain," so as to avoid students
misunderstanding. Indeed some instructors would contend that use of the
word *in* is itself just plain incorrect. 

My own view is that in some cases the word *in* is clearer (and
certainly correct in any case), and that it is better to add a warning
about what is random/nonrandom than engage in awkward phrasing.

:::

## The R vcov Function {#sec-rcov}

So, how does one obtain standard errors? In R, in many cases, they
will be provided by the **summary** function, but we may need an entire
estimated covariance matrix rather than just a standard error.

Due to the Multivariate Central Limit Theorem, many common statistical
estimators have approximately normal distributions.  In R, functions
such as **lm**, **glm**, **lme** and **coxph** come with an associated
function **vcov**. This gives the approximate covariance matrix of the
computed estimator, e.g. for the estimated beta coefficients vector in a
linear model. This enables formation of approximate confidence intervals
for not only individual model parameters but also linear combinations of
them, as well as computing other quantities related to confidence sets.
[The **vcov** function is an R *generic* function, playing a similar
role to **print**, **plot**, **summary** and so on. Many R statistical
functions have this for their output, including say **glm**. When we make the
call **vcov(glmOut)**, the R interpreter sees that **glmOut** is of
class "glm" and thus transfers the call to the class-specific function,
**vcov.glm(glmOut)**.  The function **coef**, also used here, is also
generic.]{.column-margin}

### Example: Iranian churn data

Here we use a logistic mode to predict whether a telecom customer will
move to another provider. We obtain $AVar(\widehat{\beta})$ using the
**vcov** function:


```{r}
data(IranianChurn)
glmOut <- glm(Exited ~ ., data = iranChurn, family = binomial)
acov <- vcov(glmOut)
acov
```

There are several categorical variables here, so after expansion to
dummies, $AVar(\widehat{\beta})$ is $12 \times 12$. This is the covariance
matrix for the vector of estimated logistic regression coefficients
$\widehat{\beta}$. 

Say we wish to compare Germany and Spain. The difference will be of the
form $a'\beta$. What should we take for $a$?

``` r
> row.names(acov)
 [1] "(Intercept)"      "CreditScore"      "GeographyGermany" "GeographySpain"  
 [5] "GenderMale"       "Age"              "Tenure"           "Balance"         
 [9] "NumOfProducts"    "HasCrCard1"       "IsActiveMember1"  "EstimatedSalary"
```

Ah, we set $a$ to (0,0,1,-1,0,0,0,0,0,0,0,0)'. Using @eq-quadform,
we compute the standard error and the CI:

```{r}
a <- c(0,0,1,-1,0,0,0,0,0,0,0,0)
avar <- t(a) %*% acov %*% a
se <- sqrt(avar)
estdiff <- t(a) %*% coef(glmOut)
c(estdiff-1.96*se,estdiff+1.96*se)
```




## The Delta Method

This is one of the most useful simple tools in statistics.  

### Motivating example

Now as a first example, say we are estimating a
population mean $\mu$ and are also interested in estimating $\log(\mu)$. 

We will probably use the sample mean $\bar{X}$ to estimate $\mu$, and
thus use $W = \log{\bar{X}}$ to estimate $\log(\mu)$. 
[If we just need to form a confidence
interval for $\log(\mu)$, we can form a CI for $\mu$ and then take the
log of both endpoints. But again, standard errors are of interest in
their own right.]{.column-margin}
But how do we obtain a standard error for $W$?

### Use of the Central Limit Theorem

The Central Limit Theorem tells us that $\bar{X}$ is asymptotically
normally distributed. But what about $\log{\bar{X}}$?

From calculus, we know that a smooth function $f$ can be written as a
Taylor series,

$$
f(x) = f(x_0) + f'(x_0) (x-x_0) + f''(x_0) (x-x_0)^2 /2 + ...
$$

where ' denotes derivative rather than matrix transpose.

In our case here, setting $f(t) = \log{t}$, $x_0 = \mu$ and $x =
\bar{X}$, we have

$$
W = \log{\mu} + \log'({\mu}) (\bar{X}-\mu) + \log''({\mu}) (\bar{X}-\mu)^2 /2 + ...
$$

and $\log'(t) = 1/t$ and so on.

The key point is that as n grows, $\bar{X}-\mu$ goes to 0, and
$(\bar{X}-\mu)^2$ goes to 0 even faster. Using theorems from probability
theory, one can show that, *in the sense of distribution*, 

$$
W \approx log(\mu) + log'(\mu) (\bar{X}-\mu)
$$

The right-hand side is a linear function of $\overline{X}$. The latter
is asymptotically normal by the Central Limit Theorem, and thus the
linear function $W$ is also asymptotically normal.

[Note the wording. The distribution of $W$ is close to a normal
distribution, but mean and variance formulas here apply to that latter
distribution, *not* that of $W$. For example, $E(W)$ could be far from
@eq-eofnormal.]{.column-margin} In other words, $W$ has an approximate
normal distribution that has mean $log(\mu)$ and variance

$$
\frac{1}{\mu^2} \sigma^2/n
$${#eq-eofnormal}

where $\sigma^2$ is the population variance $Var(X)$ . We estimate the
latter by the usual $S^2$ quantity, and thus have our standard error,

$$
\textrm{s.e.}(W) = \frac{S}{\bar{X} \sqrt{n}}
$$

### Use of the Multivariate Central Limit Theorem 

Now, what if the function $f$ has two arguments instead of one? The
above linear approximation is now

$$
f(v,w) \approx f(v_0,w_0) 
+ f_1(v_0,w_0) (v-v_0) 
+ f_2(v_0,w_0)(w-w_0)
$${#eq-taylor}

where $f_1$ and $f_2$ are partial derivatives,[A *partial derivative* of
a function of more than one variable is the derivative with respect to
one of those variables. E.g. $\partial/\partial v ~ vw^2 =
w^2$ and $\partial/\partial w ~ vw^2 =
2vw$.]{.column-margin}

$$
f_1(v,w) = \frac{\partial}{\partial v} f(v,w)
$$

$$
f_2(v,w) = \frac{\partial}{\partial w} f(v,w)
$$

So if we are estimating, for instance, a population quantity
$(\alpha,\beta)'$ by $(Q,R)'$, standard error of the latter is
the square root of

\begin{align}
f_1^2 (Q,R) AVar(Q) &+ f_2^2 (Q,R) AVar(R) \\ 
&+ 2 f_1(Q,R) f_2(Q,R) ACov(Q,R)
\end{align}

where we have made use of @eq-crossprod.

As usual, use of matrix notation can help clean up messy expressions
like this. The *gradient* of $f$, say in the two-argument case as above,
is the vector

$$
\nabla f =
\left (
\begin{array}{r}
f_1 (v_0,w_0) \\
f_2 (v_0,w_0) \\
\end{array}
\right )
$$

so that our above Taylor series @eq-taylor can be written as

$$
f(v,w) \approx f(v_0,w_0) + (\nabla f)' 
 \left (
 \begin{array}{r}
 v-v_0 \\
 w-w_0 \\
 \end{array}
 \right )
$$

(Here ' is matrix transpose, not a derivative.)

Then from @eq-quadform, 

$$
AVar[f(Q,R)] = (\nabla f)' ACov(Q,R) (\nabla f)
$${#eq-deltaquad}

Here 'ACov' means the asymptotic covariance matrix of the vector

$$
W =
\left (
\begin{array}{r}
Q \\
R  \\
\end{array}
\right )
$${#eq-covar}

Let's call that asymptotic covariance matrix $\widehat{\Sigma}$.

### Example: ratio of two means

Often $\widehat{\Sigma}$ will be provided by our application software,
such as with R's **vcov** function, but we will need to derive it in
this case, using properties of sample means.

Say our sample data consists of mother-daughter pairs,

$$
\left (
\begin{array}{r}
M \\
D \\
\end{array}
\right )
$$

representing the heights of mother and daughter. Denote the population mean
vector by

$$
\nu =
\left (
\begin{array}{r}
\mu_M \\
\mu_D \\
\end{array}
\right )
$$

We might be interested in the ratio $\omega = \mu_D / \mu_M$.  Our
estimator will be $\widehat{\omega} = \bar{D} / \bar{M}$, the ratio of
the sample means.

So take $Q = \bar{D}$ and $R =  \bar{M}$: 

$$
\left (
\begin{array}{r}
Q \\
R  \\
\end{array}
\right ) =
\left (
\begin{array}{r}
\bar{D} \\
\bar{M}  \\
\end{array}
\right ) 
$${#eq-covdm}


Then in @eq-deltaquad, with
$f(q,r) = q/r$

$$
\nabla{f} =
 \left (
 \begin{array}{r}
 1/r \\
 -q/r^2  \\
 \end{array}
 \right )
$$

which in our application here is

$$
\nabla{f} =
 \left (
 \begin{array}{r}
 1/\bar{M} \\
 -\bar{D}/\bar{M}^2  \\
 \end{array}
 \right )
$$

Thus we will need $AVar(\bar{D})$, $AVar(\bar{M})$ and $ACov(\bar{D},\bar{M})$.
These quantities are exact, not asymptotic, so we can simplify our
notation, e.g. changing  $AVar(\bar{M})$ to $Var(\bar{M})$.

Be sure to distinguish between population and sample  quantities. For example:

* $Var(M)$ measures how $M$ varies across all individuals in the
  population,

  $$
  E[(M - EM)^2]
  $$

  We can estimate it via its sample analog:

  $$
  \widehat{Var}(M) =
  \frac{1}{n}
  \sum_{i=1}^n (M_i - \bar{M})^2 
  $$

  which measures how much $M$ varies in our sample. Similarly,

  $$
  E[(D - ED)^2]
  $$

  measures how much $D$ varies in our sample, and so on.

* $Var(\bar{M})$ measures how $\bar{M}$ varies across all size-$n$ 
  samples in the population. 


* We know from statistics that

  $$
  Var(\bar{M}) = \frac{1}{n} Var(M)
  $$ 

  Similar properties hold for $Var(\bar{D})$ and 
  $Cov(\bar{D},\bar{M})$:

In other words, we obtain the needed covariance matrix of
@eq-covdm via the sample analog of @eq-matcov as

$$
\frac{1}{n^2} \sum_{i=1}^n (W_i -\bar{W})  (W_i -\bar{W})'
$$

where 

$$
W_i =
\left (
\begin{array}{r}
D_i \\
M_i  \\
\end{array}
\right )
$$

is the value of $W$ for the $i^{th}$ daughter-mother pair in our data.

## Example: Mother/Daughter Height Data

```{r}
library(WackyData)
data(Heights)
head(heights)
m <- heights[,1]
d <- heights[,2]
meanm <- mean(m)
meand <- mean(d)
fDel <- matrix(c(1/meanm,-meand/meanm^2),ncol=1)
n <- length(m)
sigma <- (1/n) * cov(cbind(m,d))
se <- sqrt(t(fDel) %*% sigma %*% fDel)
se
meanmd <- meanm / meand
meanmd
c(meanmd - 1.96*se, meanmd + 1.96*se)
```

## Regarding Those Pesky Derivatives

Though finding expressions for the derivatives in the above example was
not onerous, the function $f$ can be rather complex, with the
expressions for its derivatives even more complicated. Typically such
tedious and error-prone operations can be avoided, by having the
software calculate approximate derivatives.

Recall the definition of derivative:

$$
f'(x) = \lim_{w \rightarrow 0}
\frac{f(x+w) - f(x)}{w}
$$

So an approximate value of $f'(x)$ is obtained by choosing some small
value of $w$ and evaluating 

$$
\frac{f(x+w) - f(x)}{w}
$$

Though of course there is an issue with one's choice of $w$, the point
is that one can code the software to find approximate derivatives
automatically using this device. *This is very common in Data Science
libraries.* 

For example, the R package **numDeriv** will compute numerical
derivatives.

## Scheffe's Method

In analyzing the Iranian Churn data above, we might form many CIs, each
at the 0.95 level. However, we may wish to set an *overall* level to at
least 0.95, meaning that the probability that at least one of the CIs
fails to contain the desired population value is at most 0.05. This
concept is variously known as *multiple inference*,  *simultaneous
inference* or *multiple comparisons*.


Whether to do this is a philosophical question, and the answer will
depend on one's goals and personal preferences, or may possibly be due
to requirements of a research journal or employer. If we do wish to
pursue the matter, then how? One of the most well-known approaches makes
good use of linear algebra.

### A confidence set for $\widehat{\beta}$ in the linear model

[The chi-square distribution with $k$ degrees of freedom is defined to
be the distribution of the sum of the squares of $k$ independent N(0,1)
random variables. After applying properties such as @eq-acova, one can
show that the quadratic form here also has that
distribution.]{.column-margin} It can be shown that if a random vector
$W$ has a $k$-dimensional normal distribution random vector with mean
vector $\mu$ and covariance matrix $\Sigma$, then

$$
Q = (W-\mu)' \Sigma^{-1} (W-\mu)
$$

has a *chi-square* distribution with $k$ *degrees of freedom*. Say
$d_{\alpha}$ is the upper-$\alpha$ quantile of that distribution, i.e.

$$
P[(W-\mu)' \Sigma^{-1} (W-\mu) \leq d_{\alpha}] = 1-\alpha
$$

In the case $k=2$, the set of all $t = (t_1,...,t_k)'$ such that

$$ (t-\mu)' \Sigma^{-1} (t-\mu) = d_{\alpha} $$

is an ellipse in the $(t_1,t_2)$ plane. (If $\Sigma = I$, we have a circle.) For $k=3$ we have an
ellipsoid, a "football," and so on in higher dimensions..  

[We will use the linear model here for concretenss, but the same
analysis holds for any asymptotically normal statistical estimator, e.g.
Maximum Likelihood estimates.]{.column-margin} Now, in the context of
the linear model, take $W$ to be $\widehat{\beta}$, $\mu = \beta$ etc.
Here $\Sigma$ must be estimated by $\widehat{\Sigma}$ (given to us via
**vcov**), so we should replace the chi-square distribution by the
*F-distribution*. But for simplicity, let's stick with chi-square, which
is a good approximation for large $n$ anyway.

So, the event

$$
(\widehat{\beta}-\beta)' \widehat{\Sigma}^{-1} (\widehat{\beta}-\beta) 
\leq d_{\alpha}
$$

has probability $1-\alpha$. Equivalently, the event

$$
(\beta - \widehat{\beta})' \widehat{\Sigma}^{-1} (\beta-\widehat{\beta}) 
\leq d_{\alpha}
$${#eq-Falpha}

has that probability as well.

> Therefore, the set of all $\beta$ satisfying @eq-Falpha is a
> $100(1-\alpha)$ confidence set for the true population $\beta$.

This gives us a confidence ellipse for $\beta$ in two dimensions, a
confidence ellipsoid in three dimensions and so on.

The **ellipse** library can draw this for us:

```{r}
library(ellipse)
data(mtcars)
fit <- lm(mpg ~ disp + cyl , mtcars)
plot(ellipse(fit, which = c('disp', 'cyl'), level = 0.90), type = 'l')
```

Here the axes are $\beta_{disp}$ and $\beta_{cyl}$. 

By the way, note that the smaller $\alpha$ is, the larger will be the
value of $d_{\alpha}$, thus the larger the ellipse. The reader should
pause to confirm that this makes sense.

However, this is but an intermediate step toward our goal of a multiple
inference method. Our next step will be to set up a math tool.

### Lagrange multipliers {#sec-lagrange}

In @sec-linmods, we saw the power of matrix derivatives, in our case
to minimize a sum of squares.  Here we go one step further, again doing
optimization, but in this case under a constraint. To this end,
we first need to introduce the concept of *Lagrange multipliers*.

The context is that we wish to minimize/maximize a quantity $f(w)$,
subject to $g(w) = 0$, where $w$ is a vector argument. We set up the
expression $f(w) + \lambda g(w)$, and find its extreme values.

For instance, say we wish to minimize 

$$
f(x,y) = x^2 + 2y^2
$$

subject to the constraint.

$$
3x + y = 8
$$ 

We form the expression

$$
x^2 + 2y^2 + \lambda (3x + y - 8)
$$

and take partial derivatives with respect to $x$, $y$ and $\lambda$:

$$
0 = 2x + 3 \lambda
$$

$$
0 = 4y + \lambda
$$

$$
0 = 3x+y-8
$$

From the first two equations, we have

$$
0 = 2x - 12y
$$

Substituting, we find that 

$$
y = \frac{8}{19}
$$

and so on.

The Lagrange multiplier here is $\lambda$. If we have several
constraints, we have several multipliers.

### Simultaneous confidence intervals for quantities $a'\beta$

In @sec-rcov, we saw how to form confidence intervals for a quantity
$a'\beta$. We may wish to form several, or even many, such intervals.
Here is how the Scheffe' method can make the intervals simultaneous.

[There is an easily-missed subtlety here. Our phrasing "subject to
$\beta$ being *in* the ellipsoid" (i.e. either in the interior or on the
boundary) is at odds with the Langrange formulation, which stipulates
that the min or max values occur on the boundary. But the latter property
is implied by the linearity of $a'\beta$: Consider any point $q$ that
is strictly interior to the ellipsoid, so our objective function has value
$a'q$.  Then there is room for us to move away from $q$ yet still be
inside the ellipsoid, say to the point $u$, yet with $a'u$ either larger
or smaller that $a'q$, depending on the direction we move in. So the min
or max cannot be in the interior.]{.column-margin} To find a CI for
$a'\beta$, we find its maximum and minimum values subject to $\beta$ being
in the ellipsoid, i.e. subject to the constraint

$$
(\beta-\widehat{\beta})' \widehat{\Sigma}^{-1} (\beta-\widehat{\beta}) = 
d_{\alpha}
$${#eq-constraint}

In other words, we solve the Lagrange multiplier problem

$$
min, max_{\beta, \lambda} ~~
a'\beta + \lambda [(\beta - \widehat{\beta})' 
\widehat{\Sigma}^{-1} (\beta-\widehat{\beta}) - d_{\alpha}] 
$${#eq-scheffeobjftn}

Those max and min values then define our CI.

Let $C$ be a symmetric matrix. The derivative of a quadratic form $u'Cu$
with respect to $u$ can be shown to be $2Cu$. Differentiating with
respect to $\beta$, we thus have 

$$
0 = a 
+ 2 \lambda 
[\widehat{\Sigma}^{-1} (\beta - \widehat{\beta})]
$$

so that 

$$
\beta - \widehat{\beta} =
-\frac{1}{2 \lambda}
\widehat{\Sigma} a
$${#eq-sothat}

Now substitute for $\beta - \widehat{\beta}$ in the constraint
@eq-constraint:

$$
d_{\alpha} = 
(-\frac{1}{2\lambda} \widehat{\Sigma} a)' \widehat{\Sigma}^{-1} 
(-\frac{1}{2 \lambda} \widehat{\Sigma} a) =
\frac{1}{4 \lambda^2} (a' \widehat{\Sigma} a)
$$

Solve for $\lambda$:

$$
2 \lambda = \pm \sqrt{\frac{1}{d_{\alpha}} a'\widehat{\Sigma} a}
$$

Finally, recall that the quantity whose min and max interest us is
$a'\beta$. Using @eq-sothat, we have our CI for $a'\beta$,

$$
a'\widehat{\beta} \pm
\sqrt{d_{\alpha} a'\widehat{\Sigma} a}
$${#eq-scheffeci}

Now, why does this interval hold *simultaneously* over *all* $a$?  The
point is that once we know $\beta$ is in the ellipsoid, then the above
algebraic computations show that for *any* $a$, the quantity $a'\beta$
will be between $a'\widehat{\beta} - \sqrt{d_{\alpha} a'\widehat{\Sigma}
a}$ and $a'\widehat{\beta} + \sqrt{d_{\alpha} a'\widehat{\Sigma} a}$.
Again, this is solely an algebraic property, not a probabilistic one; the only
probabilistic action occurred in $\beta$ being in the ellipsoid.  Thus
@eq-scheffeci will hold for all $a$ simultaneously. 

However, note that we pay a price for the simultaneity, in that
@eq-scheffeci will be wider than the ordinary CI for $a'\beta$, using
@eq-pmse,

$$
a'\widehat{\beta} \pm
z_{\alpha} \sqrt{a'\widehat{\Sigma} a}
$$

Consider our Iranian Churn data analysis above, with $k = 12$. Let's
find $\sqrt{d_{\alpha}}$, for $\alpha = 0.05$:

```{r}
sqrt(qchisq(0.95,12))
```

By contrast, $z_{\alpha} = 1.96$. The Scheffe' interval is more than
twice as wide. As the saying goes, there is no "free lunch."

## Your Turn

❄️  **Your Turn:** In the mother/daughter data, find the estimated
covariance between the two heights.

❄️  **Your Turn:** 



Write an R function with call form

``` r 
regFtnCI(lmOut,t,alpha)
```

that returns an approximate $(1-\alpha)$ confidence interval for
the conditional mean $E(Y | X=t)$. Here **lmOut** is the object returned
by a call to **lm**.

❄️  **Your Turn:** 

The *geometric mean* of a set of $n$ numbers is the $n^{th}$ root of
their product. Write an R function with call form

``` r 
geoGMxbarybar(x,y,alpha)
```

that returns an approximate $100(1-\alpha)$ percent CI for the
population geometric mean of the two numbers $E(X)$ and $E(Y)$,
using the sample analog

$$
\sqrt{\bar{x} ~ \bar{y}}
$$


