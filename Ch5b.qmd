
```{r}
#| include: false
library(dsld)
library(qeML)
```

{{< pagebreak >}}

 
# Inner Product Spaces

The usefulness of vector spaces is greatly enhanced with the addition of
an *inner product* structure.[Inner product spaces are also known as *normed*
spaces.]{.column-margin}

## Geometric Aspirations

You may recall from your high school geometry course the key concept of
perpendicularity, represented by the ⊥ symbol. You may also recall that
in 2-dimensional space,, given a point P and a line L, the line drawn
from point P to the closest point P' within L is perpendicular to L.
The same is true if L is a plane. The point P' is called the
*projection* of P onto L.

This was shown in this book's cover, shown here:

![Projections](Projections.png)

The early developers of linear algebra wanted to extend such concepts to
abstract vector spaces. This aids intuition, and has very powerful
applications.

## Definition

You may have seen *dot products in a course on vector calculus or
physics. For instance, the dot product of the vectors (3,1,1.5)' and
(0,5,6)' is

3x0 + 1x5 + 1.5x6 = 14

This in fact is a standard inner product on $\cal R^3$, but the general
definition is as follows.

<blockquote>

An *inner product* on a vector space $\cal V$, denoted by the ``angle
brackets'' notation $<u,v>$ is a function with two vectors as arguments
and a numerical output, with the following properties:

* $<u,v> = <v,u>$

* The function is bilinear: 

  $$
  <u,av+bw> = a <u,v> + b <u,w>
  $$

* $<u,u> ~ \geq 0$, with equality if and only if $u = 0$.

</blockquote>

## Examples

$\cal R^n$:

As noted, ordinary dot product is the most common inner product on this
space.

$<(a_1,...,a_n),(b_1,...,b_n> =
a_1 b_1 + ... + a_n b_n$

❄️  **Your Turn:** An $n ~ \textrm{x} ~ n$ symmetric matrix $M$ is called
*positive definite* if 

$$
w'Mw > 0
$$

Show that $<u,v> = u'Mv$ is an inner product on $\cal R^n$.

*C(0,1)*:

One inner product on this space as

$$
<f,g> = \int_{0}^{1} f(t) g(t) ~ dt
$$  

For instance, with $f(t) = t^2$ and $g(t) = \sin(t)$, the inner product
can be computed with R:

```{r}
f <- function(t) t^2
g <- function(t) sin(t)
fg <- function(t) f(t) * g(t)
integrate(fg,0,1)
```

This clearly fits most requirements for inner products, but what about
$<f,f> = 0$ only if $f = 0$?[Note that the 0 vector in this space is the
function that is identically 0, not just 0 at some points]{.column-margin}
A non-0 $f$ will have $f^2(t) > 0$ for at least one $t$, and by
continuity, $f^2(t) > 0$ on an interval containing that $t$, thus making
a nonzero contribution to the integral and thus to the inner product.


$\cal RV(\Omega)$:

From here on, we'll restrict to the vector space of all random variables
on $\Omega$ having finite variance. We define

$$
<X,Y> = E(XY)
$$

The properties of expected value, e.g. linearity, show most of the
requirements for an inner product hold.  But again, we need to show that

$$
<X,X> = E(X^2) > 0 
$$

for any $X$ such that $P(X = 0) < 1$. To see this,
recall that for any random variable $X$ with finite variance, we have

$$
Var(X) = E[(X - E(X))^2] = E(X^2) - [E(X)]^2
$$

so that $E(X^2) > 0$ for any nonzero $X$, i.e. any $X$ that is not
identically 0.

## Norm of a Vector

This concept extends the notion of a the length of a vector, as we know
it in $\cal R^2$ and $\cal R^3$.

*Definition:*

<blockquote>

The *norm* of a vector $x$, denoted $||x||$, is

$$
(<x,x>)^{0.5}
$$

</blockquote>

## Important Inequalities

These relations are highly useful, as we will see in the sequel.

### The Cauchy-Schwarz Inequality

<blockquote>

Say $u$ and $v$ are vectors in an inner product space. Then

$$
|<u,v>| \leq ||u|| ~ ||v||
$$

</blockquote>

### Application: Correlation

*Correlation coefficients* are ubiquitous in data science. It is well
known that their values fall into the interval [-1,1]. Let's prove that.

Recall from @sec-cov the notion of the covariance between two random
variables (from which the covariance matrix of a random vector is
formed). We remarked that covariance is intuitively like correlation,but
that latter is a scaled form. Formally,

$$
\rho(X,Y) =
\frac{E[(X-EX)(Y-EY)}{\sqrt{Var(X)} \sqrt{Var(Y)}}
$$

By dividing the covariance by the product of the standard deviations, we
obtain a unitless quantity, i.e. free of units such as centimeters and
degrees.

Now, $X$ and $Y$ are in $\cal RV(\Omega)$. To simplify the algebra,
consider the case $EX = EY = 0$.

Recalling our inner product for this space, we have

$$
<X,Y> = E(XY)
$$

and 

$$
||X||^2 = <X,X> = E(X^2) = Var(X)
$$

with the analogous relations for $Y$.

Cauchy=Schwarz then says

$$
|E(XY)| \leq \sqrt{Var(X)} \sqrt{Var(Y)}
$$

which says the correlation is between -1 and 1 inclusive.

*Proof:*  (follow
https://www.perplexity.ai/search/in-using-the-gnu-screen-comman-47bUohl0TF63gZodzQ2dzQ?__cf_chl_tk=4PwrRobefvA_D4M64udWv.CY6CdcFyhvn9LO7Yk2Xlk-1734144596-1.0.1.1-zyojn6rqoKNR7.a6H_jnMFMYGreSr0ISSXwrgYuRBlI)

### The Triangle Inequality

In the world of ordinary physical geometry, we know the following

<blockquote>

The distance from A to B is less than or equal to the sum of the
distances from A to C and C to B.

</blockquote>

(proof following
https://math.stackexchange.com/questions/91181/proof-for-triangle-inequality-for-vectors)

### Exercizes

❄️  **Your Turn:** Consider the space $C(0,1)$. For function $f$ and $g$ 
of your own choosing, verify that the Cauchy-Schwarz and Triangle
Inequalities hold in that case.


## Projections

(follow Hilbert Projection Theorem,
https://en.wikipedia.org/wiki/Hilbert_projection_theorem

As mentioned, the extension of classical geometry to abstract vector
spaces has powerful applications. There is no better example of this
than the idea of *projections*.

### Definitions

<blockquote>

Consider an inner product space $\cal V$, with subspace $\cal W$.

* We say that vectors $x$ and $y$ in $\cal V$ are
  *orthogonal* if $<x,y> = 0.$ 

* Let $x$ be a vector in $\cal V$. Then the *orthogonal projection*
  of $x$ onto $\cal W$, denoted $Px$, is a vector in $\cal W$
  for which $<Px,x-Px> = 0$.

</blockquote>

Projections are generally unique, but there are mathematical niceties to
consider, such as whether $W$ is topologically closed. Such issues are
beyond the scope of this book, and we will assume uniqueness.

### Minimum-distance property

This will be key in the sequel, as many methods in data science involve
minimizing some quantity.

<blockquote>

*Theorem*:The minimum value of the quantity $||x - y||$ over all $y$ in
$\cal W$ is attained by setting $y = Px$, so that 

</blockquote>

## Orthogonal Bases

### The Gram-Schmidt method

### Application: Best polynomial approximation to a function

## Shrinkage Estimators

## Application: Fairness in Algorithms

