```{r}
#| include: false
library(dsld)
library(qeML)
```

{{< pagebreak >}}

 
# Inner Product Spaces

::: {.callout-note}

## Goals of this chapter:

The usefulness of vector spaces is greatly enhanced with the addition of
an *inner product* structures.  We motivate and define such structures
here, and present applications.

Among other things, we will analyze a method for removing racial, gender
etc. bias in machine learning algorithms.

The material in this chapter will be crucial to the remaining chapters.
Extra attention on the part of the reader will yield major dividends.

:::

## Geometric Aspirations

You may recall from your high school geometry course the key concept of
perpendicularity, represented by the ⊥ symbol. You may also recall that
in 2-dimensional space, given a point P and a line L, the line drawn
from point P to the closest point P' within L is perpendicular to L.
The same is true if L is a plane. The point P' is called the
*projection* of P onto L.

This was shown in this book's cover, shown here:

![Projections](Projections.png)

The point at the end of the green vector is projected onto the
mustard-colored plane, a subspace, producing the red vector. It in turn
is projected onto the blue line, the latter being a subspace of the
mustard-colored one. There are right angles in each case.


The early developers of linear algebra wanted to extend such concepts to
abstract vector spaces. This aids intuition, and has very powerful
applications.

## Definition

You may have seen dot products in a course on vector calculus or
physics. For instance, the dot product of the vectors (3,1,1.5)' and
(0,5,6)' is

3x0 + 1x5 + 1.5x6 = 14

This in fact is a standard inner product on $\cal R^3$, but the general
definition is as follows.

<blockquote>

An *inner product* on a vector space $\cal V$, denoted by the "angle
brackets" notation $<u,v>$, is a function with two vectors as arguments
and a numerical output, with the following properties:

* $<u,v> = <v,u>$

* The function is bilinear: 

  $$
  <u,av+bw> = a <u,v> + b <u,w>
  $$

* $<u,u> ~ \geq ~ 0$, with equality if and only if $u = 0$.

</blockquote>

## Examples

### $\cal R^n$

As noted, ordinary dot product is the most common inner product on this
space.

$<(a_1,...,a_n),(b_1,...,b_n> =
a_1 b_1 + ... + a_n b_n$

But other inner products may be defined. For instance, say an $n \times
n$ matrix $A$ is *positive definite*, meaning that 

$$
q' A q \geq 0
$$

for all $q$ in $\cal R^n$, with equality only if $q = 0$. Then it is
easily seen that the function 

$$
g(q,r) = q'Ar
$$

fits the definition of inner product.

### *C(0,1)*

One inner product on this space is

$$
<f,g> = \int_{0}^{1} f(t) g(t) ~ dt
$$  

For instance, with $f(t) = t^2$ and $g(t) = \sin(t)$, the inner product
can be computed with R:

```{r}
f <- function(t) t^2
g <- function(t) sin(t)
fg <- function(t) f(t) * g(t)
integrate(fg,0,1)
```

This clearly fits most requirements for inner products, but what about
$<f,f> = 0$ only if $f = 0$?[Note that the 0 vector in this space is the
function that is identically 0, not just 0 at some points]{.column-margin}
A non-0 $f$ will have $f^2(t) > 0$ for at least one $t$, and by
continuity, $f^2(t) > 0$ on an interval containing that $t$, thus making
a nonzero contribution to the integral and thus to the inner product.

### $\cal RV(\Omega)$

[We need to restrict attention to this subspace, in order to meet the
requirement that $<U,U> = 0$ if and only if $U = 0$. Otherwise, any
constant random variable would violate that condition.]{.column-margin}
We will focus on the subspace $\cal RV(\Omega)_0$ of random variables
that have mean 0, and take covariance as our inner product:

$$
<U,V> = Cov(U,V) = E[(U - EU) (V - EV)] = E(UV)
$$

The properties of expected value, e.g. linearity, show that the
requirements for an inner product hold.

## Norm of a Vector

This concept extends the notion of a the length of a vector, as we know
it in $\cal R^2$ and $\cal R^3$.

*Definition:*

<blockquote>

The *norm* of a vector $x$, denoted $||x||$, is

$$
(<x,x>)^{0.5}
$$

The *distance* from a vector $x$ to a vector $y$ is

$$
||y - x||
$$

</blockquote>

## Sets of Orthogonal Vectors

We say that vectors $u$ and $v$ are *orthogonal* if $<u,v> = 0$. This is
the general extension of the notion of perpendicularity in high school
geometry. 

If the set of vectors $u_1,...,u_r$ form a basis for a vector space, 
it is called an *orthogonal* basis. If each $u_i$ has length 1 (which we
can arrange by dividing by its norm), the set is called an *orthonormal
basis*.

## The Cauchy-Schwarz Inequality

::: {#thm-cs}

## Cauchy-Schwarz Inequality

Say $u$ and $v$ are vectors in an inner product space. Then

$$
|<u,v>| \leq ||u|| ~ ||v||
$$

:::

::: {.proof}

See the Your Turn problem below.

:::

### Application: Correlation{#sec-corr}

*Correlation coefficients* are ubiquitous in data science. It is well
known that their values fall into the interval [-1,1]. Let's prove that.

In @sec-covar, in introducing the notion of the covariance between two
random variables, we remarked that covariance is intuitively like
correlation, but that the latter is a scaled form. Formally,

$$
\rho(X,Y) =
\frac{E[(X-EX)(Y-EY)]}{\sqrt{Var(X)} \sqrt{Var(Y)}}
$$ {#eq-rhodef}

By dividing the covariance by the product of the standard deviations, we
obtain a unitless quantity, i.e. free of units such as centimeters and
degrees.

Now, say $X$ and $Y$ are in $\cal RV(\Omega)_0$.  Recalling our inner
product for this space, we have

$$
<X,Y> = E(XY)
$$

and 

$$
||X||^2 = <X,X> = E(X^2) = Var(X)
$$

with the analogous relations for $Y$.

Cauchy-Schwarz then says

$$
|E(XY)| \leq \sqrt{Var(X)} \sqrt{Var(Y)}
$$

which says the correlation is between -1 and 1 inclusive.

## The Triangle Inequality

In the world of ordinary physical geometry, we know the following

*The distance from A to B is less than or equal to the sum of the
distances from A to C and C to B.* This is true as well in general,
abstract inner product spaces.

::: {#thm-triangle}

## Triangle Inequality

In a general inner product space,

$$
||x - z|| \leq ||x - y|| + ||y - z|| 
$$

:::

::: {.proof}

See the Your Turn problem below.

:::

## The Pythagorean Theorem

That this ancient theorem in geometry still holds in general inner
product spaces is a tribute to the power of abstraction.

::: {#thm-pythag}

If vectors $X$ and $Y$ are orthogonal, then

$$
||X+Y||^2 = ||X||^2 + ||Y||^2
$${#eq-pythag}

:::

::: {.proof}

Replace the norms by expression in inner products. Simplify using
properties of inner product.

:::


## Your Turn

❄️  **Your Turn:** Consider the space $C(0,1)$. For function $f$ and $g$ 
of your own choosing, verify that the Cauchy-Schwarz and Triangle
Inequalities hold in that case.

❄️  **Your Turn:** Derive the Cauchy-Schwarz Inequality, using the
following algebraic outline:

* The inequality

  $$
  0 ~ \leq ~ <(au+v),(au+v)> ~
  $$

  holds for any scalar $a$.

* Expand the right-hand side (RHS), using the bilinear property of inner 
  products.

* Minimize the resulting RHS with respect to $a$.

* Collect terms to yield

   $$
   <u,v>^2 \leq ||u||^2 ||v||^2
   $$

❄️  **Your Turn:** Use the Cauchy-Schwarz Inequality to prove the
Triangle Inequality, using the following algebraic outline.

* Start with

  $$
  ||u+v||^2 = <(u+v,u+v>
  $$

* Expand the RHS algebraically.

* Using Cauchy-Schwarz to make the equation an inequality.

* Collect terms to yield the Triangle Inequality.

❄️  **Your Turn:** Explain why any set of orthogonal vectors is linearly
independent.


