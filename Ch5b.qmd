
```{r}
#| include: false
library(dsld)
library(qeML)
```

{{< pagebreak >}}

 
# Inner Product Spaces

::: {.callout-note}

## Goals of this chapter:

The usefulness of vector spaces is greatly enhanced with the addition of
an *inner product* structures.[Inner product spaces are also known as *normed*
spaces.]{.column-margin} We motivate and define such structures here,
and present applications.

Among other things, we will analyze a method for removing racial, gender
etc. bias in machine learning algorithms.

:::



## Geometric Aspirations

You may recall from your high school geometry course the key concept of
perpendicularity, represented by the ⊥ symbol. You may also recall that
in 2-dimensional space, given a point P and a line L, the line drawn
from point P to the closest point P' within L is perpendicular to L.
The same is true if L is a plane. The point P' is called the
*projection* of P onto L.

This was shown in this book's cover, shown here:

![Projections](Projections.png)

The point at the end of the green vector is projected onto the
mustard-colored plane, producing the red vector. It in turn is projected
onto the blue line. There are right angles in each case.


The early developers of linear algebra wanted to extend such concepts to
abstract vector spaces. This aids intuition, and has very powerful
applications.

## Definition

You may have seen dot products in a course on vector calculus or
physics. For instance, the dot product of the vectors (3,1,1.5)' and
(0,5,6)' is

3x0 + 1x5 + 1.5x6 = 14

This in fact is a standard inner product on $\cal R^3$, but the general
definition is as follows.

<blockquote>

An *inner product* on a vector space $\cal V$, denoted by the ``angle
brackets'' notation $<u,v>$, is a function with two vectors as arguments
and a numerical output, with the following properties:

* $<u,v> = <v,u>$

* The function is bilinear: 

  $$
  <u,av+bw> = a <u,v> + b <u,w>
  $$

* $<u,u> ~ \geq 0$, with equality if and only if $u = 0$.

</blockquote>

## Examples

$\cal R^n$:

As noted, ordinary dot product is the most common inner product on this
space.

$<(a_1,...,a_n),(b_1,...,b_n> =
a_1 b_1 + ... + a_n b_n$

❄️  **Your Turn:** An $n ~ \textrm{x} ~ n$ symmetric matrix $M$ is called
*positive definite* if 

$$
w'Mw > 0
$$

Show that $<u,v> = u'Mv$ is an inner product on $\cal R^n$.

*C(0,1)*:

One inner product on this space is

$$
<f,g> = \int_{0}^{1} f(t) g(t) ~ dt
$$  

For instance, with $f(t) = t^2$ and $g(t) = \sin(t)$, the inner product
can be computed with R:

```{r}
f <- function(t) t^2
g <- function(t) sin(t)
fg <- function(t) f(t) * g(t)
integrate(fg,0,1)
```

This clearly fits most requirements for inner products, but what about
$<f,f> = 0$ only if $f = 0$?[Note that the 0 vector in this space is the
function that is identically 0, not just 0 at some points]{.column-margin}
A non-0 $f$ will have $f^2(t) > 0$ for at least one $t$, and by
continuity, $f^2(t) > 0$ on an interval containing that $t$, thus making
a nonzero contribution to the integral and thus to the inner product.


$\cal RV(\Omega)$:

From here on, we'll restrict to the vector space of all random variables
on $\Omega$ having finite variance. We define

$$
<X,Y> = E(XY)
$$

The properties of expected value, e.g. linearity, show most of the
requirements for an inner product hold.  But again, we need to show that

$$
<X,X> = E(X^2) > 0 
$$

for any nonzero $X$, i.e. any $X$ such that $P(X = 0) < 1$. 

❄️  **Your Turn:** Show that if $<X,X> = 0$, then $X$ must be the 0
vector. Make use of the facts that $Var(X) = E(X^2) - [E(X)]^2$ and
$Var(X) \geq 0$ with equality only of $X = c$ for some constant $c$.


## Norm of a Vector

This concept extends the notion of a the length of a vector, as we know
it in $\cal R^2$ and $\cal R^3$.

*Definition:*

<blockquote>

The *norm* of a vector $x$, denoted $||x||$, is

$$
(<x,x>)^{0.5}
$$

The *distance* from a vector $x$ to a vector $y$ is

$$
||y - x||
$$

</blockquote>

## Important Inequalities

These relations are highly useful, as we will see in the sequel.

### The Cauchy-Schwarz Inequality

<blockquote>

Say $u$ and $v$ are vectors in an inner product space. Then

$$
|<u,v>| \leq ||u|| ~ ||v||
$$

*Proof:* See the Your Turn problem below.

</blockquote>

❄️  **Your Turn:** Derive the Cauchy-Schwarz Inequality, using the
following algebraic outline:

* The inequality

  $$
  0 \leq <(au+v),(au+v)>
  $$

  holds for any scalar $a$.

* Expand the right-hand side (RHS), using the bilinear property of inner 
  products.

* Minimize the resulting RHS with respect to $a$.

* Collect terms to yield

   $$
   <u,v>^2 \leq ||u||^2 ||v||^2
   $$

### Application: Correlation

*Correlation coefficients* are ubiquitous in data science. It is well
known that their values fall into the interval [-1,1]. Let's prove that.

Recall from @sec-cov the notion of the covariance between two random
variables (from which the covariance matrix of a random vector is
formed). We remarked that covariance is intuitively like correlation,but
that latter is a scaled form. Formally,

$$
\rho(X,Y) =
\frac{E[(X-EX)(Y-EY)]}{\sqrt{Var(X)} \sqrt{Var(Y)}}
$$

By dividing the covariance by the product of the standard deviations, we
obtain a unitless quantity, i.e. free of units such as centimeters and
degrees.

Now, $X$ and $Y$ are in $\cal RV(\Omega)$. To simplify the algebra,
consider the case $EX = EY = 0$.[Actually, we should say, 
``Make the transformation $X \rightarrow X - EX$,
and note that it leaves both sides of the above correlation formula
unchanged. We can thus assume $EX = EY = 0$.'' This is a common strategy,
and should be kept in mind.]{.column-margin}

Recalling our inner product for this space, we have

$$
<X,Y> = E(XY)
$$

and 

$$
||X||^2 = <X,X> = E(X^2) = Var(X)
$$

with the analogous relations for $Y$.

Cauchy=Schwarz then says

$$
|E(XY)| \leq \sqrt{Var(X)} \sqrt{Var(Y)}
$$

which says the correlation is between -1 and 1 inclusive.

### The Triangle Inequality

<blockquote>

In the world of ordinary physical geometry, we know the following

*The distance from A to B is less than or equal to the sum of the
distances from A to C and C to B.* 

In a general inner product space,

$$
||x - z|| \leq ||x - y|| + ||y - z|| 
$$

*Proof:* See the Your Turn problem below.

</blockquote>



### Exercises{#sec-cauchyexercises}

❄️  **Your Turn:** Use the Cauchy-Schwarz Inequality to prove the
Triangle Inequality, using the following algebraic outline.

* Start with

  $$
  ||u+v||^2 = <(u+v,u+v>
  $$

* Expand the RHS algebraically.

* Using Cauchy-Schwarz to make the equation an inequality.

* Collect terms to yield the Triangle Inequality.


❄️  **Your Turn:** Consider the space $C(0,1)$. For function $f$ and $g$ 
of your own choosing, verify that the Cauchy-Schwarz and Triangle
Inequalities hold in that case.

❄️  **Your Turn:** Say we roll a die once, producing $X$ dots. If $X = 6$,
we get a bonus roll, yielding $B$ additional dots; otherwise, $B = 0$.
Let $Y = X+B$. Verify that $X$ and $B$ satisfy the Cauchy-Schwarz and
Triangle Inequalities, and also find $\rho(X,B)$.


## Projections

As mentioned, the extension of classical geometry to abstract vector
spaces has powerful applications. There is no better example of this
than the idea of *projections*.

### Theorem

We say that vectors $u$ and $v$ are *orthogonal* if $<u,v> = 0$. This is
the general extension of the notion of perpendicularity in high school
geometry.

<blockquote>

Consider an inner product space $\cal V$, with subspace $\cal W$.  Then
for any vector $x$ in $\cal W$, there is a unique vector $z$ in $\cal
V$, such that $z$ is the closest vector to $x$ in $\cal W$. 

Furthermore, $x-z$ is orthogonal to any vector $r$ in $\cal W$.

</blockquote>

The full proof is beyond the scope of this book, as it requires
background in real analysis. Indeed, even the statement of the theorem
is not mathematically tight.[For readers who do have such background,
this is the Hilbert Projection Theorem. "Closest" is defined in terms of
infimum, and $\cal W$ needs to be a topologically closed
set. See for example [the Wikipedia entry.l](https://en.wikipedia.org/wiki/Hilbert_projection_theorem)]{.column-margin}

For example, in the case of $R^n$, It will be seen shortly that for each
$\cal W$ in the theorem, there is a matrix $P_{W}$ that implements the
projection, i.e.

$$
z = P_W x
$$

Note that projection operators are *idempotent*, meaning that if you
apply a projection twice, the effect is the same as applying it once. In
the matrix equation above, this means $P_W^2 = P_W$. This makes sense;
once you drop down to the subspace, there is no further dropping down to
that same space.

The case of $\cal RV(\Omega)$ deserves its own section, coming up next.

## Projections in$\cal RV(\Omega)$

Here is what this very abstract vector space becomes useful in practical
applications, such as will be presented in @sec-fairness. We will need
to build a bit of infrastructure.[The material in this section is rather
involved, but it is needed for the section on racial, gender etc.
fairness in machine learning, @sec-fairness, which hopefully makes it
worthwhile.]{.column-margin}

### Conditional expectation

Recall the Your Turn problem in @sec-cauchyexercises:

<blockquote>

We roll a die once, producing $X$ dots. If $X = 6$,
we get a bonus roll, yielding $B$ additional dots; otherwise, $B = 0$.
Let $Y = X+B$...

</blockquote>

*The basics:*

Now, what is $E(Y | B = 2)$? If $B = 2$, then we got the bonus roll, so
$X = 6$ and $Y = X + B = 8$:

$$
E(Y | B = 2) = 8
$$

More generally, 

$$
E(Y | B = i) =
\begin{cases}
3 & i = 0 \\
6 + i & i = 1,2,3,4,5
\end{cases}
$$

❄️  **Your Turn:** Show the details in the above computation.

*Random variable form:*

The quantity $E(Y | B = i)$, a number, can be converted to a random
variable, in the form of a function of $B$, which we will call $Q$, and 
denoted $E(Y | B)$, where

$$
Q = 
\begin{cases}
3 & B = 0 \\
6 + B & B = 1,2,3,4,5
\end{cases}
$$

$B$ is random, so $r(B)$ is also random, and here is its distribution:

<blockquote>

$$
Q \textrm{ takes on the values } 3,7,8,9,10,11 \textrm{ with
probabilities } 
$$
$$
1/6, 1/36, 1/36, 1/36, 1/36, 1/36
$$ 

</blockquote>

We need one more thing:

*The Law of Iterated Expectation:*

For random variable $U$ and $V$, set

$$
R = E[V |U]
$$

Then 

$$
E(R) = E(V)
$$

More concisely:

$$
E[E(V|U)] = E(V)
$${#eq-iterexpect}

Intuitive explanation: Say we wish to compute the mean height $E(H)$ of
all students at a university. We might ask each department $D$ to
measure their own students, and report to us the resulting mean $E(H|D)$. 
We could then average all those departmental means to get the overall
mean for the university:

$$
E[E(H|D)] = E(H)
$$

Note, though that that outer $E()$ (the first 'E') is a weighted
average, since some departments are larger than others. The weights are
the distribution of $D$.

❄️  **Your Turn:** In the die rolling example above, verify that

$$
E[E(Y|B)] = E(Y)
$$

### Projections in $\cal RV(\Omega)$: how they work

Consider random variables $X$ and $Y$. Let $W$ be the set of all
functions of $X$ with finite variance, which is a subspace of the vector
space $\cal RV(\Omega)$. The above theorem talks of a closest vector $C$ in
$W$ to $Y$. Let's see what form $C$ might take in this vector space.

Remember, the (squared) distance from $Y$ to $C$ is

$$
||Y - C||^2 = <Y-C,Y-C> = E[(Y-C)^2]
$$

That last term is 

$$
E[ E((Y-C)^2 | X) ]
$$

For any random variable $Q$ of finite variance, the minimum value of
$E[(Q-d)^2]$ over all constants $d$ is attained by $d = E(Q)$. (See Your
Turn problem below.) So, the minimum of $E((Y-C)^2 | C)$, for all random
variables $C$, is attained by the conditional mean,[Note that since we
are conditioning on the random variable $C$, it becomes a constant, like
$d$ above.]{.column-margin}

$$
C = E(Y | X)
$$

In other words:

<blockquote>

Projections in $\cal RV(\Omega)$ are conditional means.

</blockquote>

Moreover:

Since the difference between a vector and its projection onto a subspace
is orthogonal to that subspace we have:

<blockquote>

The vector $Y - E(Y|X)$ is uncorrelated with $E(Y|X)$.  In other words,
the prediction error (also called the *residual* has 0 correlation with
the prediction itself.

</blockquote>

❄️  **Your Turn:** Prove this assertion.

## Projections in the Linear Model

The case of the linear model will deepen our understanding, and will
lead to a method for outlier detection that is commonly used in
practice.

### The least-squares solution is a projection

Armed with our new expertise on inner product spaces, we see that
@eq-matrixss is

$$
<S-Ab,S-Ab>
$$

in the vector space $R^n$, where $n$ is the number of our data
points.[Of course, "data points" means rows in the data frame.  In the
statistics realm, people often speak of "observations."]{.column-margin}
Since we are minimizing that quantity with respect to $b$, the solution,
$A \widehat{\beta}$, is the projection of $Y$ onto the subspace.

But wait -- *what* subspace?  Well, it is the subspace consisting of all
vectors of the form $Ab$:

<blockquote>

The linear model projects the vector $Y$ onto the column space of $A$.

</blockquote>

### Application: identifying outliers

An *outlier* is a data point that is rather far from the others. It
could be an error, or simply an anomalous case. Even in the latter
situation, such a data point could distort our results, so in both
cases, identifying outliers is important.

Recall @eq-linregformula, the general solution to our linear regression
model: 

$$
\widehat{\beta} = (A'A)^{-1} A'S
$$

The projection itself is then 

$$
A \widehat{\beta} = A(A'A)^{-1} A'S = HS
$$

where the matrix $H = A(A'A)^{-1} A'$, which projects $S$ onto the column
space of $A$, is called the *hat matrix*. 

As a projection, $H$ is idempotent, which one can easily verify by
multiplication.  $H$ is also symmetric.

❄️  **Your Turn:** Show this.

Let $h_{ii}$ denote element $i$ of the diagonal of $H$, with $x_i$
denoting row $i$ of A. One can show that

$$
h_{ii} = x_{i} (A'A)^{-1} x_i'
$$

❄️  **Your Turn:** Show this.

The quantity $h_{ii}$ is called the *leverage* for datapoint $i$.

Ah, so we know $h_{ii} \geq 0$.  But also,using an extension of
@eq-traceab,[It can be shown that trace is invariant under *circular
shifts*, e.g. $UVW$, $VWU$ and $WUV$ all have the same
trace.]{.column-margin} we have

$$
tr(H) = 
tr[\underbrace{A(A'A)^{-1}} A'] =
tr[A'\underbrace{A(A'A)^{-1}}] = tr(I) = p
$$

for $A$ of size $n \textrm{x} p$.

Thus the average value of $h_{ii}$ is $p/n$. Accordingly, we might
suspect an outlier if $h_{ii}$ is considerably larger than $p/n$. 

For example, let's look at the census data we've seen earlier:

```{r}
library(qeML)
data(mlb1)
ourData <- as.matrix(mlb1[,-1]) # must have matrix to enable %*%
A <- cbind(1,ourData[,c(1,3)])
dim(A)
S <- as.vector(mlb1[,3])
H <- A %*% solve(t(A) %*% A) %*% t(A)
hist(diag(H))
```

The ratio $p/n$ here is 3/1015, about 0.003. We might take a look at the 
observations having $h_{ii}$ above 0.01, say.

## Orthogonal Bases
### The Gram-Schmidt method

### Application: 
Best polynomial approximation to a function

## Application: Racial, Gender Etc. Fairness in Algorithms{#sec-fairness}

