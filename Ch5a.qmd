
```{r}
#| include: false
library(dsld)
library(qeML)
```

{{< pagebreak >}}


# Vector Spaces   

$$
x^2  
$$

::: {.callout-note}

## Goals of this chapter:

As noted earlier, the two main structures in linear algebra are matrices
and vector spaces. We now introduce the latter, a bit more abstract than
matrices but even more powerful. We lay the crucial foundation in this
chapter. There won't be any direct practical applications in this
chapter, but we will then reap the huge benefits of this material in the
applications in the remaining chapters.

:::

## Review of a Matrix Rank Property

In the last chapter, we presented the concepts of matrix row and column
rank, defined to be the maximal number of linearly independent
combinations of the rows or columns of the matrix, respectively. We
proved that


> For any matrix $B$, 
> 
> $$
> \textrm{rowrank}(B)
> = \textrm{colrank}(B)
> = \textrm{rowrank}(B_{rref})
> = \textrm{colrank}(B_{rref})
> $$

We can say something stronger: 

::: {#thm-span}

Let $\cal V$ denote the *span* of the rows of $B$, i.e. the
set of all possible linear combinations of rows of $B$. Define $\cal
V_{rref}$ similarly for $B_{rref}$. Then

$$
\cal V = \cal V_{rref}
$$

The analogous result holds for columns.

:::

::: {.proof}

Actually, the theorem follows immediately from our comment following
@lem-elemnotchangerank: "every nonzero linear combination of the rows of
$A$ corresponds to one for the rows of $A_{rref}$, and vice versa."
This showed a one-to-one correspondence between the two sets of linear
combinations.

$\square$

:::

So, not only do the two matrices have the same maximal numbers of 
linearly independent rows, they also generate *the same linear
combinations* of those rows.

The sets $\cal V$ and $\cal V_{rref}$ are called the *row spaces*
of the two matrices, and yes, they are examples of vector spaces, as we
will now see.

## Vector Space Definition

> A set of objects $\cal W$ is called a *vector space* if it satisfies the
> following conditions:
> 
> * Some form of addition between vectors $u$ and $v$, denoted $u+v$, is
>   defined in $\cal W$, with the result that *u+v* is also in $\cal W$.
>   We describe that latter property by saying $\cal W$ is *closed* under
>   addition. 
> 
> * There is a unique element called "0" such that  $u +
>   0 = 0 + u = u$.
> 
> * Some form of scalar multiplication is defined, so that for any number
>   $c$ and any $u$ in $\cal W$, $cw$ exists and is in $\cal W$. 
>   We describe that latter property by saying $\cal W$ is *closed* 
>   under scalar multiplication
> 
> * This being a practical book with just a dash of theory, we'll skip the
>   remaining conditions, involving algebraic properties such as
>   commutativity of addition ($u+v = v+u$).

### Examples

### $\cal R^{n}$

In the vast majority of examples in this book, our vector space will be
$\cal R^{n}$.

Here $\cal R$ represents the set of all real numbers, and $\cal R^{n}$ is
simply the set of all vectors consisiting of $n$ real numbers. In an $m
\times k$ matrix the rows are members of $\cal R^{m}$ and the
columns are in $\cal R^{k}$.

### The set C(0,1) of all continuous functions on the interval [0,1]

Here elements of the vector space are functions, as in calculus. Each
function is an object in the vector space.  Vector addition and scalar
multiplication are done as functions. If say $u$ is the square root
function and $v$ is the sine function, then for example $3u$ is the
function

$$
f(x) = 3x^{0.5}
$$

and $u+v$ is defined to be

$$
f(x) = x^{0.5} + \sin(x)
$$

### A set $\cal RV(\Omega)$ of random variables 

This vector space will consist of all random variables $X$
defined on some probability space $\Omega$, with the additional
restriction that $E(X^2) < \infty$, i.e. $X$ has finite variance.

Consider the example in {@sec-mlb} on major league baseball players.  We
choose a player at random.  Denote weight, height and age by $W$, $H$
and $A$. Since the player is random, these three quantities are random
variables.

Vector addition and scalar multiplication are defined in a
straightforward manner. For instance, the sum of $H$ and $A$
is simply height + age. That is a random variable, thus a member of this
vector space.

This may seem like a rather nonsensical sum, but it fits the technical
definition, and moreover, we have already been doing things like this!
This after all is what is happening in our prediction expression from
the baseball data, e.g.

$$
\textrm{predicted weight} =
-187.6382 + 4.9236 H+ 0.9115 A
$$

In fact, in this vector space, the above is a linear combination of the
random variables $1$, $H$ and $A$. ($1$ is a nonrandom constant, but
still counts as a random variable.) Note that random variables such as
$HW^{1.2}$ and so on are also members of this vector space, essentially
any function of $W$, $H$ and $A$ is a random variable and thus in this
vector space.

## Subspaces

Say $\cal W_{1}$ a subset of a vector space $\cal W$, such that $\cal
W_{1}$ is closed under addition and scalar multiplication. $\cal W_{1}$
is called a *subspace* of $\cal W$.  Note that a subspace is also a
vector space in its own right.

### Examples

$R^3$: 

For instance, take $\cal W_{1}$ to be all vectors of the form
(a,b,0). Clearly, $\cal W_{1}$ is closed under addition and scalar
multiplication, so it is subspace of $R^3$.

Another subspace of $R^3$ is the set of vectors of the form (a,a,b), i.e. those
vectors whose first two element are equal. The reader should check that
this set of vectors is closed under addition and scalar multiplication,
thus is a subspace of $R^3$.

What about vectors of the form (a,b,a+b)? Yes.

We saw earlier that the *row space* of an $m \times n$ matrix $A$,
consisting of all linear combinations of the rows of $A$, is a subspace
of $R^m$. Similarly, the *column space*, consisting of all linear
combinations of columns of $A$, is a subspace of $R^n$.

Another important subspace is the *null space*, the set of all $x$ 
such that $Ax = 0$. The reader should verify that this is indeed a
subspace of $R^n$.

$C(0,1)$: 

One subspace is the set of all polynomial functions. Again,
the sum of two polynomials is a polynomial, and the same holds for
scalar multiplication, so the set of polynomials is closed under those
operations, and is a subspace.

$\cal RV(\Omega)$: 

The set of all random variables that are functions of $H$, say, is a
subspace of $\cal RV(\Omega)$. Another subspace is the set of all random
variable with mean 0.

### Span of a set of vectors

As noted earlier, the *span* of a set of vectors $G = {v_1,...,v_k}$ is
the set of all linear combinations of those vectors. It's a subspace of
the main space that the $v_i$ are members of.

## Basis 

Consider a set of vectors $u_1,...u_r$ in a vector space $\cal W$.
Recall that the span of these vectors is defined to be the set of all
linear combinations of them. In verb form, we say that $u_1,...u_r$
*spans* $\cal W$ if we can generate the entire vector space from those
vectors via linear combinations. It's even nicer if the vectors are
linearly independent:

> We say the vectors $u_1,...u_r$ in a vector space $\cal W$ form a *basis*
> for $\cal W$ if they are linearly independent and span $\cal W$.

Note that subspaces are vector spaces in their own right, and thus also
have bases.

### Example: $\cal R^3$: 

The vectors (1,0,0), (0,1,0) and (0,0,1) are easily seen to be a basis
here.[Our convention has been that vectors are considered in matrix
terms as column vectors by default. However, in nonmatrix contexts, it
will be convenient to write $\cal R^n$ vectors as rows.]{.column-margin}
They are linearly independent, and clearly span $\cal R^3$. For
instance, to generate (3,1,-0.2), we use this linear combination:

(3,1,-0.2) = 3 (1,0,0) + 1 (0,1,0) + (-0.2) (0,0,,1)

But bases are not unique; for instance, the set (1,0,0, (0,1,0), (0,1,1) works
equally well as a basis for this space, as do (infinitely) many others..

A basis for the subspace of vectors of the form (a,a,b) is (1,1,0) and
(0,0,1). 

### Infinite-dimensional vector spaces

Note too that we have implictly assumed above that our vector spaces here are 
*finite-dimensional*, i.e. that any basis consists of finitely-many
vectors ($r$ in the above definition). This need not be the case.

### Example: $\cal Poly(0,1)$

This is the space of all polynomials defined on the interval (0,1), such
as $f(t) = 2.5t$ and $g(t) = t^{13} - t^{12} - t^{5} + t + 1$. Vector addition
and scalar multiplication can be defined in the obvious manner, e.g.

$$
h(t) = 2f(t) + 3g(t) = 3t^{13} - 3t^{12} - 3 t^{5} - 2t + 3
$$

A natural basis is 

$$
u_i (t) = t^i, ~ i = 1,2,3,...
$${#eq-polybasis}

$C(0,1)$: 

There is no basis here, finite or infinite. But with more sophisticated
mathematics, something like an infinite basis can be formed. A full
account would be beyond the scope of this book. 

But just to tantalize the reader, we point out that under the more
general definitiion, @eq-polybasis is actually a basis for $C(0,1)$!
Again, there are mathematics niceties that must be filled in, but it
turns out that any continuous function on a finite interval can be
approximated by some polynomial.[For those with a background in
mathematical analysis, here is how the problem is handled. One starts
with a subspace that is *dense* in the full space, i.e.\ any vector in
the full space can be approximated to any precision by some linear
combination of vectors in the dense set. For instance, the famoua
Stone-Weierstrauss Theorem says that the set of all polynomials is dense
in $C(0,1)$. One then defines the dense set to be a "basis." The use
of trig functions as the dense set is much more common than use of
polynomials, in the familiar *Fourier series*.]{.column-margin}

$\cal RV(\Omega)$:

The situation here is the same as for C(0,1).

## Dimension

Geometrically, we often refer to what is called $\cal R^3$ here as
``3-dimensional.'' We extend this to general vector spaces as follows:

> The *dimension* of a vector space is the number of vectors in any
> of its bases.

There is a bit of a landmine in that definition, as it presumes that all
the bases do consist of the same number of vectors. This is true, but
must be proven. We will not do so here, and refer the interested reader
to the [elegant
proof](https://www.google.com/books/edition/Algebra_and_Geometry/tVTd78vYzPkC?hl=en&gbpv=1&pg=PA106&printsec=frontcover)
by AF Beardon (*Algebra and Geometry*, 2005, Cambridge).


## Linear Transformations

Consider vector spaces $\cal{V}_1$ and $\cal{V}_2$. A function $f$ whose
input is in $\cal{V}_1$ and output is in  $\cal{V}_2$ is called a
*transformation* (or *map*) from $\cal{V}_1$ to $\cal{V}_2$. 

An important special case is that in which  $f$ is linear, i.e.

$$
f(av + bw) = a f(v) + b f(w)
$$

for all scalars $a$ and $b$, and all vectors $v$ and $w$ in $\cal{V}_1$. 

A further important special case is that in which $\cal{V}_1$ and
$\cal{V}_2$ are $R^n$ and $R^m$, respectively.  Then it can be shown
that there a some $m \times n$ matrix $A$ that does the work of $f$, i.e.

$$
f(x) = Ax
$$

for all $x$ in $R^n$. 

Note that it is not necessarily true that a linear transformation will
be *one-to-one* (*injective*), meaning that $x \neq y$ implies 
$f(x) \neq f(y)$. Nor is it necessarily *onto* (*surjective*), i.e.
thst for any $w$ in $R^m$, there exists some $v$ such that
$f(v) = w$.

To see that the first is false, consider the matrix $A$ above. If say
$f(v) = w$ then we will also have $f(v+s) = f(v) + f(s) = w$ for any $s$
in the null space of $A$, a contradiction if $A$'s null space consists
of more than the 0 vector.  The reader is encouraged to verify that the
second property also does not necessarily hold either.

## Your Turn

❄️  **Your Turn:** In the vector space $C(0,1)$, consider the subset of
all polynomial functions of degree $k$ or less. Explain why this is a
subspace of $C(0,1)$. What is its dimension? Give an example of a basis
for this subspace, for $k = 3$.

❄️  **Your Turn:** Say $U$ and $V$ are subspaces of $W$. Explain why $U
\cap V$ is also a subspace, and that its dimension is at most the
minimum of the dimensions of $U$ and $V$. Show by counterexample that
the result does not hold for union.

❄️  **Your Turn:** Citing the properties of expected value, E(), show
that the set of all random variable with mean 0 is a
subspace of $\cal RV(\Omega)$.

❄️  **Your Turn:** Prove that the coefficients in a basis representation
are unique. In other words, in a representation of the vector $x$ in
terms of a basis $u_1,...,u_n$, there cannot be two different sets of
coefficients $c_1,...,c_n$ such that $c_1 u_1 + ... + c_n u_n = x$.
