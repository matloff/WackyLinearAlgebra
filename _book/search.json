[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Linear Algebra in Data Science",
    "section": "",
    "text": "Preface\nWelcome to my magnum opus! :-) I’ve written a number of books, but consider this one to be the most important. This work is licensed under Creative Commons Zero v1.0 Universal\n\nSubtlety in the Title\nLet’s start with the title of this book, “Linear Algebra in Data Science.” It would be more typical of “math support for field X” books to use “for” rather than “in,” but the use of the latter aims to emphasize the fact that:\n\nLinear algebra is absolutely fundamental to the Data Science field. For us data scientists, it is “our” branch of math. Mastering this branch, which is definitely within the reach of all, pays major dividends.\n\n\n\nPhilosophy\nThis is an unconventional linear algebra textbook, doing everything “backwards.” The presentation of each concept begins with a problem to be solved, almost always from Data Science, leading up to a linear algebra solution. Basically, the math sneaks up on the reader, who suddenly realizes they’ve just learned a new general concept!\n\n\nWho is this book for?\nOf course the book should work well as a course textbook. The “applications first” approach should motivate student, and the use of Quarto enables easy conversion to Powerpoint by instructors.\nI hope the book’s emphasize on the Why? and How? especially appeals to do-it-yourselfers, those who engagement in self-study is motivated by intellectual curiosity rather than a course grade.\n\n\nPrerequisite background\nBasic data science:\n\nCalculus.\nSome exposure to R is recommended, but the text can be read without it.\nBasics of random variables, expected value and variance.\n\n\n\nFor a quick, painless introduction to R, see my fasteR tutorial, say the first 8 lessons."
  },
  {
    "objectID": "Ch1.html",
    "href": "Ch1.html",
    "title": "1  Matrix Multiplication",
    "section": "",
    "text": "2 A Further Look at Markov Chains\n\\[\ng_j = P(X_1 = j)\n= \\sum_{i=1}^k P(X_0 = i) P(X_1 = j | X_0 = i)\n= \\sum_{i=1}^k f_i a_{ij}\n\\]\nwhere \\(a_{ij}\\) is the row i, column j element of the chain’s transition matrix \\(P\\).\nFor example, consider \\(g_5\\). How could we be at state 5 at time 1? We could start in state 1, probability \\(f_1\\), then move to state 5, probability \\(a_{15}\\), for a total probability of \\(f_1 a_{15}\\). Or, we could start in state 2, probability \\(f_2\\), then move to state 5, probability \\(a_{25}\\), for a total probability of \\(f_2 a_{25}\\). And so on.\nSo,\n\\[\ng_j = \\sum_{i=1}^k f_i a_{ij}\n\\]\nPutting this is more explicit matrix terms,\n\\[\ng = \\left (\n\\begin{array}{rrrr}\ng_1 \\\\\ng_2 \\\\\n... \\\\\ng_k \\\\\n\\end{array}\n\\right )\n=\n\\left (\n\\begin{array}{rr}\nf_1 a_{11} + f_2 a_{21} + ... + f_k a_{k1} \\\\\nf_1 a_{12} + f_2 a_{22} + ... + f_k a_{k2} \\\\\n... \\\\\nf_1 a_{1k} + f_2 a_{2k} + ... + f_k a_{kk} \\\\\n\\end{array}\n\\right )\n\\]\nThat last expression is\n\\[\nf'P\n\\]\n\\[\ng' = f'P\n\\]\nAnd setting h to the distribution of \\(X_2\\), the same reasoning gives us\n\\[\nh' = g'P\n\\]\n\\[\nh' = f'P^2\n\\]\nand so on. Iterating, we obtain\n\\[\nd_j' = d_0' P^j\n\\]\nwhere \\(d_i\\) is the distribution of \\(X_i\\).\nSimilarly,\n\\[\nd_j' = d_{j-1}' P\n\\]\nFor convenience, let’s take transposes:[Recall that \\((AB)' = B'A'\\).\n\\[\nd_j = P' d_{j-1}\n\\]\nNow suppose our chain as a long-run distribution, as in {Section 1.4}. Let’s call that distribution \\(\\nu\\). By lettting \\(j \\rightarrow \\infty\\) above, we have\n\\[\n\\nu = P' \\nu\n\\]\nSince P is known, this provides us with a way to compute \\(\\nu\\). Yes, finding a high power of P would do this too, but that would involve a lot of computation, and even then it would not yield the exact answer. We will return to this in the next chapter."
  },
  {
    "objectID": "Ch1.html#a-random-walk-model",
    "href": "Ch1.html#a-random-walk-model",
    "title": "1  Matrix Multiplication",
    "section": "1.1 A Random Walk Model",
    "text": "1.1 A Random Walk Model\nLet’s consider a random walk on {1,2,3,4,5} in the number line. Time is numbered 1,2,3,… Our current position is termed our state. The notation Xk = i means that at time k we are in state/position i.\nOur rule will be that at any time k, we flip a coin. If we are currently at position i, we move to either i+1 or i-1, depending on whether the coin landed heads or tails. The exceptions are k = 1 and k = 5, in which case we stay put if tails or move to the adjacent position if heads.\nWe can summarize the probabilities with a matrix, a two-dimensional array:\n\\[\nP_1 =\n\\left (\n\\begin{array}{rrrrr}\n0.5 & 0.5 & 0 & 0 & 0\\\\\n0.5 & 0 & 0.5 & 0 & 0\\\\\n0 & 0.5 & 0 & 0.5 & 0\\\\\n0 & 0 & 0.5 & 0 & 0.5\\\\\n0 & 0 & 0 & 0.5 & 0.5 \\\\\n\\end{array}\n\\right )\n\\]\nFor instance, look at Row 2. There are 0.5 values in Columns 1 and 3, meaning there is a 0.5 chance of a move 2 \\(\\rightarrow\\) 1, and a 0.5 chance of a move 2 \\(\\rightarrow\\) 3. Note that each row in a transition matrix must sum to 1. After, from state i we must go somewhere.\nWe use a subscript 1 here in \\(P_1\\), meaning “one step.” We go from, say, state 2 to state 1 in one step with probability 0.5. \\(P_1\\) is called the one-step transition matrix (or simply the transition matrix) for this process.\nWhat about the two-step transition matrix \\(P_2\\)? From state 3, we could go to state 1 in two steps, by two tails flips of the coin. The probability of that is \\(0.5^2 = 0.25\\). So the row 3, column 1 element in \\(P_2\\) is 0.25. On the other hand, if from state 3 we flip tails then heads, or heads then tails, we are back to state 3. So, the row 3, column 3 element in \\(P_2\\) is 0.25 + 0.25 = 0.5.\nThe reader should verify the correctness here:\n\\[\nP_2 =\n\\left (\n\\begin{array}{rrrrr}\n0.5 & 0.25 & 0.25 & 0 & 0\\\\\n0.25 & 0.5 & 0 & 0.25 & 0\\\\\n0.25 & 0 & 0.5 & 0 & 0.25\\\\\n0 & 0.25 & 0 & 0.5 & 0.25\\\\\n0 & 0 & 0.25 & 0.25 & 0.5 \\\\\n\\end{array}\n\\right )\n\\]\nWell, finding two-step transition probabilities would be tedious in general, but it turns out that is a wonderful shortcut: Matrix multiplication. We will cover this in the next section, but first a couple of preliminaries.\nThe above random walk is a Markov chain. The Markov Property says that the system “has no jemory.” If say we land at position 2, we will go to 1 or 3 with probability 1/2 no matter what the previous history of the system was; it doesn’t matter how we got to state 3. That in turn comes from the independence of the successive coin flips.\nNotation: Individual elements of a matrix are usually written with double subscripts. For instance, a25 will mean the row 2, column 5 element of the matrix A."
  },
  {
    "objectID": "Ch1.html#matrix-multiplication",
    "href": "Ch1.html#matrix-multiplication",
    "title": "1  Matrix Multiplication",
    "section": "1.2 Matrix Multiplication",
    "text": "1.2 Matrix Multiplication\nThis is the most fundamental operation in linear algebra. It is defined as follows:\n\nGiven matrix A of k rows and m columns and matrix B of m rows and r columns, the product C = AB is an kxm matrix, whose row i, column j element is\n\\[\na_{i1} b_{i1} +\na_{i2} b_{i1} + ... +\na_{m1} b_{m1}\n\\]\nThis is the “dot product” of row i of A and column j of B: Find the products of the paired elements in the two vectors, then sum.\n\nBe sure to remember that the number of rows of B must match the number of columns of A. In this case, we say thst the matrices are conformable.\nFor example, set\n\\[\nA = \\left (\n\\begin{array}{rr}\n5 & 2 & 6 \\\\\n1 & 1 & 8 \\\\\n\\end{array}\n\\right )\n\\]\nand\n\\[\nB = \\left (\n\\begin{array}{rrr}\n5 & -1 \\\\\n1 & 0 \\\\\n0 & 8 \\\\\n\\end{array}\n\\right )\n\\]\nLet’s find the row 2, column 2 element of C = AB. Again, that means taking the dot product of row 2 of A and column 2 of B, which we’ve highlighted below.\n\\[\nA = \\left (\n\\begin{array}{rr}\n5 & 2 & 6 \\\\\n\\color{red}{1} & \\color{red}{1} & \\color{red}{1} \\\\\n\\end{array}\n\\right )\n\\]\nand\n\\[\nB = \\left (\n\\begin{array}{rrr}\n5 & \\color{red}{-1} \\\\\n1 & \\color{red}{0} \\\\\n0 & \\color{red}{8} \\\\\n\\end{array}\n\\right )\n\\]\nThe value in question is then\n1 (-1) + 1 (0) + 1 (8) = 7\nLet’s check it, with R:\n.The rbind and cbind functions (“row bind” and “column bind”) are very handy tools for creating matrices.\n\na &lt;- rbind(c(5,2,6),c(1,1,1))\nb &lt;- cbind(c(5,1,0),c(-1,0,8))\na %*% b\n\n     [,1] [,2]\n[1,]   27   43\n[2,]    6    7\n\n\nThe reader should make sure to check the other elements by hand."
  },
  {
    "objectID": "Ch1.html#vectors",
    "href": "Ch1.html#vectors",
    "title": "1  Matrix Multiplication",
    "section": "1.3 Vectors",
    "text": "1.3 Vectors\nA matrix that has only one row or only one column is called a vector. Depending on which of those two shapes it has, we may refer to it as a row vector or column vector. Usually we will simply say “vector,” in which case it will be meant as a column vector."
  },
  {
    "objectID": "Ch1.html#sec-introMCs",
    "href": "Ch1.html#sec-introMCs",
    "title": "1  Matrix Multiplication",
    "section": "1.4 Application to Markov Chain Transition Matrices",
    "text": "1.4 Application to Markov Chain Transition Matrices\nNow let’s return to the question of how to easily compute \\(P_2\\), the two-step transition matrix. It turns out that:\n\nLet P denote the transition matrix of a (finite-state) Markov chain. The k-step transition matrix is \\(P^k\\).\n\nAt first, this may seem amazingly fortuitous, but it makes sense in light of the “and/or” nature of the probability computations involved. Recall our computation for the row 1, column 2 element of \\(P_2\\) above. From state 1, we could either stay at 1 for one flip, then move to 2 on the second flip, or we could go to 2 then return to 1. Each of these has probability 0.5, so the total probability is\n\\[\n(0.5)(0.5) + (0.5)(0.5)\n\\]\nBut this is exactly the form of our “dot product” computation in the definition of matrix multiplication,\n\\[\na_{i1} b_{i1} +\na_{i2} b_{i1} + ... +\na_{m1} b_{m1}\n\\]\nStatisticians and computer scientists like to look at the asymptotic behavior of systems. Let’s see where we might be after say, 6 steps:The identity matrix I of size n is the nxn matrix with 1s on the diagonal and 0s elsewhere. IB = B and AI = A for any conformable A and B.\n\nmatpow &lt;- function(m,k) {\n   nr &lt;- nrow(m)\n   tmp &lt;- diag(nr)  # identity matrix\n   for (i in 1:k) tmp &lt;- tmp %*% m\n   tmp\n}\n\np1 &lt;- rbind(c(0.5,0.5,0,0,0), c(0.5,0,0.5,0,0), c(0,0.5,0,0.5,0), \n   c(0,0,0.5,0,0.5), c(0,0,0,0.5,0.5))\nmatpow(t(p1),6) # R uses t() for transpose\n\n         [,1]     [,2]     [,3]     [,4]     [,5]\n[1,] 0.312500 0.234375 0.234375 0.109375 0.109375\n[2,] 0.234375 0.312500 0.109375 0.234375 0.109375\n[3,] 0.234375 0.109375 0.312500 0.109375 0.234375\n[4,] 0.109375 0.234375 0.109375 0.312500 0.234375\n[5,] 0.109375 0.109375 0.234375 0.234375 0.312500\n\n\nSo for instance if we start at position 2, there is about an 11% chance that we will be at position 3 at time 6. What about time 25?\n\nmatpow(t(p1),25)\n\n          [,1]      [,2]      [,3]      [,4]      [,5]\n[1,] 0.2016179 0.2016179 0.1993820 0.1993820 0.1980001\n[2,] 0.2016179 0.1993820 0.2016179 0.1980001 0.1993820\n[3,] 0.1993820 0.2016179 0.1980001 0.2016179 0.1993820\n[4,] 0.1993820 0.1980001 0.2016179 0.1993820 0.2016179\n[5,] 0.1980001 0.1993820 0.1993820 0.2016179 0.2016179\n\n\nSo, no matter which state we start in, at time 25 we are about 20% likely to be at any of the states. In fact, as time n goes to infinity, this probability vector becomes exactly (0.20,0.20,0.20,0.20,0.20), as we will see in the next chapter..\n❄️ Your Turn: The long-run probabilities here turned out to be uniform, with value 0.20 for all five states. In fact, that is usually not the case. Make a small change to \\(P_1\\) – remember to keep the row sums to 1 – and compute a high power to check whether the long-run distribution seems nonuniform.\n❄️ Your Turn: Not every Markov chain, even ones with finitely many states, have long-run distributions. Some chains have periodic states. It may be, for instance, that after leaving state \\(i\\), once can return only after an even number of hops. Modify our example chain here so that states 1 and 5 (and all the others) have that property. Then compute \\(P^n\\) for various large values of \\(n\\) and observe oscillatory behavior, rather than long-run convergence.\n❄️ Your Turn: Consider the following model of a discrete-time, single-server queue:\n\nModel parameters are p (probability of job completion), q (probability of new job arriving) and m (size of the buffer).\nJobs arrive, are served (possibly after queuing) and leave.\nOnly one job can be in service at a time.\nAt each time epoch:\n\nThe job currently in service, if any, will complete with probability p.\nSlightly after a possible job completion, a job in the queue, if any, will start service.\n\na Slightly after that, anew job will arrive with probability q. If the queue is empty, this job starts service. If not, and if the queue is not full, it will join the queue. Otherwise, the job is discarded.\nThe system is memoryless.\nThe current state is the number of jobs in the system, taking on the values 0,1,2,..,m+1; that last state means m jobs in the queue and 1 in service.\n\nFor instance, say p = 0.4, q = 0.2, m = 5, Suppose the current state is 3, so there is a job in service and two jobs in the queue. Our next state will be 2 with probability (0.4) (0.8); it will be 3 with probability (0.4) (0.2), and so on.\nAnalyze this system for the case given above.` Find the approximate long-run distribution, and also the proportion of jobs that get discarded."
  },
  {
    "objectID": "Ch1.html#network-graph-models",
    "href": "Ch1.html#network-graph-models",
    "title": "1  Matrix Multiplication",
    "section": "1.5 Network Graph Models",
    "text": "1.5 Network Graph Models\nThere has always been lots of analyses of “Who is connected to who,” but activity soared after the advent of Facebook and the film, A Social Network. See for instance Statistical Analysis of Network Data with R by Eric Kolaczy and Gábor Csárdi. As the authors say,\n\nThe oft-repeated statement that “we live in a connected world” perhaps best captures, in its simplicity…From on-line social networks like Facebook to the World Wide Web and the Internet itself, we are surrounded by examples of ways in which we interact with each other. Similarly, we are connected as well at the level of various human institutions (e.g., governments), processes (e.g., economies), and infrastructures (e.g., the global airline network). And, of course, humans are surely not unique in being members of various complex, inter-connected systems. Looking at the natural world around us, we see a wealth of examples of such systems, from entire eco-systems, to biological food webs, to collections of inter-acting genes or communicating neurons.\n\nAnd of course, at the center of it all is a matrix! Here is why:\nLet’s consider the famous Karate Club dataset:\n\n\n# remotes::install_github(\"schochastics/networkdata\") \nlibrary(networkdata)\ndata(karate)\nlibrary(igraph)\nplot(karate)\n\n\n\n\nThere is a link between node 13 and node 4, meaning that club members 13 and 4 are friends.This graph is undirected, as friendship is mutual. Many graphs are directed, but we will assume undirected here.\nSpecifically, the adjacency matrix has row i, column j element as 1 or 0, according to whether a link exists between nodes i and j.\n\nadjK &lt;- as_adjacency_matrix(karate)\nadjK\n\n34 x 34 sparse Matrix of class \"dgCMatrix\"\n                                                                         \n [1,] . 1 1 1 1 1 1 1 1 . 1 1 1 1 . . . 1 . 1 . 1 . . . . . . . . . 1 . .\n [2,] 1 . 1 1 . . . 1 . . . . . 1 . . . 1 . 1 . 1 . . . . . . . . 1 . . .\n [3,] 1 1 . 1 . . . 1 1 1 . . . 1 . . . . . . . . . . . . . 1 1 . . . 1 .\n [4,] 1 1 1 . . . . 1 . . . . 1 1 . . . . . . . . . . . . . . . . . . . .\n [5,] 1 . . . . . 1 . . . 1 . . . . . . . . . . . . . . . . . . . . . . .\n [6,] 1 . . . . . 1 . . . 1 . . . . . 1 . . . . . . . . . . . . . . . . .\n [7,] 1 . . . 1 1 . . . . . . . . . . 1 . . . . . . . . . . . . . . . . .\n [8,] 1 1 1 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n [9,] 1 . 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 . 1 1\n[10,] . . 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\n[11,] 1 . . . 1 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n[12,] 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n[13,] 1 . . 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n[14,] 1 1 1 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\n[15,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 1\n[16,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 1\n[17,] . . . . . 1 1 . . . . . . . . . . . . . . . . . . . . . . . . . . .\n[18,] 1 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n[19,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 1\n[20,] 1 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\n[21,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 1\n[22,] 1 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n[23,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 1\n[24,] . . . . . . . . . . . . . . . . . . . . . . . . . 1 . 1 . 1 . . 1 1\n[25,] . . . . . . . . . . . . . . . . . . . . . . . . . 1 . 1 . . . 1 . .\n[26,] . . . . . . . . . . . . . . . . . . . . . . . 1 1 . . . . . . 1 . .\n[27,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 . . . 1\n[28,] . . 1 . . . . . . . . . . . . . . . . . . . . 1 1 . . . . . . . . 1\n[29,] . . 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 . 1\n[30,] . . . . . . . . . . . . . . . . . . . . . . . 1 . . 1 . . . . . 1 1\n[31,] . 1 . . . . . . 1 . . . . . . . . . . . . . . . . . . . . . . . 1 1\n[32,] 1 . . . . . . . . . . . . . . . . . . . . . . . 1 1 . . 1 . . . 1 1\n[33,] . . 1 . . . . . 1 . . . . . 1 1 . . 1 . 1 . 1 1 . . . . . 1 1 1 . 1\n[34,] . . . . . . . . 1 1 . . . 1 1 1 . . 1 1 1 . 1 1 . . 1 1 1 1 1 1 1 .\n\nadjK[13,4]\n\n[1] 1\n\n\nAccordingly, row 13, column 4 does have a 1 entry.\nAs is the case with Markov transition matrices, powers of an adjacency matrix can yield valuable information. In the Markov case, multiplication gives us sums of paired products, computing probabilities. What about the network graph case?\nHere products are of the form 0x0, 0x1, 1x0 or 1x1. If there is a nonzero entry m in row i, column j of the square of the adjacency matrix, that means there were m 1x1 products in that sum, which would correspond to m paths. Let’s look into this.\n\nadjK2 &lt;- adjK %*% adjK\n\nWe see that adjK2[11,1] is 2. Inspection of adjK shows that its row 11, columns 6 and 7 are 1s, and that rows 6 and 7, column 1 are 1s as well. So there are indeed two two-hop paths from node 11 to node 1, specifically \\(11 \\rightarrow 6 \\rightarrow 1\\) and $ \\(11 \\rightarrow 7 \\rightarrow 1\\).\nActually, what is typically of interest is connectivity rather than number of paths. For any given pair of nodes, is there a multihop path between them? Or does the graph break down to several “islands” of connected nodes?\nAgain consider the karate club data.\n\nu &lt;- matpow(adjK,33)\nsum(u == 0)\n\n[1] 0\n\n\nSo, in this graph, no pair of nodes has 0 paths between them. The graph is connected.\nMaking this kind of analysis fully correct requires paying attention to things such as cycles. The details are beyond the scope of this book."
  },
  {
    "objectID": "Ch1.html#matrix-algebra",
    "href": "Ch1.html#matrix-algebra",
    "title": "1  Matrix Multiplication",
    "section": "1.6 Matrix Algebra",
    "text": "1.6 Matrix Algebra\n\n1.6.1 Other Basic operations\nMatrix multiplication may seem odd at first, but other operations are straightforward.\nAddition: We just add corresponding elements. For instance,\n\\[\nA = \\left (\n\\begin{array}{rr}\n5 & 2 & 6 \\\\\n1 & 2.6 & -1.2 \\\\\n\\end{array}\n\\right )\n\\]\n\\[\nB = \\left (\n\\begin{array}{rr}\n0 & 20 & 6 \\\\\n3 & 5.8 & 1 \\\\\n\\end{array}\n\\right )\n\\]\n\\[\nA+B = \\left (\n\\begin{array}{rr}\n5 & 22 & 12 \\\\\n4 & 8.4 & -0.2 \\\\\n\\end{array}\n\\right )\n\\]\nWe do have to make sure the addends match in terms of numbers of rows and columns, 2 and 3 in the example here.\nScalar multiplication: Again, this is simply elementwise. E.g. with A as above,\n\\[\n1.5 A = \\left (\n\\begin{array}{rr}\n7.5 & 3 & 9 \\\\\n1.5 & 3.9 & -1.8 \\\\\n\\end{array}\n\\right )\n\\]\nDistributive property:\nFor matrices A, B and C of suitable conformability (A and B match in numbers of rows and columns, and their common number of columns matches the number of rows in C), we have\n(A+B) C = AC + BC\n\n\n1.6.2 Matrix transpose\nThis is a very simple but very important operation: We merely exchange rows and columns of the given matrix. For instance, with A as above, its transpose (signified with “’”), is\n\\[\nA' = \\left (\n\\begin{array}{rr}\n5 & 1 \\\\\n2 & 2.6 \\\\\n6 & -1.2 \\\\\n\\end{array}\n\\right )\n\\]\nIt can be shown that if A and B are conformable, then\n(AB)’ = B’A’\nFor some matrices C, we have C’ = C. C is then termed symmetric.\nWe will often write a row vector in the form (a,b,c,…). So (5,1,88) means the 1x3 matrix with those elements. If we wish this to be a column vector, we use transpose, so that for instance (5,1,88)’ means a 3x1 matrix.\n\n\n1.6.3 Partitioned matrices: an invaluable visualization tool\nHere, “visualization” is not a reference to graphics but rather to highlighting certain submatrices.\nConsider a matrix-vector product Mv. Of course, that means that v is a column vector whose length is equal to the number of columns of M. If M is of size rxs, then v is sx1.\nLet’s denote column j of M by Cj. Then we will see later in this chapter that\n\\[\nMv =\nv_1 C_{1} +\nv_2 C_{2} + ... +\nv_s C_{s}  \n\\]\nFor instance, take\n\\[\nA = \\left (\n\\begin{array}{rr}\n5 & 2 & 6 \\\\\n1 & 2.6 & -1.2 \\\\\n\\end{array}\n\\right )\n\\]\nand\n\\[\nv = \\left (\n\\begin{array}{rrr}\n10 \\\\\n2 \\\\\n1 \\\\\n\\end{array}\n\\right )\n\\]\nThe reader should check that\n\\[\n10 \\left (\n\\begin{array}{rr}\n5 \\\\\n1 \\\\\n\\end{array}\n\\right )\n+\n2 \\left (\n\\begin{array}{rr}\n2 \\\\\n2.6 \\\\\n\\end{array}\n\\right )\n+\n1 \\left (\n\\begin{array}{rr}\n6 \\\\\n-1.2 \\\\\n\\end{array}\n\\right )\n= Av\n\\]\nwhere the latter is\n\\[\n\\left (\n\\begin{array}{rr}\n60 \\\\\n14  \\\\\n\\end{array}\n\\right )\n\\]\nNote that the above expression,\n\\[\n10 \\left (\n\\begin{array}{rr}\n5 \\\\\n1 \\\\\n\\end{array}\n\\right )\n+\n2 \\left (\n\\begin{array}{rr}\n2 \\\\\n2.6 \\\\\n\\end{array}\n\\right )\n+\n1 \\left (\n\\begin{array}{rr}\n6 \\\\\n-1.2 \\\\\n\\end{array}\n\\right ),\n\\]\nis a sum of scalar products of vectors, which is called a linear combination of those vectors. In other words, we have that:\n\nThe product of a matrix and a column vector is equal to a linear combination of the columns of the matrix.\n\nSimilarly,\n\nThe product of a row vector and a matrix is equal to a linear combination of the rows of the matrix.\n\n❄️ Your Turn: Using the same matrix A and the same vector v as above, verify that v’A is indeed a linear combination of the rows of A as described.\nTo further illustrate partitioned matrices, write the above matrix \\(A\\) as\n\\[\nA =\n\\left (\n\\begin{array}{r}\nA_{11} & A_{21} \\\\\n\\end{array}\n\\right )\n\\]\nwhere\n\\[\nA_{11} =\n\\left (\n\\begin{array}{rr}\n5 & 2 \\\\\n1 & 2.6 \\\\\n\\end{array}\n\\right )\n\\]\nand\n\\[\nA_{12} =\n\\left (\n\\begin{array}{rr}\n6 \\\\\n-1.2 \\\\\n\\end{array}\n\\right )\n\\]\nSymbolically, \\(A\\) now looks like a 1x2 matrix. Similarly, writing\n\\[\nv =\n\\left (\n\\begin{array}{rrr}\nv_{11} \\\\\nv_{21} \\\\\n\\end{array}\n\\right )\n\\]\nwhere\n\\[\nv_{11} =\n\\left (\n\\begin{array}{rr}\n10 \\\\\n2 \\\\\n\\end{array}\n\\right )\n\\]\nand \\(v_{21} = 1\\) (a 1x1 matrix), \\(v\\) looks to be 2x1.\nSo, again pretending, treat the product \\(Av\\) as the multiplication of a 1x2 “matrix” and 2x1 “vector”, yielding a 1x1 result,\n\\[\nA_{11} v_{11} + A_{12} v_{21}\n\\]\nBut all that pretending actually does give the correct answer!\n\\[\nA_{11} v_{11} + A_{12} v_{21} =\n\\left (\n\\begin{array}{rr}\n5 & 2 \\\\\n1 & 2.6 \\\\\n\\end{array}\n\\right )\n\\left (\n\\begin{array}{rr}\n10 \\\\\n2 \\\\\n\\end{array}\n\\right )\n+\n\\left (\n\\begin{array}{rr}\n6 \\\\\n-1.2 \\\\\n\\end{array}\n\\right )\n1\n=\n\\left (\n\\begin{array}{rr}\n60 \\\\\n14 \\\\\n\\end{array}\n\\right )\n\\]"
  },
  {
    "objectID": "Ch2.html#example",
    "href": "Ch2.html#example",
    "title": "2  Matrix Inverse",
    "section": "2.1 Example",
    "text": "2.1 Example\nAt the end of the last chapter, we found that for a Markov chain with transition matrix P and stationary distribution \\(\\nu\\),\n\\[\n\\nu = P' \\nu\n\\]\nThis suggests a method for computing \\(\\nu\\), by solving the above equation.\nRewrite it using the identity matrix:Recall that for any square matrix C and identity matrix I of the same size, \\(IC = CI = C\\).\n\\[\n(I - P') \\nu = 0\n\\]\nFor the random walk chain in Chapter 1, we hadIn that particular model, P’ = P, but for most chains this is not the case.\n\\[\nP =\n\\left (\n\\begin{array}{rrrrr}\n0.5 & 0.5 & 0 & 0 & 0\\\\\n0.5 & 0 & 0.5 & 0 & 0\\\\\n0 & 0.5 & 0 & 0.5 & 0\\\\\n0 & 0 & 0.5 & 0 & 0.5\\\\\n0 & 0 & 0 & 0.5 & 0.5 \\\\\n\\end{array}\n\\right )\n\\]\nWith \\(\\nu = (\\nu_1,\\nu_2,\\nu_3,\\nu_4,\\nu_5)'\\), the equation to be solved, \\((I-P') \\nu = \\nu\\), is\n\\[\n\\left (\n\\begin{array}{rrrrr}\n0.5 & -0.5 & 0 & 0 & 0 \\\\\n-0.5 & 1 & -0.5 & 0 & 0 \\\\\n0 & -0.5 & 1 & -0.5 & 0 \\\\\n0 & 0 & -0.5 & 1 & -0.5 \\\\\n0 & 0 & 0 & -0.5 & 0.5 \\\\\n\\end{array}\n\\right )\n\\left (\n\\begin{array}{rrrrr}\n\\nu_1 \\\\\n\\nu_2 \\\\\n\\nu_3 \\\\\n\\nu_4 \\\\\n\\nu_5 \\\\\n\\end{array}\n\\right ) =\n\\left (\n\\begin{array}{rrrrr}\n0 \\\\\n0 \\\\\n0 \\\\\n0 \\\\\n0 \\\\\n\\end{array}\n\\right )\n\\]\nIf we perform the matrix multiplication, we have an ordinary system of linear equations:\n\\[\n\\begin{array}{rrrrr}\n0.5 \\nu_1 - 0.5 \\nu_2 = 0 \\\\\n-0.5 \\nu_1 + \\nu_2 - 0.5 \\nu_3 = 0 \\\\\n-0.5 \\nu_2 + \\nu_3 - 0.5 \\nu_4 = 0 \\\\\n-0.5 \\nu_3 + \\nu_4 - 0.5 \\nu_5 = 0 \\\\\n-0.5 \\nu_4 + 0.5 \\nu_5 = 0 \\\\\n\\end{array}\n\\]\nThis is high school math, and we could solve the equations that way. But this is literaally what linear algebra was invented for, solving systems of equations! We will use matrix inverse.\nBut first, we have a problem to solve: The only solution to the above system is with all \\(\\nu_i = 0\\). We need an equation involving a nonzero quantity.\nBut we do have such an equation. The \\(\\nu\\) is a stationary distribution for a Markov chain, and thus must sum to 1.0. Let’s replace the last row by that relation:\n\\[\n\\left (\n\\begin{array}{rrrrr}\n0.5 & -0.5 & 0 & 0 & 0 \\\\\n-0.5 & 1 & -0.5 & 0 & 0 \\\\\n0 & -0.5 & 1 & -0.5 & 0 \\\\\n0 & 0 & -0.5 & 1 & -0.5 \\\\\n1 & 1 & 1 & 1 & 1 \\\\\n\\end{array}\n\\right )\n\\left (\n\\begin{array}{rrrrr}\n\\nu_1 \\\\\n\\nu_2 \\\\\n\\nu_3 \\\\\n\\nu_4 \\\\\n\\nu_5 \\\\\n\\end{array}\n\\right ) =\n\\left (\n\\begin{array}{rrrrr}\n0 \\\\\n0 \\\\\n0 \\\\\n0 \\\\\n1 \\\\\n\\end{array}\n\\right )\n\\]\nMore compactly,\n\\[\nB \\nu = d\n\\]\nMany square matrices \\(A\\) have a multiplicative inverse, denoted by \\(A^{-1}\\), with the property that\n\\[\nA^{-1} A = A A^{-1} = I\n\\]\nIf our matrix \\(B\\) is invertible, we can premultiply both sides of our equation above, yielding\n\\[\nB^{-1} d = B^{-1} B \\nu = \\nu\n\\]\nSo, we have obtained our solution for the stationary distribution. We can evaluate it numerically via the R solve function, which finds matrix inverse:\n\nB &lt;-\nrbind(c(0.5,-0.5,0,0,0), c(-0.5,1,-0.5,0,0), c(0,-0.5,1,-0.5,0),\n   c(0,0,-0.5,1,-0.5), c(1,1,1,1,1))\nB\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]  0.5 -0.5  0.0  0.0  0.0\n[2,] -0.5  1.0 -0.5  0.0  0.0\n[3,]  0.0 -0.5  1.0 -0.5  0.0\n[4,]  0.0  0.0 -0.5  1.0 -0.5\n[5,]  1.0  1.0  1.0  1.0  1.0\n\nBinv &lt;- solve(B)\n# check the inverse\nBinv %*% B  # yes, get I (of course with some roundoff error)\n\n              [,1]         [,2]          [,3]          [,4]          [,5]\n[1,]  1.000000e+00 1.942890e-16 -1.387779e-16  1.942890e-16  8.326673e-17\n[2,]  1.942890e-16 1.000000e+00  3.053113e-16 -2.775558e-17  8.326673e-17\n[3,] -2.775558e-17 0.000000e+00  1.000000e+00  0.000000e+00  2.775558e-17\n[4,] -1.665335e-16 4.996004e-16 -3.608225e-16  1.000000e+00 -2.775558e-17\n[5,] -1.665335e-16 7.216450e-16 -4.996004e-16  5.551115e-17  1.000000e+00\n\nnu &lt;- Binv %*% c(0,0,0,0,1)\nnu\n\n     [,1]\n[1,]  0.2\n[2,]  0.2\n[3,]  0.2\n[4,]  0.2\n[5,]  0.2\n\n\nThis confirms our earlier speculation based on powers of \\(P'\\)."
  },
  {
    "objectID": "Ch2.html#matrix-algebra",
    "href": "Ch2.html#matrix-algebra",
    "title": "2  Matrix Inverse",
    "section": "2.2 Matrix Algebra",
    "text": "2.2 Matrix Algebra\nIf the inverses of \\(A\\) and \\(B\\) exist, and \\(A\\) and \\(B\\) are conformable, then \\((AB)^{-1}\\) exists and is equal to \\(B^{-1} A^{-1}\\).\nAlso, \\((A')^{-1}\\) exists and is equal to \\((A^{-1})'\\).\n❄️ Your Turn: Prove these assertions."
  },
  {
    "objectID": "Ch2.html#computation-of-the-matrix-inverse",
    "href": "Ch2.html#computation-of-the-matrix-inverse",
    "title": "2  Matrix Inverse",
    "section": "2.3 Computation of the Matrix Inverse",
    "text": "2.3 Computation of the Matrix Inverse\nFinding the inverse of a large matrix – in data science applications, the number of rows and columns \\(n\\) can easily be hundreds or more – can be computationally challenging. The run time is proportional to \\(n^3\\), and roundoff error can be an issue. Sophisticated algorithms have been developed, such as QR and Choleski decompositions. So in R, we should use, say, qr.solve rather than solve if we are working with sizable matrices.\nThe classic “pencil and paper” method for matrix inversion is instructive, and will be presented here.\n\n2.3.1 Pencil-and-paper computation\nThe basic idea follows the pattern the reader learned for solving systems of linear equations, but with the added twist of involving some matrix multiplication.\nLet’s take as our example\n\\[\nA =\n\\left (\n\\begin{array}{rr}\n4 & 7 \\\\\n-8 & 15  \\\\\n\\end{array}\n\\right )\n\\]\nWe aim to transform this to the 2x2 identity matrix, via a sequence of row operations.\n\n\n2.3.2 Use of elementary matrices\nLet’s multiply row 1 by 1/4, to put 1 in the first element:\n\\[\n\\left (\n\\begin{array}{rr}\n1 & \\frac{7}{4} \\\\\n-8 & 15  \\\\\n\\end{array}\n\\right )\n\\]\nIn matrix terms, that operation is equivalent to premultiplying \\(A\\) by\n\\[\nE_1=\n\\left (\n\\begin{array}{rr}\n\\frac{1}{4} & 0  \\\\\n0 & 1  \\\\\n\\end{array}\n\\right )\n\\]\nWe then add 8 times row 1 to row 2, yielding\n\\[\n\\left (\n\\begin{array}{rr}\n1 & \\frac{7}{4} \\\\\n0 & 29  \\\\\n\\end{array}\n\\right )\n\\]\nThe premultiplier for this operation is\n\\[\nE_2=\n\\left (\n\\begin{array}{rr}\n1 & 0  \\\\\n8 & 1  \\\\\n\\end{array}\n\\right )\n\\]\nMultiply row 2 by 1/29:\n\\[\n\\left (\n\\begin{array}{rr}\n1 & \\frac{7}{4} \\\\\n0 & 1  \\\\\n\\end{array}\n\\right )\n\\]\ncorresponding to\n\\[\nE_3=\n\\left (\n\\begin{array}{rr}\n1 & 0  \\\\\n8 & 1/29  \\\\\n\\end{array}\n\\right )\n\\]\nAnd finally, add -7/4 row 2 to row 1.\n\\[\n\\left (\n\\begin{array}{rr}\n1 & 0 \\\\\n0 & 1  \\\\\n\\end{array}\n\\right )\n\\]\nThe premultiplier is\n\\[\nE_4 =\n\\left (\n\\begin{array}{rr}\n1 & \\frac{7}{4} \\\\\n0 & 1  \\\\\n\\end{array}\n\\right )\n\\]\nNow, how does that give use \\(A^{-1}\\)? The method your were taught probably set up the partioned matrix \\((A,I)\\). The row operations that transformed \\(A\\) to \\(I\\) also transformed \\(I\\) to \\(A^{-1}\\). Here’s why:\nAs noted, the row operations are such that\n\\[\nE_4 E_3 E_2 E_1 A\n\\]\ngive us the final transformed result, i.e. \\(I\\):\n\\[\nE_4 E_3 E_2 E_1 A = I\n\\]\nThenRecall that the inverse of a product (if it exists is the reverse product of the inverses.\n\\[\nA = (E_4 E_3 E_2 E_1)^{-1} I = E_4^{-1} E_3^{-1} E_2^{-1} E_1^{-1}\n\\]\nSo this accomplishes our goal of computing \\(A^{-1}\\), provided we have the inverses of the elementary matrices \\(E_i\\), But those are easy, as each simply “undoes” its partner. \\(E_1\\), for instance, multiplies the row 1, column 1 element by 1/4, which is “undone” by multiplying that element by 4,\n\\[\nE_1^{-1} =\n\\left (\n\\begin{array}{rr}\n4 & 0 \\\\\n0 & 1  \\\\\n\\end{array}\n\\right )\n\\]\nAlso, to undo the operation of adding 8 times row 1 to row 2, we add -8 times row 1 to row 2:\n\\[\nE_2^{-1} =\n\\left (\n\\begin{array}{rr}\n1 & 0 \\\\\n-8 & 1 \\\\\n\\end{array}\n\\right )\n\\]\n❄️ Your Turn: Find the inverses of \\(E_3\\) and \\(E_4\\) using similar reasoning, and thus find \\(A^{-1}\\)."
  },
  {
    "objectID": "Ch2.html#nonexistent-inverse",
    "href": "Ch2.html#nonexistent-inverse",
    "title": "2  Matrix Inverse",
    "section": "2.4 Nonexistent inverse",
    "text": "2.4 Nonexistent inverse\nSuppose our matrix \\(A\\) had been slightly different:\n\\[\nA =\n\\left (\n\\begin{array}{rr}\n4 & 7 \\\\\n-8 & -14  \\\\\n\\end{array}\n\\right )\n\\]\nThis would have led to\n\\[\n\\left (\n\\begin{array}{rr}\n1 & \\frac{7}{4} \\\\\n0 & 0  \\\\\n\\end{array}\n\\right )\n\\]\nThis cannot lead to \\(i\\), indicating that \\(A^{-1}\\) does not exist, and the matrix is said to be singular. And it’s no coincidence that row 2 of \\(A\\) is double row 1. This has many implications, as will be seen in our chapter on vector spaces."
  },
  {
    "objectID": "Ch3.html#linear-regression-through-the-origin",
    "href": "Ch3.html#linear-regression-through-the-origin",
    "title": "3  Linear Statistical Models",
    "section": "3.1 Linear Regression through the Origin",
    "text": "3.1 Linear Regression through the Origin\nLet’s consider the Nile dataset built-in to R. It is a time series, one measurement per year.\n\nhead(Nile)\n\n[1] 1120 1160  963 1210 1160 1160\n\n# predict current year from previous year?\nn1 &lt;- Nile[-(length(Nile))]\nhead(n1)\n\n[1] 1120 1160  963 1210 1160 1160\n\nn2 &lt;- Nile[-1]\nhead(n2)\n\n[1] 1160  963 1210 1160 1160  813\n\nplot(n1,n2,cex=0.4,xlim=c(0,1400),yli=c(0,1400))\n\n\n\n\nWe would like to fit a straight line through that data point cloud. We might have two motivations for doing this:\n\nThe line might serve as nice summary of the data.\nMore formally, let \\(C\\) and \\(V\\) denote the current and previous year’s measurements..Then the model\n\n\\[E(C | V) = \\beta V\\]\nmay be useful. Here the slope \\(\\beta\\) is an unknown value to be estimated from the data. Readers with some background in linear regression models should note that this assumption of a linear trend through the origin is the only assumption we are making here. Nothing on normal distributions etc.\n\n\n\n\n\n\nModel validity\n\n\n\nThe great statistician George Box once said, “All models are wrong but some are useful.” All data scientists should keep this at the forefronts of their minds.\n\n\n\n3.1.1 Least squares approach\n Let \\(b\\) denote our estimate. How should we obtain it? We wish to estimate \\(\\beta\\) from our data, which we regard as a sample from the data generating process. Denote our data by \\((C_i,V_i), i = 1,...,100\\).This too is something all data scientists should keep at the forefronts of their minds. We are always working with sample data, subject to intersample variations. The quantity \\(b\\) is a random variable.\nPretend for a moment that we don’t know, say, \\(C_{28}\\). Using our estimated \\(\\beta\\), our predicted value would be \\(b V_{28}\\). Our squared prediction error would then be \\((C_{28} - b W_{28})^2\\).\nWell, we actually do know \\(C_{28}\\) (and the others in our data), so we can answer the question:\n\nIn our search for a good value of \\(b\\), we can ask how well we would predict our known data, using that candidate value of \\(b\\) in our data. Our total squared prediction error would be\n\\[\n\\sum_{i=1}^{100} [C_{i} - b V_{i} )^2\n\\]\nA natural choice for \\(b\\) would be the value that minimizes this quantity. Why not look at the absolute value instead of the square? The latter makes the math flow well, as will be seen shortly.\n\n\n\n3.1.2 Calculation\nAs noted, our choice for \\(b\\) will be the minimizer of\n\\[\n\\sum_{i=1}^{100} (C_{i} - b V_{i})^2\n\\]\nThis is a straightforward calculus problem. Setting\n\\[\n0 = \\frac{d}{db} \\sum_{i=1}^{100} (C_{i} - b V_{i} )^2 =\n-2 \\sum_{i=1}^{100} (C_{i} - b V_{i}) V_i\n\\]\nand solving \\(b\\), we find that\n\\[\nb = \\frac{\\sum_{i=1}^n C_i V_i}{\\sum_{i=1}^nV_i^2}\n\\]\n\n\n3.1.3 R code\n\nlm(n2 ~ n1-1)\n\n\nCall:\nlm(formula = n2 ~ n1 - 1)\n\nCoefficients:\n  n1  \n0.98  \n\n\nThis says, “Fit the model \\(E(C | V) = \\beta V\\)$ to the data, with the line constrained to pass through the origin.” The constraint is specified by the -1 term.\nWe see that the estimate regression line is\n\\[E(C | V) = 0.98 V\\]"
  },
  {
    "objectID": "Ch3.html#full-linear-regression-model",
    "href": "Ch3.html#full-linear-regression-model",
    "title": "3  Linear Statistical Models",
    "section": "3.2 Full linear regression model",
    "text": "3.2 Full linear regression model\nSay we do not want to constrain the model to pass the line through the origin. Our model is then\n\\[E(C | V) = \\beta_0 + \\beta_1 V\\]\nwhere we now have two unknown parameters to be estimated.\n\n3.2.1 Least-squares estimation, single predictor\nOur sum of squared prediction errors is now\n\\[\n\\sum_{i=1}^{100} [O_{i} - (b_0 + b_1 V_{i}) ]^2\n\\]\nThis means setting two derivatives to 0 and solving. Since the derivatives involve two different quantities to be optimized, \\(b_0\\) and \\(b_1\\), the derivatives are termed partial, and the \\(\\partial\\) symbol is used instead of ‘d’.\n\\[\n\\begin{align}\n0 &=\n\\frac{\\partial}{\\partial b_0}\\sum_{i=1}^{100} [C_{i} - (b_0 + b_1 V_{i })\n]^2 \\\\\n&=\n-2 \\sum_{i=1}^{100} [C_{i} - (b_0 + b_1 V_{i}) ]\n\\end{align}\n\\tag{3.1}\\]\nand\n\\[\n\\begin{align}\n0 &=\n\\frac{\\partial}{\\partial b_1}\\sum_{i=1}^{100} [C_{i} - (b_0 + b_1 V_{i})\n]^2 \\\\\n&=\n-2 \\sum_{i=1}^{100} [C_{i} - (b_0 + b_1 V_{i}) V_i]\n\\end{align}\n\\tag{3.2}\\]\nWe could then solve for the \\(b_i\\), but let’s go straight to the general case."
  },
  {
    "objectID": "Ch3.html#least-squares-estimation-general-number-of-predictors",
    "href": "Ch3.html#least-squares-estimation-general-number-of-predictors",
    "title": "3  Linear Statistical Models",
    "section": "3.3 Least-squares estimation, general number of predictors",
    "text": "3.3 Least-squares estimation, general number of predictors\n\n3.3.1 Nile example\nAs we have seen, systems of linear equations are natural applications of linear algebra. Equations Equation 3.1 and Equation 3.2 can be written in matrix terms as\n\\[\n\\left (\n\\begin{array}{rr}\n\\sum_{i=1}^n C_i \\\\\n\\sum_{i=1}^n C_i V_i \\\\\n\\end{array}\n\\right ) =\n\\left (\n\\begin{array}{rr}\n100 & \\sum_{i=1}^n V_i \\\\\n\\sum_{i=1}^n V_i & b_1 \\sum_{i=1}^n V_i^2 \\\\\n\\end{array}\n\\right )\n\\left (\n\\begin{array}{rr}\nb_0 \\\\\nb_1 \\\\\n\\end{array}\n\\right )\n\\]\nActually, that matrix equation can be derived more easily by using matrices to begin with:\nDefine \\(S\\) and \\(T\\):\n\\[\nS =\n\\left (\n\\begin{array}{rr}\nC_1 \\\\\nC_2 \\\\\n... \\\\\nC_{100} \\\\\n\\end{array}\n\\right )\n\\]\nand\n\\[\nT =\n\\left (\n\\begin{array}{rr}\nV_1 \\\\\nV_2 \\\\\n... \\\\\nV_{100} \\\\\n\\end{array}\n\\right )\n\\]\nThen our linear assumption, \\(E(C | V) = b_0 + b1 V\\), applied to \\(S\\) and \\(T\\), is\n\\[\nE(S | T) =\nA b\n\\]\nwhere\n\\[\nA =  \n\\left (\n\\begin{array}{rrrr}\n1 & V_1 \\\\\n1 & V_2 \\\\\n... & ... \\\\\n1 & V_{100} \\\\\n\\end{array}\n\\right )\n\\]\nand \\(b = (b_0,b_1)'.\\)\nOur predicted error vector is very simply expressed:\n\\[\nS - Ab\n\\]\nAnd since for any column vector \\(u\\), the sum of its squared elements is\n\\[\nu'u\n\\]\nour sum of squared prediction errors is\n\\[\n(S - Ab)'(S - Ab)\n\\]\nNow how we will minimize that matrix expression with respect to the vector \\(b\\). That is the subject of the next section.\n❄️ Your Turn: State what the matrix \\(A\\) would have been under the earlier model in which the regression line is assmed to go through the origin. And what about \\((S - Ab)'(S - Ab)\\) and so on? Does the math still produce the correct answer? Note: \\(b\\) is now a vector of length 1, a 1x1 matrix.\n❄️ Your Turn: Derivative of a Show that\n\\[\n\\frac{d}{du} u'Qu = 2Qu\n\\]\nfor a constant symmetric matrix \\(Q\\) and a vector \\(u\\). (\\(u'Qu\\) is called a quadratic form.)*\n\n\n3.3.2 Matrix derivatives.\nThe (column) vector of partial derivatives of a scalar quantity is called the gradient of that quantity. For instance, with\n\\[\nu = 2x + 3y^2 + xy\n\\]\nwe have that its gradient is\n\\[\n\\frac{du}{dx dy} =\n\\left (\n\\begin{array}{rr}\n2 + y \\\\\n6y + x \\\\\n\\end{array}\n\\right )\n\\]\nWith care, we can compute gradients entirely at the matrix level, using easily derivable properties, without ever resorting to returning to the scalar expressions. Let’s apply them to the case at hand in the last section,\n\\[\n(S - Ab)'(S - Ab)\n\\tag{3.3}\\]\n\n\n3.3.3 Differentiation purely in matrix terms\nIt can be shown that for a column vector \\(a\\),\n\\[\n\\frac{d}{da} a'a = 2a\n\\]\n❄️ Your Turn: Show this. Write \\(a = (a_1,...,a_k)'\\) and find the gradient “by hand.” Compare to \\(2a\\).\nEquation Equation 3.3 is indeed of the form \\(a'a\\), but the problem here is that \\(a\\) in turn is a function of \\(b\\), This calls for the Chain Rule, which does exist at the matrix level:\nFor example if \\(u = Mv + w\\), with \\(M\\) and \\(w\\) constants (i.e. not functions of \\(v\\), then\n\\[\n\\frac{d}{dv} u'u = 2M'u\n\\]\nWith \\(M = -A\\) and \\(w = S\\), we have\n\\[\n\\frac{d}{db} [(S - Ab)'(S - Ab) = -2 A'(S - Ab)\n\\]\nSo, set\n\\[\n0 = A'(S - Ab) = A'S - A'A b\n\\]\nyield our minimizing \\(b\\):\n\\[\nb = (A'A)^{-1} A'S\n\\]\nproviding the inverse exists (more on this in the next chapter).\nLet’s check this with the Nile example:\n\nA &lt;- cbind(1,n1)\nS &lt;- n2\nAp &lt;- t(A)\nsolve(Ap %*% A) %*% Ap %*% S\n\n          [,1]\n   452.7667508\nn1   0.5043159\n\n# check via /R\nlm(n2 ~ n1)\n\n\nCall:\nlm(formula = n2 ~ n1)\n\nCoefficients:\n(Intercept)           n1  \n   452.7668       0.5043  \n\n\n\n\n3.3.4 The general case\nSay our data consists of \\(n\\) points, each of which is of length \\(p\\). Write the \\(j^{th}\\) element of the \\(i^{th}\\) data point as \\(X_{ij}\\). Then set\n\\[\nA =\n\\left (\n\\begin{array}{rrrr}\n1 & X_{11} & ... & X_{p1} \\\\\n1 & X_{21} & ... & X_{p2} \\\\\n... & ... & ... & ... \\\\\n1 & X_{n1} & ... & X_{np} \\\\\n\\end{array}\n\\right )\n\\]\nConinue to set \\(S\\) to thw length-\\(n\\) column vector of our response variable, and write\n\\[\nb = (b_0,b_1,...,b_p)'\n\\]\nThen, using the same reasoning as before, we have the minimizing value of \\(b\\):\n\\[\nb = (A'A)^{-1} A'S\n\\]\nagain providing that the inverse exists.\nAs an examples, let’s take the mlb1 from my qeML (’Quick and Easy Machine Learning package.  The data is on major league baseball players. We will predict weight from height and age.Kindly provided by the UCLA Dept. of Statistics\n\nlibrary(qeML)\ndata(mlb1)\nhead(mlb1)\n\n        Position Height Weight   Age\n1        Catcher     74    180 22.99\n2        Catcher     74    215 34.69\n3        Catcher     72    210 30.78\n4  First_Baseman     72    210 35.43\n5  First_Baseman     73    188 35.71\n6 Second_Baseman     69    176 29.39\n\nourData &lt;- as.matrix(mlb1[,-1]) # must have matrix to enable %*%\nhead(ourData)\n\n  Height Weight   Age\n1     74    180 22.99\n2     74    215 34.69\n3     72    210 30.78\n4     72    210 35.43\n5     73    188 35.71\n6     69    176 29.39\n\nA &lt;- cbind(1,ourData[,c(1,3)])\nAp &lt;- t(A)\nS &lt;- as.vector(mlb1[,3])\nsolve(Ap %*% A) %*% Ap %*% S\n\n               [,1]\n       -187.6381754\nHeight    4.9235994\nAge       0.9115326\n\n# check via R\nlm(Weight ~ .,data=mlb1[,-1])\n\n\nCall:\nlm(formula = Weight ~ ., data = mlb1[, -1])\n\nCoefficients:\n(Intercept)       Height          Age  \n  -187.6382       4.9236       0.9115"
  },
  {
    "objectID": "Ch3.html#determinants",
    "href": "Ch3.html#determinants",
    "title": "3  Linear Statistical Models",
    "section": "3.4 Determinants",
    "text": "3.4 Determinants\nThis is a topic that is quite straightforward and traditional, even old-fashioned. Yet the theme of a book by Sheldon Axler,, Linear Algebra Done Right is that determoinants are overemphasized. He relegates the topic to the very end of the book. Yet determinants do appear often in applied linear algebra settings. Moreover, they will be convenient to use in explaing very concepts in this book.\nBut why place the topic in this particular chapter? The answer lies in the fact that earlier in this chapter we had the proviso “If \\((A'A)^{-1}\\) exists.” The following property of determinants is then relevant:\n\nA square matrix \\(G\\) is invertible if and only if \\(det(G) \\neq 0\\).\n\nThere are better ways to ascertain invertibility than this, but it is conceptually helpful. Determinants play a similar role in the topic of eigenveectors in Chapter 5.\n\ndet(Ap %*% A) \n\n[1] 103306202970\n\n\nNonzero! So \\((A'A)^{-1}\\) does exist, confirming that our R code that needed that property.\n\n3.4.1 Definition\nThe standard definition is one of the ugliest, and least useful, in all of mathematics. Instead we will define the term using one of the methods for calculating determinants.\n\nConsider an \\(r \\textrm{x} r\\) matrix \\(G\\). For \\(r = 2\\), write \\(G\\) as\n\\[\nG =\n\\left (\n\\begin{array}{rr}\na & b \\\\\nc & d  \\\\\n\\end{array}\n\\right )\n\\]\nand define \\(\\det(G)\\) to be \\(ad -bc\\). For \\(r &gt; 2\\), define submatrices as follows.\n\\(G_i\\) is the \\((r-1) \\textrm{x} (r-1)\\) submatrix obtained by removing row 1 and column \\(j\\) from \\(G\\). Then \\(\\det(G)\\) is defined recursively as\n\\[\n\\sum_{i=1}^r (-1)^{i+1} \\det(G_i)\n\\]\n\nFor instance, consider\n\\[\nM =\n\\left (\n\\begin{array}{rrr}\n5 & 1 & 0 \\\\\n3 & -1 & 7 \\\\\n  0 & 1 & 1 \\\\\n  \\end{array}\n\\right )\n\\]\nThe \\(\\det(M) = 5(-8) - 3 + 0 = -43\\).\n❄️ Your Turn: If you are familiar with recursive calls, write a function \\(\\verb+dt(a)+\\) to compute the determinant of a square matrix using this scheme.\n\n\n3.4.2 Properties\nWe state these without proof:\n\n\\(G^{-1}\\) exists if and only if \\(\\det(G) \\neq 0\\)\n\\(\\det(GH) = \\det(G) \\det(H)\\)"
  },
  {
    "objectID": "Ch3.html#update-formulas",
    "href": "Ch3.html#update-formulas",
    "title": "3  Linear Statistical Models",
    "section": "3.5 Update formulas",
    "text": "3.5 Update formulas\nOne important theme in developing prediction models (linear regression, neural networks etc.) is that, in testing a model we fit to only a subset of our data, then predict the remaining – “fresh” – holdout data using the model. Or, we might do this many times, say with many holdout sets of size 1, fitting the model on the remaining n-1 data points each time..\nThis could become computationally challenging, as we would need to refit the model each time. It would be nice if we could have an “update” formula that would quickly recalculate the model found on the full dataset. In rhw case of linear models, such a formula exists, in the Sherman-Morrison-Woodbury relation."
  },
  {
    "objectID": "Ch4.html#example-census-data",
    "href": "Ch4.html#example-census-data",
    "title": "4  Matrix Rank, and Vector Spaces Part I",
    "section": "4.1 Example: census data",
    "text": "4.1 Example: census data\nThis dataset is also from qeML. It is data for Silicon Valley engineers in the 2000 Census. Let’s focus on just a few columns.\n\ndata(svcensus) \nhead(svcensus) \n\n       age     educ occ wageinc wkswrkd gender\n1 50.30082 zzzOther 102   75000      52 female\n2 41.10139 zzzOther 101   12300      20   male\n3 24.67374 zzzOther 102   15400      52 female\n4 50.19951 zzzOther 100       0      52   male\n5 51.18112 zzzOther 100     160       1 female\n6 57.70413 zzzOther 100       0       0   male\n\nsvc &lt;- svcensus[,c(1,4:6)] \nhead(svc) \n\n       age wageinc wkswrkd gender\n1 50.30082   75000      52 female\n2 41.10139   12300      20   male\n3 24.67374   15400      52 female\n4 50.19951       0      52   male\n5 51.18112     160       1 female\n6 57.70413       0       0   male\n\nlm(wageinc ~ .,data=svc) \n\n\nCall:\nlm(formula = wageinc ~ ., data = svc)\n\nCoefficients:\n(Intercept)          age      wkswrkd   gendermale  \n   -29384.1        496.7       1372.8      10700.8  \n\n\nSo, we estimate that, other factors being equal, men about paid close to $11,000 more than women. This is a complex issue, but for our purposes here, how did gender become gendermale, no explicit mention of women?\nLet’s try to force the issue:\n\nsvc$man &lt;- as.numeric(svc$gender == 'male')\nsvc$woman &lt;- as.numeric(svc$gender == 'female')\nsvc$gender &lt;- NULL\nhead(svc)\n\n       age wageinc wkswrkd man woman\n1 50.30082   75000      52   0     1\n2 41.10139   12300      20   1     0\n3 24.67374   15400      52   0     1\n4 50.19951       0      52   1     0\n5 51.18112     160       1   0     1\n6 57.70413       0       0   1     0\n\nlm(wageinc ~ .,data=svc)\n\n\nCall:\nlm(formula = wageinc ~ ., data = svc)\n\nCoefficients:\n(Intercept)          age      wkswrkd          man        woman  \n   -29384.1        496.7       1372.8      10700.8           NA  \n\n\nWell, we couldn’t force the issue after all. Why not? We hinted above that \\(A' A\\) may not be invertible. Let’s check its row-reduced form.\n\nA &lt;- cbind(1,svc[,-2])\nA &lt;- as.matrix(A)\nApA &lt;- t(A) %*% A\nApA\n\n               1        age  wkswrkd      man    woman\n1        20090.0   794580.7   907240  15182.0   4908.0\nage     794580.7 33956543.6 35869770 600860.8 193719.9\nwkswrkd 907240.0 35869770.5 45252608 692076.0 215164.0\nman      15182.0   600860.8   692076  15182.0      0.0\nwoman     4908.0   193719.9   215164      0.0   4908.0\n\nlibrary(pracma)\nrref(ApA) \n\n        1 age wkswrkd man woman\n1       1   0       0   0     1\nage     0   1       0   0     0\nwkswrkd 0   0       1   0     0\nman     0   0       0   1    -1\nwoman   0   0       0   0     0\n\n\nAha! The row reduction process ended prematurely. This matrix has no inverse. We say that the matrix has rank 4, when it needs to be 5; we also say that \\(A'A\\) is not of full rank.\nRecall that in the nonfull rank example we presented in the last chapter, one row was double another. Here the sum of the last two columns of \\(A\\) was equal to the first column. We say the columns are linearly dependent. This is the culprit in non-full rank matrices.\nIn fact, not only was \\(A'A\\) not of full rank, but so was \\(A\\):\n\nqr(ApA)$rank\n\n[1] 4\n\nqr(A)$rank\n\n[1] 4\n\n\nThis is rather startling. \\(A\\) has over 20,000 rows — yet only 4 linearly independent ones? But it follows from this fact:There are many subsets of 4 rows that are linearly independent. But no sets of 5 or more are linearly independent.\n\nFor any rxs matix \\(G\\), the rank of \\(B\\) is less than or equal to \\(\\min(r,s)\\).\n\nDetails to follow."
  }
]