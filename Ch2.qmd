
```{r}
#| include: false
library(dsld)
library(qeML)
```

{{< pagebreak >}}


# Matrix Inverse

::: {.callout-note}

## Goals of this chapter:

Central to matrix operations is the matrix *inverse*, which is somewhat
analogous to reciprocals in arithmetic.

This is a fundamental operation in linear algebra (though ironically,
related quantities are often used instead of using the inverse directly).

:::

## Example: Computing Long-Run Markov Distribution{#sec-markovsolve}

At the end of the last chapter, we found that for a Markov chain with
transition matrix P and stationary distribution $\nu$,

$$
\nu = P' \nu
$${#eq-MarkovPi}

This suggests a method for computing $\nu$, by solving the above
equation.[]{.column-margin}

Rewrite it using the identity matrix:[Recall that for any square matrix
C and identity matrix I of the same size, $IC = CI =
C$.]{.column-margin}

$$
(I - P') \nu = 0
$$

For the random walk chain in Chapter 1, we had[In that particular model,
P' = P, but for most chains this is not the case.]{.column-margin}

$$
P =
\left (
\begin{array}{rrrrr}
0.5 & 0.5 & 0 & 0 & 0\\
0.5 & 0 & 0.5 & 0 & 0\\
0 & 0.5 & 0 & 0.5 & 0\\
0 & 0 & 0.5 & 0 & 0.5\\
0 & 0 & 0 & 0.5 & 0.5 \\
\end{array}
\right )
$$

With $\nu = (\nu_1,\nu_2,\nu_3,\nu_4,\nu_5)'$, the equation to be
solved, $(I-P') \nu = \nu$, is

$$
\left (
\begin{array}{rrrrr}
0.5 & -0.5 & 0 & 0 & 0 \\
-0.5 & 1 & -0.5 & 0 & 0 \\
0 & -0.5 & 1 & -0.5 & 0 \\
0 & 0 & -0.5 & 1 & -0.5 \\
0 & 0 & 0 & -0.5 & 0.5 \\
\end{array}
\right )
\left (
\begin{array}{r}
\nu_1 \\
\nu_2 \\
\nu_3 \\
\nu_4 \\
\nu_5 \\
\end{array}
\right ) = 
\left (
\begin{array}{r}
0 \\
0 \\
0 \\
0 \\
0 \\
\end{array}
\right ) 
$$

If we perform the matrix multiplication, we have an ordinary system of
linear equations:

$$
\begin{array}{r}
0.5 \nu_1 - 0.5 \nu_2 = 0 \\
-0.5 \nu_1 + \nu_2 - 0.5 \nu_3 = 0 \\
-0.5 \nu_2 + \nu_3 - 0.5 \nu_4 = 0 \\
-0.5 \nu_3 + \nu_4 - 0.5 \nu_5 = 0 \\
-0.5 \nu_4 + 0.5 \nu_5 = 0 \\
\end{array}
$$

This is high school math, and we could solve the equations that way. But
this is literally what linear algebra was invented for, solving systems
of equations! We will use matrix inverse.

But first, we have a problem to solve: The only solution to the above
system is with all $\nu_i = 0$. We need an equation involving a nonzero
quantity.

But we do have such an equation. The vector $\nu$ is a stationary distribution
for a Markov chain, and thus must sum to 1.0. Let's replace the last row
by that relation:

$$
\left (
\begin{array}{rrrrr}
0.5 & -0.5 & 0 & 0 & 0 \\
-0.5 & 1 & -0.5 & 0 & 0 \\
0 & -0.5 & 1 & -0.5 & 0 \\
0 & 0 & -0.5 & 1 & -0.5 \\
1 & 1 & 1 & 1 & 1 \\
\end{array}
\right )
\left (
\begin{array}{r}
\nu_1 \\
\nu_2 \\
\nu_3 \\
\nu_4 \\
\nu_5 \\
\end{array}
\right ) =
\left (
\begin{array}{r}
0 \\
0 \\
0 \\
0 \\
1 \\
\end{array}
\right )
$$

More compactly, 

$$
B \nu = d
$$

Many square matrices $A$ have a multiplicative inverse, denoted by
$A^{-1}$, with the property that

$$
A^{-1} A = A A^{-1} = I
$$

(We will often speak of *the* inverse of $A$. In fact, if $A$ is
invertible, its inverse is unique.)

If our matrix $B$ is invertible, we can premultiply both sides of our
equation above, yielding

$$
B^{-1} d = B^{-1} B \nu = \nu
$$

So, we have obtained our solution for the stationary distribution $\nu$.
We can evaluate it numerically via the R **solve** function, which finds
matrix inverse:

```{r}
B <-
rbind(c(0.5,-0.5,0,0,0), c(-0.5,1,-0.5,0,0), c(0,-0.5,1,-0.5,0),
   c(0,0,-0.5,1,-0.5), c(1,1,1,1,1))
B
Binv <- solve(B)
# check the inverse
Binv %*% B  # yes, get I (of course with some roundoff error)
nu <- Binv %*% c(0,0,0,0,1)
nu
```

This confirms our earlier speculation based on powers of $P'$.

## Matrix Algebra{#sec-matalg}

Several properties to note:

* If the inverses of $A$ and $B$ exist, and $A$ and $B$ are conformable,
  then $(AB)^{-1}$ exists and is equal to $B^{-1} A^{-1}$.

  *Proof:* Consider the product $(AB) (B^{-1} A^{-1})$. The $B factors 
  give us $I$, leaving $A A^{-1}$, which too is $I$. 

* $(A')^{-1}$ exists and is equal to $(A^{-1})'$.

  *Proof:* Follows immediately from $A A^{-1} = I$.

* If $A$ is invertible and symmetric, then $(A^{-1})'$ is also
  symmetric.

  *Proof:* For convenience, let $B$ denote $A^{-1}$. Then

  $$
  AB = I
  $$

  Recalling from @sec-transpose that the transpose of a product is the
  reverse product of the transposes, we have

  $$
  I = I' = (AB)' = B'A'
  $$

  But since $A' = A$, we have

  $$
  I = B' A
  $$

  In other words, not only is $B$ the inverse of $A$, $B'$ is too!
  So, $B = B'$.

## Computation of the Matrix Inverse

Finding the inverse of a large matrix -- in data science applications,
the number of rows and columns $n$ can easily be hundreds or more -- can
be computationally challenging. The run time is proportional to $n^3$,
and roundoff error can be an issue. Sophisticated algorithms have been
developed, such as QR and Choleski decompositions. So in R, we should
use, say, **qr.solve** rather than **solve** if we are working with
sizable matrices.

The classic "pencil and paper" method for matrix inversion is
instructive, and will be presented here.

### Pencil-and-paper computation{#sec-pencilpaper}

The basic idea follows the pattern the reader learned for solving
systems of linear equations, but with the added twist of involving some
matrix multiplication.

Let's take as our example 

$$
 A = 
 \left (
 \begin{array}{rr}
 4 & 7 \\
 -8 & 15  \\
 \end{array}
 \right )
 $$

We aim to transform this to the 2x2 identity matrix, via a sequence of
row operations.

### Use of elementary matrices{#sec-elemmats}

Let's multiply row 1 by 1/4, to put 1 in the first element:

$$
 \left (
 \begin{array}{rr}
 1 & \frac{7}{4} \\
 -8 & 15  \\
 \end{array}
 \right )
 $$

In matrix terms, that operation is equivalent to premultiplying $A$ by 

$$
E_1=
\left (
\begin{array}{rr}
\frac{1}{4} & 0  \\
0 & 1  \\
\end{array}
\right )
$$

We then add 8 times row 1 to row 2, yielding

$$
 \left (
 \begin{array}{rr}
 1 & \frac{7}{4} \\
 0 & 29  \\
 \end{array}
 \right )
 $$

The premultiplier for this operation is

$$
E_2=
\left (
\begin{array}{rr}
1 & 0  \\
8 & 1  \\
\end{array}
\right )
$$

Multiply row 2 by 1/29:

$$
 \left (
 \begin{array}{rr}
 1 & \frac{7}{4} \\
 0 & 1  \\
 \end{array}
 \right )
 $$

corresponding to 

$$
E_3=
\left (
\begin{array}{rr}
1 & 0  \\
8 & 1/29  \\
\end{array}
\right )
$$

And finally, add -7/4 row 2 to row 1.

$$
 \left (
 \begin{array}{rr}
 1 & 0 \\
 0 & 1  \\
 \end{array}
 \right )
 $$

The premultiplier is

$$
E_4 =
 \left (
 \begin{array}{rr}
 1 & \frac{7}{4} \\
 0 & 1  \\
 \end{array}
 \right )
 $$

Now, how does that give use $A^{-1}$? The method your were taught
probably set up the partioned matrix $(A,I)$. The row operations that
transformed $A$ to $I$ also transformed $I$ to $A^{-1}$. Here's why:

As noted, the row operations are such that 

$$
E_4 E_3 E_2 E_1 A 
$$

give us the final transformed result, i.e. the matrix $I$: 

$$
(E_4 E_3 E_2 E_1) A = I
$$

Aha! We have found the inverse of $A$--it's $E_4 E_3 E_2 E_1$.

Note too that[Recall that the inverse of a product (if it exists is the reverse
product of the inverses.]{.column-margin}

$$
A = (E_4 E_3 E_2 E_1)^{-1} I = E_4^{-1} E_3^{-1} E_2^{-1} E_1^{-1}
$$

So apparently the inverses of the elementary matrices $E_i$
also exist, and in fact we can obtain them easily:

Each $E_i^{-1}$ simply "undoes" its partner. $E_1$, for instance, multiplies the
row 1, column 1 element by 1/4, which is "undone" by multiplying that
element by 4,

$$
E_1^{-1} =
 \left (
 \begin{array}{rr}
 4 & 0 \\
 0 & 1  \\
 \end{array}
 \right )
$$
 
Also, to undo the operation of adding 8 times row 1 to row 2, we
add -8 times row 1 to row 2:

$$
E_2^{-1} =
 \left (
 \begin{array}{rr}
 1 & 0 \\
 -8 & 1 \\
 \end{array}
 \right )
$$

## Nonexistent Inverse{#sec-noinverse}

Suppose our matrix $A$ had been slightly different:

$$
 A =
 \left (
 \begin{array}{rr}
 4 & 7 \\
 -8 & -14  \\
 \end{array}
 \right )
 $$

This would have led to

$$
 \left (
 \begin{array}{rr}
 1 & \frac{7}{4} \\
 0 & 0  \\
 \end{array}
 \right )
 $$

This cannot lead to $i$, indicating that $A^{-1}$ does not exist, and
the matrix is said to be *singular*. And it's no coincidence that row 2
of $A$ is double row 1. This has many implications, as will be seen in
our chapter on vector spaces.

## Determinants

This is a topic that is quite straightforward and traditional, even
old-fashioned--in fact, *too* old-fashioned, according to mathematician
Sheldon Axler. The theme of  his book, *Linear Algebra Done Right*, is
that determinants are overemphasized. He relegates the topic to the very
end of the book. Yet determinants do appear often in applied linear
algebra settings. Moreover, they will be convenient to use in explaing
very concepts in this book on linear algebra in *Data Science*.

But why place the topic in this particular chapter? The answer lies in
the fact that earlier in this chapter we had the proviso "If
$(A'A)^{-1}$ exists." The following property of determinants is then
relevant:

> A square matrix $G$ is invertible if and only if $det(G) \neq 0$.

There are better ways to ascertain invertibility than this, but it is
conceptually helpful. Determinants play a similar role in the topic of
eigenvectors in Chapter 5.

### Definition{#sec-detdef}

The standard definition is one of the ugliest, in all of mathematics.
Instead we will define the term using one of the methods for calculating
determinants.

<blockquote>

Consider an $r \textrm{ x } r$ matrix $G$.  For $r = 2$, write $G$ as

$$
 G = 
 \left (
 \begin{array}{rr}
 a & b \\
 c & d  \\
 \end{array}
 \right )
 $$

and define $\det(G)$ to be $ad -bc$. For $r > 2$, 
define submatrices as follows. 

$G_i$ is the $(r-1) \textrm{ x } (r-1)$ submatrix obtained by removing row
1 and column $j$ from $G$. Then $\det(G)$ is defined recursively as

$$
\sum_{i=1}^r (-1)^{i+1} \det(G_i)
$$

</blockquote>

For instance, consider 

$$
 M = 
 \left (
 \begin{array}{rrr}
 5 & 1 & 0 \\
 3 & -1 & 7 \\
  0 & 1 & 1 \\
  \end{array}
 \right )
$$

The $\det(M) = 5(-1 - 7) - 1(3 - 0) + 0 = -43$.

*A glimpse at the classical definition:*

Using the same approach as in the last computation, we would find that
the determinant of a general $3 \textrm{ x } 3$ matrix

$$
 \left (
 \begin{array}{rrr}
 a & b & c \\
 d & e & f \\
 g & h & i \\
  \end{array}
 \right )
$$

is 

$$
aei + bfg + cdh - afh - bdi - ceg
$$

Each term here is involves a product of 3 of the elements of the matrix.
In general, the determinant involves sums and differences of permuted
products of distinct elements of the matrix, as we see above.  The
formation of the terms in general, and the determination of 
+ and - signs, is done in complex but precise manner that we will not
present here. But the reader should at least keep in mind that each term
is a product of $n$ elements of the matrix, a fact that will be relevant
in the sequel.


### Properties

We state these without proof:

* $G^{-1}$ exists if and only if $\det(G) \neq 0$

* $\det(GH) = \det(G) \det(H)$

## The Multivariate Normal Distribution Family{#sec-mvn}

The familiar "bell-shaped curve" refers to the *normal* (or *Gaussian*)
family, whose densities have the form

$$
\frac{1}{\sigma \sqrt{2 \pi}}
e^{-0.5 (\frac{t-\mu}{\sigma})^2}
$$

The values of $\mu$ and $\sigma$ are the mean and standard deviation.
But what if we have a random vector, say of length $k$? Is there a
generalized normal family? 

### Example: k = 2

The answer is yes.  Here is an example for $k = 2$:

![3D bell density](Bell.png)

### General form

Well, then, what is the form of the $k$-dimensional density function?
Just as the univariate normal family is parameterized by mean and
variance, the multivariate one is parameterized via its mean vector $\mu$
and a *covariance matrix* $\Sigma$, a term we will define in a later
chapter. The specific form is 

$$
(2\pi)^{-k/2} \det(\Sigma)^{-k/2}
e^{-0.5 (t-\mu)'(\Sigma)^{-1}(t-\mu)}
$$

Note the intuition:

* Instead of $1/\sigma^2$, i.e. instead of dividing by variance,
we "divide by $\Sigma$," intuitively viewing matrix inverse as a
"reciprocal" of a matrix.

* In other words, covariance matrices operate roughly like
generalized variances.

* Instead of squaring the scalar $t - \mu$, we "square" it in the vector
case by peforming a $w'w$ operation, albeit with $\Sigma^{-1}$ in the
middle. 

Clearly, we should not stretch these analogies very far, but they do
help our intuition here.

### Properties

::: {#thm-conditMVnormal}

if a random vector is MV normally distributed, then
the conditional distribution of any one of its components $Y$, given the
others $X_{others} = t$ (note that $t$ is a vector if $k > 2$) has the
following properties:

* It has a (univariate) normal distribution.

* Its mean $E(Y | X_{others}) = t$ linear in $t$.

* Its variance $Var(Y | X_{others}) = t$ does not involve $t$.

These of course are the classical assumptions of linear regression
models. They actually come from the MV normal model.

:::

::: {.proof}
This comes out of writing down the conditional density (overall density
divided by marginal), and then doing some algebra.

$\square$

::: 

::: {#thm-xnormalthenaxnormal}

If $X$ is a multivariate-normal random vector, then so is $AX$ for any
conformable nonrandom matrix $A$.

::: 

::: {.proof}

Again, perform direct evaluation of the density.

$\square$
::: 

::: {#thm-mvuvnormal}

A random vector $X$ has a multivariate normal distribution if and only
if $w'X$ has a univariate normal distribution for all conformable
nonrandom vectors $w$.

:::

::: {#thm-mvclt}

## The Multivariate Central Limit Theorem

Let $X_1, X_2,...$ be a sequence on statistically independent random
vectors, with common distribution multivariate normal with mean vector
$\mu$ and covariance matrix $\Sigma$. Write

$$
\bar{X} = \frac{X_1+...+X_n}{n}
$$

Then the distribution of the random vector

$$
W_n = \sqrt{n} (\bar{X} - \mu)
$$

goes to multivariate normal with the 0 vector as mean and[The usual form
would involve the "square root" of a matrix, but we will not discuss
that concept until our chapter on inner product spaces.]{.column-margin}
covariance matrix $\Sigma$.

::: {.proof}

@thm-mvuvnormal reduces the problem to the univariate case, where we
know the Central Limit Theorem holds.

$\square$

:::

## Your Turn

❄️  **Your Turn:** Prove the assertions in @sec-matalg.  Note that for
the identity matrix $I$, $I' = I$.
 
❄️  **Your Turn:** In @sec-elemmats, find the inverses of $E_3$
and $E_4$ using similar reasoning, and thus find $A^{-1}$.

❄️  **Your Turn:** If you are familiar with recursive calls, write a
function $\verb+dt(a)+$ to compute the determinant of a square matrix
$A$.

❄️  **Your Turn:** The determinant of a 3 x 3 matrix

$$
M = 
 \left (
 \begin{array}{rrr}
 a & b & d \\
 d & e & f \\
 g & h & i \\
 \end{array}
 \right )
$$ 

is 

$$
aei+bfg+cdh-ceg-bdi-afh
$$

Suppose the elements of $M$ are independent random variables with uniform
distributions on (0,1). Argue that P(M is invertible) = 1.

❄️  **Your Turn:** Show that in the scalar context,

$$
Cov(X,Y) = E(XY) - EX ~ EY
$$

❄️  **Your Turn:** Show that in the vector context,

$$
Cov(X) = E(X X') - (EX) (EX)'
$$

