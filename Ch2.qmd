
```{r}
#| include: false
library(dsld)
library(qeML)
```

{{< pagebreak >}}


# Matrix Inverse

::: {.callout-note}

## Goals of this chapter:

Central to matrix operations is the matrix *inverse*, which is somewhat
analogous to reciprocals in arithmetic.

This is a fundamental operation in linear algebra (though ironically,
related quantities are often used instead of using the inverse directly).

:::

To motivate our discussion of matrix inverse, we first revisit the topic
of Markov chains.

## A Further Look at Markov Chains

Suppose $X_0$, our state at time 0, is random. Let $f$ denote its
distribution, i.e. its list of probabilities: $f_i = P(X_0 = i)$, i =
1,...,k, where k is the number of states in the chain. What about the
distribution of $X_1$, the state at time 1?  Let's find an expression
for $g$, the distribution of $X_1$, in terms of $f$.

$$
g_j = P(X_1 = j)
= \sum_{i=1}^k P(X_0 = i) P(X_1 = j | X_0 = i)
= \sum_{i=1}^k f_i a_{ij}
$$

where $a_{ij}$ is the row $i$, column $j$ element of the chain's
transition matrix $P$.

Putting this is more explicit matrix terms,

$$
g' = (g_1,...,g_k)' 
=
(f_1 a_{11} + ... + f_k a_{k1},
...,
f_1 a_{1k} + ... + f_k a_{kk})
$$

Setting $b_j$ to column $j$ of $P$ and using matrix partitioning,
we see that that last expression is

$$
(f' b_1,..., f' b_k) = f' (b_1,...,b_k) = f'P
$$


So we have a nice compact relation for the distribution of $X_1$ in
terms of the distribution of $X_0$.

$$
g' = f'P
$$


And setting $h$ to the distribution of $X_2$, the same reasoning gives us

$$
h' = g'P 
$$

::: {.column-margin}

Note that we used the Markov property, "memorylessness." Once we reach time 1, "time starts over," regradless of the previous history, i.e.  regardless of where we were at time 0.

:::

Let $d_r$ denote the distribution of $X_r$. Generalizing
the above reasoning gives us

$$
d_r' = d_{r-1}' P
$$

For convenience, let's take transposes (recalling that $(AB)' =
B'A'$):

$$
d_r = P' d_{r-1}
$${#eq-djInf}

Now suppose our chain has a long-run distribution $\nu$, as in
{@sec-introMCs}, so that

$$
\lim_{r \rightarrow \infty} d_r = \nu
$$

Applying this to @eq-djInf, we have

$$
\nu = P' \nu
$${#eq-nuSolve}

Since P is known, this provides us with a way to compute $\nu$. All we
need to do is solve @eq-nuSolve. Well, how do we do that? It turns out
that use of matrix inverses will solve our problem.



## Definition

> For any square matrix $A$, its *inverse* $B$ (if it exists)
> is a square matrix of the same size such that
> 
> $$
> AB = BA = I
> $$
> 
> where $I$ is the identity matrix of that size.

As hinted, many matrices do not have inverses. For instance, if $A$
consists of all 0s, there is no way to get $I$ from $AB$.

In very rough terms, it sometimes helps the intuition to think of an
inverse as the "reciprocal" of the matrix.

We will often speak of *the* inverse of $A$. In fact, if $A$ is
invertible, its inverse is unique.

## Example: Computing Long-Run Markov Distribution{#sec-markovsolve}

Now let us return to @eq-nuSolve, which expresses the vector of
long-run state probabilities for a Markov chain with transition matrix P
and stationary distribution $\nu$, How can we use matrix inverses to
solve this equation?

Starting with 

$$
\nu = P' \nu
$${#eq-MarkovPi}

rewrite using the identity matrix:

$$
(I - P') \nu = 0
$$

For the random walk chain in Chapter 1, we had[In that particular model,
P' = P, but for most chains this is not the case.]{.column-margin}

$$
P =
\left (
\begin{array}{rrrrr}
0.5 & 0.5 & 0 & 0 & 0\\
0.5 & 0 & 0.5 & 0 & 0\\
0 & 0.5 & 0 & 0.5 & 0\\
0 & 0 & 0.5 & 0 & 0.5\\
0 & 0 & 0 & 0.5 & 0.5 \\
\end{array}
\right )
$${#eq-randwalkP}

With $\nu = (\nu_1,\nu_2,\nu_3,\nu_4,\nu_5)'$, the equation to be
solved, $(I-P') \nu = \nu$, is

$$
\left (
\begin{array}{rrrrr}
0.5 & -0.5 & 0 & 0 & 0 \\
-0.5 & 1 & -0.5 & 0 & 0 \\
0 & -0.5 & 1 & -0.5 & 0 \\
0 & 0 & -0.5 & 1 & -0.5 \\
0 & 0 & 0 & -0.5 & 0.5 \\
\end{array}
\right )
\left (
\begin{array}{r}
\nu_1 \\
\nu_2 \\
\nu_3 \\
\nu_4 \\
\nu_5 \\
\end{array}
\right ) = 
\left (
\begin{array}{r}
0 \\
0 \\
0 \\
0 \\
0 \\
\end{array}
\right ) 
$$

If we perform the matrix multiplication, we have an ordinary system of
linear equations:

$$
\begin{array}{r}
0.5 \nu_1 - 0.5 \nu_2 = 0 \\
-0.5 \nu_1 + \nu_2 - 0.5 \nu_3 = 0 \\
-0.5 \nu_2 + \nu_3 - 0.5 \nu_4 = 0 \\
-0.5 \nu_3 + \nu_4 - 0.5 \nu_5 = 0 \\
-0.5 \nu_4 + 0.5 \nu_5 = 0 \\
\end{array}
$${#eq-all0s}

This is high school math, and we could solve the equations that way. But
this is literally what linear algebra was invented for, solving systems
of equations! We will use matrix inverse.

But first, we have a problem to solve: The only solution to the above
system is with all $\nu_i = 0$. We need an equation involving a nonzero
quantity.

But we do have such an equation. The vector $\nu$ is a stationary
distribution for a Markov chain, i.e. the set of long-run probabilities,
and thus it must sum to 1.0. Let's replace the last row by that
relation:

$$
\left (
\begin{array}{rrrrr}
0.5 & -0.5 & 0 & 0 & 0 \\
-0.5 & 1 & -0.5 & 0 & 0 \\
0 & -0.5 & 1 & -0.5 & 0 \\
0 & 0 & -0.5 & 1 & -0.5 \\
1 & 1 & 1 & 1 & 1 \\
\end{array}
\right )
\left (
\begin{array}{r}
\nu_1 \\
\nu_2 \\
\nu_3 \\
\nu_4 \\
\nu_5 \\
\end{array}
\right ) =
\left (
\begin{array}{r}
0 \\
0 \\
0 \\
0 \\
1 \\
\end{array}
\right )
$${#eq-lastRow1s}

More compactly, 

$$
G \nu = q
$$

If our matrix $G$ is invertible,
[$I-P'$ might not be invertible, as there may not be a long-run
distribution.]{.column-margin}
we can premultiply both sides of our
equation above, yielding

$$
G^{-1} q = G^{-1} G \nu = \nu
$$

So, we have obtained our solution for the stationary distribution $\nu$,

$$
\nu = G^{-1} q
$$

We can evaluate it numerically via the R **solve** function, which finds
matrix inverse:

```{r}
G <-
rbind(c(0.5,-0.5,0,0,0), c(-0.5,1,-0.5,0,0), c(0,-0.5,1,-0.5,0),
   c(0,0,-0.5,1,-0.5), c(1,1,1,1,1))
G
Ginv <- solve(G)
# check the inverse
Ginv %*% G  # yes, get I (of course with some roundoff error)
nu <- Ginv %*% c(0,0,0,0,1)  # recall that q = c(0,0,0,0,1)
nu
```

This confirms our earlier speculation in @sec-introMCs based on powers of $P$.

## Matrix Algebra{#sec-matalg}

Several properties to note:

* If the inverses of $A$ and $B$ exist, and $A$ and $B$ are conformable,
  then $(AB)^{-1}$ exists and is equal to $B^{-1} A^{-1}$.

  *Proof:* Consider the product $(AB) (B^{-1} A^{-1})$. The $B$ factors 
  give us $I$, leaving $A A^{-1}$, which too is $I$. 

* $(A')^{-1}$ exists and is equal to $(A^{-1})'$.

  *Proof:* Follows immediately from $A A^{-1} = I$ and the fact that
$(UV)' = V'U'$.

* If $A$ is invertible and symmetric, then $(A^{-1})'$ is also
  symmetric.

  *Proof:* For convenience, let $B$ denote $A^{-1}$. Then

  $$
  AB = I
  $$

  Again using the fact that the transpose of a product is the
  reverse product of the transposes, we have

  $$
  I = I' = (AB)' = B'A'
  $$

  But since $A' = A$, we have

  $$
  I = B' A
  $$

  In other words, not only is $B$ the inverse of $A$, $B'$ is too!
  So, $B = B'$.

## Computation of the Matrix Inverse

Finding the inverse of a large matrix -- in data science applications,
the number of rows and columns $n$ can easily be hundreds or more -- can
be computationally challenging. The run time is proportional to $n^3$,
and roundoff error can be an issue. Sophisticated algorithms have been
developed, such as the QR decompositions. So in R, we should
use, say, **qr.solve** rather than **solve** if we are working with
sizable matrices, or even use methods that do not directly compute the
inverse.

The classic "pencil and paper" method for matrix inversion is
instructive, and will be presented here.

### Pencil-and-paper computation{#sec-pencilpaper}

Note: Some readers will notice some similarity here with elementary
methods they learned in high school, but actually the treatment here is
much more sophisticated. It will play an important practical and
theoretical role in @sec-matrank.

The basic idea follows the pattern the reader learned for solving
systems of linear equations, but with the added twist of involving some
matrix multiplication.

Let's take as our example 

$$
 A = 
 \left (
 \begin{array}{rr}
 4 & 7 \\
 -8 & 15  \\
 \end{array}
 \right )
 $$

We aim to transform this to the 2x2 identity matrix, via a sequence of
row operations.

### Use of elementary matrices{#sec-elemmats}

Let's multiply row 1 by 1/4, to put 1 in the first element:

$$
 \left (
 \begin{array}{rr}
 1 & \frac{7}{4} \\
 -8 & 15  \\
 \end{array}
 \right )
 $$

In matrix terms, that operation is equivalent to premultiplying $A$ by 

$$
E_1=
\left (
\begin{array}{rr}
\frac{1}{4} & 0  \\
0 & 1  \\
\end{array}
\right )
$$

We then add 8 times row 1 to row 2, yielding

$$
 \left (
 \begin{array}{rr}
 1 & \frac{7}{4} \\
 0 & 29  \\
 \end{array}
 \right )
 $$

The premultiplier for this operation is

$$
E_2=
\left (
\begin{array}{rr}
1 & 0  \\
8 & 1  \\
\end{array}
\right )
$$

Multiply row 2 by 1/29:

$$
 \left (
 \begin{array}{rr}
 1 & \frac{7}{4} \\
 0 & 1  \\
 \end{array}
 \right )
 $$

corresponding to 

$$
E_3=
\left (
\begin{array}{rr}
1 & 0  \\
8 & 1/29  \\
\end{array}
\right )
$$

And finally, add -7/4 row 2 to row 1.

$$
 \left (
 \begin{array}{rr}
 1 & 0 \\
 0 & 1  \\
 \end{array}
 \right )
 $$

The premultiplier is

$$
E_4 =
 \left (
 \begin{array}{rr}
 1 & \frac{7}{4} \\
 0 & 1  \\
 \end{array}
 \right )
 $$

Now, how does that give use $A^{-1}$? The method your were taught
probably set up the partioned matrix $(A,I)$. The row operations that
transformed $A$ to $I$ also transformed $I$ to $A^{-1}$. Here's why:

As noted, the row operations are such that 

$$
E_4 E_3 E_2 E_1 A 
$$

give us the final transformed result, i.e. the matrix $I$: 

$$
(E_4 E_3 E_2 E_1) A = I
$$

Aha! We have found the inverse of $A$ -- it's $E_4 E_3 E_2 E_1$.

Note too that[Recall that the inverse of a product (if it exists is the reverse
product of the inverses.]{.column-margin}

$$
A = (E_4 E_3 E_2 E_1)^{-1} I = E_4^{-1} E_3^{-1} E_2^{-1} E_1^{-1}
$$

So apparently the inverses of the elementary matrices $E_i$
also exist, and in fact we can obtain them easily:

Each $E_i^{-1}$ simply "undoes" its partner. $E_1$, for instance, multiplies the
row 1, column 1 element by 1/4, which is "undone" by multiplying that
element by 4,

$$
E_1^{-1} =
 \left (
 \begin{array}{rr}
 4 & 0 \\
 0 & 1  \\
 \end{array}
 \right )
$$
 
Also, to undo the operation of adding 8 times row 1 to row 2, we
add -8 times row 1 to row 2:

$$
E_2^{-1} =
 \left (
 \begin{array}{rr}
 1 & 0 \\
 -8 & 1 \\
 \end{array}
 \right )
$$

## Nonexistent Inverse{#sec-noinverse}

Suppose our matrix $A$ had been slightly different:

$$
 A =
 \left (
 \begin{array}{rr}
 4 & 7 \\
 -8 & -14  \\
 \end{array}
 \right )
 $$

This would have led to

$$
 \left (
 \begin{array}{rr}
 1 & \frac{7}{4} \\
 0 & 0  \\
 \end{array}
 \right )
 $$

This cannot lead to $I$, indicating that $A^{-1}$ does not exist, and
the matrix is said to be *singular*. And it's no coincidence that row 2
of $A$ is double row 1. This has many implications, as will be seen in
our chapter on vector spaces.

## Determinants

This is a topic that is quite straightforward and traditional, even
old-fashioned -- in fact, *too* old-fashioned, according to mathematician
Sheldon Axler. The theme of  his book, *Linear Algebra Done Right*, is
that determinants are overemphasized. He relegates the topic to the very
end of the book. Yet determinants do appear often in applied linear
algebra settings. Moreover, they will be convenient to use in explaining
concepts in this book on linear algebra in *Data Science*.

But why place the topic in this particular chapter? The answer lies in
the fact that earlier in this chapter we had the proviso "If
$(A'A)^{-1}$ exists." The following property of determinants is then
relevant:

> A square matrix $G$ is invertible if and only if $\det(G) \neq 0$.

There are better ways to ascertain invertibility than this, but it is
conceptually helpful. Determinants play a similar role in the topic of
eigenvectors in @sec-eigenChap.

### Definition{#sec-detdef}

The standard definition is one of the ugliest in all of mathematics.
Instead we will define the term using one of the methods for calculating
determinants.

> Consider an $r \times r$ matrix $G$.  For $r = 2$, write $G$ as
> 
> $$
>  G = 
>  \left (
>  \begin{array}{rr}
>  a & b \\
>  c & d  \\
>  \end{array}
>  \right )
>  $$
> 
> and define $\det(G)$ to be $ad -bc$. For $r > 2$, 
> define submatrices as follows. 
> 
> $G_j$ is the $(r-1) \times (r-1)$ submatrix obtained by removing row
> 1 and column $j$ from $G$. Then $\det(G)$ is defined recursively as
> 
> $$
> \sum_{i=1}^r (-1)^{i+1} \det(G_i)
> $$
> 
> Actually, we can alternatively remove row $i$ instead of row 1. If $i$ 
> is an odd number, the same recursive formula holds, but for even $i$,
> replace $(-1)^{i+1}$ by $(-1)^i$.
>
> The same rules apply if 'row' is replaced by 'column' above.
> 

For instance, consider 

$$
 M = 
 \left (
 \begin{array}{rrr}
 5 & 1 & 0 \\
 3 & -1 & 7 \\
  0 & 1 & 1 \\
  \end{array}
 \right )
$$

Then $\det(M) = 5(-1 - 7) - 1(3 - 0) + 0 = -43$.

*A glimpse at the classical definition:*

Using the same approach as in the last computation, we would find that
the determinant of a general $3 \times 3$ matrix

$$
 \left (
 \begin{array}{rrr}
 a & b & c \\
 d & e & f \\
 g & h & i \\
  \end{array}
 \right )
$$

is 

$$
aei + bfg + cdh - afh - bdi - ceg
$$

Each term here is involves a product of 3 of the elements of the matrix.
In general, the determinant involves sums and differences of permuted
products of distinct elements of the matrix, as we see above.  The
formation of the terms in general, and the determination of 
+ and - signs, is done in complex but precise manner that we will not
present here. But the reader should at least keep in mind that each term
is a product of $n$ elements of the matrix, a fact that will be relevant
in the sequel.


### Properties

We state these without proof:

* $G^{-1}$ exists if and only if $\det(G) \neq 0$

* $\det(GH) = \det(G) \det(H)$

## Your Turn
 
❄️  **Your Turn:** In @sec-elemmats, find the inverses of $E_3$
and $E_4$ using similar reasoning, and thus find $A^{-1}$.

❄️  **Your Turn:** If the matrix $A$ has a 0 row, then
$\det(A)$ must be 0. Explain why.

❄️  **Your Turn:** We say a square matrix $A = (a_{ij})$ is
*upper-triangular* if its below-diagonal elements are all 0s.
Give a closed-form formula for $\det(A)$ in terms of the $a_{ij}$.

❄️  **Your Turn:** In @eq-randwalkP, consider the variant

$$
P =
\left (
\begin{array}{rrrrr}
0.5 & 0.5 & 0 & 0 & 0\\
0.5 & 0 & 0.5 & 0 & 0\\
0 & 0.5 & 0 & 0.5 & 0\\
0 & 0 & 0.5 & 0 & 0.5\\
0 & 0 & 0 & 1.0 & 0.0 \\
\end{array}
\right )
$${#eq-randwalkP}

Here, the walker at state 5 immediately "bounces back" to state 4,
rather than remaining at state 5 for one or more epochs.

In the original chain, we found that $\nu = (0.2,0.2,0.2,0.2,0.2)'$.
Speculate as to the effect on $\nu_5$ of the above change in model.
Then investigate to determine if our speculation was correct.


❄️  **Your Turn:** Consider a Markov chain with transition probability
matrix 

$$
P = 
\left (
\begin{array}{rr}
0 & 1 \\
1 & 0  \\
\end{array}
\right )
$$

This chain is termed *periodic*, with period 2. It alternates between
states 1 and 2, and thus a long-run distribution $\nu$ does not exist.
That would suggest the $I-P'$ noninvertible. Confrim this.

❄️  **Your Turn:** If you are familiar with recursive calls, write a
function $\verb+dt(a)+$ to compute the determinant of a square matrix
$A$.

❄️  **Your Turn:** Prove the assertions in @sec-matalg.  Note that for
the identity matrix $I$, $I' = I$.

❄️  **Your Turn:** The determinant of a 3 x 3 matrix

$$
M = 
 \left (
 \begin{array}{rrr}
 a & b & d \\
 d & e & f \\
 g & h & i \\
 \end{array}
 \right )
$$ 

is 

$$
aei+bfg+cdh-ceg-bdi-afh
$$

Suppose the elements of $M$ are independent random variables with uniform
distributions on (0,1). Argue that P(M is invertible) = 1.
