
```{r} 
#| include: false
library(dsld)
library(qeML)
```

{{< pagebreak >}}

# Attention

::: {.callout-note}

## Goals of this chapter:

The emergence of Large Language Models (LLMs) really startled the
general public, leading to an explosion of public attention to the
field.  In turn, the pivotal research paper behind the success of LLMs
was *Attention Is All You Need*, by Ashish Vaswani *et al*, 2017.
Actually, the mathematical operation called *attention* is rather simple
in its foundation.  This approach is in turn related to material that
has arisen here in earlier chapters. Here we explore the basics of this
idea.

:::

```{r} 
#| include: false
library(dsld)
library(qeML)
```

## Dot Product as a Measure of Similarity

As discussed in @sec-multcolcause, the row $i$, column $j$
element of the matrix $A'A$ in @eq-linregformula is a measure of the
strength of relation between predictor variables $i$ and $j$. And since
that matrix element is the dot product between rows $i$ and $j$, we see
that dot products serve as measures of similarity.

Recall the key role of dot products in @sec-kerneltrick, with kernels
acting like generalized dot products. One of the reasons dot products
are so important is that they serve as measures of similarity.

This point also arises in recommender systems. Recall @sec-recsys:

> In order to assess interuser similarity of the nature described above,
> we might form a matrix , as follows. There would be 943 rows, one for
> each user, and 1682 columns, one for each movie. The element in row ,
> column would be the rating user gave to movie . Most of the matrix
> would be 0s.
> 
> Determining the similarity of users becomes a matter of measuring
> similarity of rows of the matrix. This paves the way to exploiting the
> wealth of matrix-centric methodology we have developed in this book.

And *how* might this similarity between users be measured? By dot
product, of course! 

## Revisiting the Linear Model

Before going to the LLM usage of attention, it will be instructive to
view the familiar linear regression model in terms of attention (aside
from the point regarding $A'A$ above). Here we follow *Ordinary Least
Squares as an Attention Mechanism*, by Philippe Goulet Coulombe, 2025.

One again, consider the model 

$$
E(S|A) = A \beta
$$

whose least-squares solution is

$$
\widehat{\beta} = (A'A)^{-1} A'S
$$

where

* $A$, $n \times p$, is our predictor variable data, column $i$
  containing predictor $i$, "X",

* $S$, $n \times 1$, is our response variable data, "Y", and

* $\beta$, $p \times 1$, is a population vector, with sample estimate as above.

Alluding to the fact that fitting a model is often referred to as
"training" the model, below we will use the notation $A_{train}$ and
$Y_{train}$ for $A$ and $S$.

After computing $\widehat{\beta}$, we are ready to predict new cases!
Say we have $m$ of them, stored in the $m \times p$ matrix $A_{new}$.
Our predictions are

$$
Y_{preds} = A_{new} \widehat{\beta} = A_{new} (A'A)^{-1} A'_{train} Y_{train}
$$

As in @eq-diagonalize, write 

$$
A'A = P D P'
$$ 


At this point, we turn to the notion of the "square root" of a symmetric
matrix, introduced in @sec-squareroot. Write

$$
Y_{preds} = (A_{new} P D^{-0.5}) ~ (D^{-0.5} P'A'_{train}) ~ Y_{train} 
$$

so

$$
Y_{preds} = (F_{new} F'_{train}) ~ Y_{train}
$$ {#eq-ffy}

where $F_{new}$ and $F_{train}$ are $m \times p$ and $p \times n$.

Things to note about @eq-ffy:

* The matrix $F_{new} F'_{train}$ transforms our training "Y"
  values into predicted "Y" values.

* Each predicted "Y" value is a linear combination of the 
  training "Y" values.

* Thus the predicted values are *weighted* sums of the training values.

* The weights matirx $F_{new} F'_{train}$ factors neatly into a product
  of a matrix involving the new data and a matrix involving the
  training data.

These last two bullet points share the foundation of the attention
concept.

## Attention

Though the concept of attention is simple, the application can be quite
complex. Our presentation here will merely provide an overview.

### Basic structures

* There is a *query* matrix $Q$, analogous to $F_{new}$.

* There is a *keys* matrix $K$ analogous to $F_{train}$. 

* There is a *values* matrix $V$ analogous to $Y_{train}$.

Major differences from the last section: 

* The data in $Q$ and $K$ are *ordered sequences*, such as words within
  a sentence, genes within a chromosome or daily stock market prices in
  a time series.

* Rather than predicting a numerical quantity, these applications are
  driven by probabilities of sentences or other sequences.

The fundamental computations consist of dot products of rows of $Q$ and
$K$.

### Iterative computation

The word "attention" alludes to the weighting. Heavier weights are given
to more important parts of the input sequence.

Recall that in motivating the least-squares approach to estimation for
the linear regression model in @sec-leastsquaresintro, we said, "Pretend
for a moment that we donâ€™t know, say, $C_{28}$," the latter being one of
the "Y" values in our training set. We predict that value with our
model, and evaluate our error.

The idea is the same with LLMs, though as with most machine learning
algorithms, the computation is iterative. We take a query from our
training set, and predict "Y" value using our current weights. At each
iteration, the current guesses for probabilities in $V$ are updated.

(As noted, this involves clever updating algorithms, using tremendous
amounts of computation, whose details are beyond the scope of this
book.) 

### Next-token prediction

The items in a sequence, say words in a sentence or genes in a
chromosome, are called *tokens*. During training, we step through a
query sequence, one token at a time. At each step, we predict the next
token according to which would make the sentence most probable, given
our current guess for $V$. After building up sequences in this manner,
we build up a candidate full sequence. After doing this step for all
sequences in $Q$, we check to see how well we predicted. We then use
this to update all in our next iteration.

The idea of next-token prediction is easiest to understand in
an application like translating English test to, say, French. Here we
input a sentence in English, and output a sentence in French; our output
is the one determined to have the highest probability.


