
```{r} 
#| include: false
library(dsld)
library(qeML)
```

{{< pagebreak >}}

# Attention

::: {.callout-note}

## Goals of this chapter:

::: 

The emergence of Large Language Models (LLMs) really brought machine
learning/artificial intelligence startled the general public. Actually,
the technology is rather simple in its foundation, centered around a
mathematical operation called *attention*. This approach is in turn
related to material that has arisen here in earlier chapters. Here
we explore the basics of this idea.

:::

```{r} 
#| include: false
library(dsld)
library(qeML)
```

## Dot Product as a Measure of Similarity

Recall the key role of dot products in @sec-kerneltrick, with kernels
acting like generalized dot products. One of the reasons dot products
are so important is that they serve as measures of similarity.

### Correlation

Consider for example the matrix $A'A$ in @eq-linregformula. Its row $i$,
column $j$ element is the dot product of rows $i$ and $j$ of $A$.  But
if our variables are scaled to mean 0, variance 1, this quantity is the
correlation between those rows, revealing the similarity interpretation
of a dot product.

### Recommender systems

### Revisiting the linear model

